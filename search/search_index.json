{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#gremaus-research-notebook","title":"@gremau's research notebook","text":"<p>This is my set of notes about ecology and earth system science. I use it as a research notebook to document project activities, lab and field procedures, notes on data analysis and statistics, preliminary results, and general info on the nuts and bolts of doing research. Topics here slant towards my reseach interests (biogeochemistry and ecohydrology) and pages are not very organized or well-written. I've posted it nevertheless so others might derive some value from it.</p> <p>Cautionary note: All data, code, figures, results, and discussions are presented in their raw, preliminary form. They have not been peer reviewed and are provided without guarantee of quality, accuracy, or safety. Feel free to contact me with questions and to suggest changes or additions.</p>"},{"location":"#navigation","title":"Navigation","text":"<p>The best place to start is the Complete topic index</p>"},{"location":"#copyright-information","title":"Copyright information","text":"<p>All site content, unless otherwise noted and including sourcefiles at my GitHub account, is licensed under the CC Attribution Share Alike (CC-BY-SA) license 4.0.</p> <p>Unless otherwise noted, software and source code at my GitHub account that these pages link to is copyrighted 2007-2025 (c) Greg Maurer and is licensed under the Apache License, version 2.0. You may not use these files except in compliance with this license.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#questions-on-open-notebook-science-and-licensing","title":"Questions on open notebook science and licensing","text":"<ul> <li>Why put your research results up on a publicly viewable and editable website?</li> <li> <p>The reasons for this are many. First, scientific research is a collaborative enterprise, and this site exists to facilitate open communication and collaboration between researchers, students, and others inside and outside of academia. Second, most scientific research is publicly funded via NSF, NIH, or other sources that ultimately derive their money from taxpayers. Because of this, it is natural that the fruits of this funding, including knowledge of the scientific process, research results, and discussions about the significance of these results be available to the public. Open access publishingand Open notebook scienceare increasingly becoming accepted norms for scientific research, and this site is in keeping with these philosophies. Finally, this site provides direct access to global change research and should be of interest to a broad segment of the public. We hope that this site will encourage those outside the \"bubble\" of academia to learn about matters of global change and to participate in the scientific process.</p> </li> <li> <p>Are all your data and research results available, or only some subset you've already analyzed/published/modified?</p> </li> <li> <p>I strive to post new methods, data, and results as quickly as possible, making this very much a \"research in progress\" site. Data presented on this wiki, including research results in tables and figures, are the product of putting raw data through some processing routine. This can include data selection/filtering, numerical analysis, and plotting figures, and it is usually accomplished usin Python, R, or Matlab scripts. The products of this data processing are what you see on the wiki. All of these products, even failed or problematic ones, will be posted as soon as I get around to it. I stress that they are unedited and haven't been peer reviewed. Thus, I do not guarantee the quality or accuracy of any research results you will find here. </p> </li> <li> <p>Can I see the raw data and processing steps that led to your results?</p> </li> <li> <p>One aim of this notebook is to make the methods and results of this research transparent and repeatable. Field and lab methods and data collection techniques will be well documented here. Data processing and analysis techniques, including the source code of all data processing/analysis scripts, will be available along with the results they generate. Raw data, including field measurements, datalogger storage files, weather station data streams, laboratory instrument output, et cetera, reside in text files or spreadsheets on a hard drive somewhere. Because these are typically large files and are sometimes formatted in a non-user-friendly way I'm not going to post them right on the wiki.  Upon request I'm happy to provide any raw data, especially if I know a little about who you are and what you would like to use the data for. Just email me at primaryproductivity@gmail.com. This data will probably come with a data sharing policy, and again, I make no guarantees about the quality, accuracy, or safety of any of this material. For more on copyright and licensing information see the copyrightpage.</p> </li> <li> <p>Are there any exceptions to this \"Open Notebook\" policy?</p> </li> <li>Yes. In some cases the data used in this research is provided by collaborators, ie. it is not collected by me. In these cases I can't post research results or otherwise provide the data without their permission, which they may not give.</li> </ul>"},{"location":"topicindex/","title":"Active research projects","text":"<p>Each research project has its own directory. Within these project directories there are overview pages, methods pages (usually site-specific methods), project logs, and results pages. Overview pages describe the project objectives, hypotheses, and research approach, and should link to all other pages in the project namespace.</p>"},{"location":"topicindex/#hidden-canyon-ecohydrology","title":"Hidden Canyon ecohydrology","text":"<ul> <li>Overview</li> <li>Experimental design: Snowmelt manipulation, Soil profiles</li> <li>Methods: Dust, Dust on snow, Snowpack sublimation, Xylem pressure, Below-snow soil resp, Warm season resp, Litter bags</li> <li>Measurement logs: Snowmelt, Soil respiration, Litter bags</li> <li>Hidden Canyon activity logs, data analysis log</li> </ul>"},{"location":"topicindex/#western-us-soil-carbon-cycling","title":"Western U.S. soil carbon cycling","text":"<ul> <li>Overview</li> <li>Methods: Protocol for SNOTEL visits, Initial sample processing</li> <li>Logs: SNOTEL visits, Sample processing, Data analysis, Sample processing tables.</li> </ul>"},{"location":"topicindex/#niwot-girdling-study-niwot_girdlinggirdling","title":"Niwot girdling study (niwot_girdling/girdling)","text":"<ul> <li>Methods: Data QC, Soil analysis, Soil extract 13C procedure</li> <li>Activity log</li> </ul>"},{"location":"topicindex/#procedures","title":"Procedures","text":"<p>Protocols for making measurements in the field or the lab, processing samples for analysis, or analyzing data. Intended as a general reference, but site or experiment-specific info can be appended as necessary.</p>"},{"location":"topicindex/#laboratory-protocols","title":"Laboratory protocols","text":"<ul> <li>Measuring exetainer gas samples for CO~2~ and C isotopes</li> <li>Acid treatment of soils to remove carbonates.</li> <li>Soil sample prep for EA-IRMS analysis at SIRFER.</li> <li>Graphitization of soils for AMS ^14^C analysis</li> <li>Preparing and taking data from tree cores</li> <li>Snowpack dust loading</li> <li>\u03b413C analysis of soil extracts</li> </ul>"},{"location":"topicindex/#field-protocols","title":"Field protocols","text":"<ul> <li>Manual soil respiration measurements (Hidden Canyon)</li> <li>Below-snow respiration measurements(Hidden Canyon)</li> <li>Litterbags</li> <li>Dust deposition on snowpacks (Hidden Canyon)</li> <li>Xylem Pressure</li> <li>Snowpack Sublimation</li> <li>Canopy photography</li> <li>Georeferencing with GPS</li> </ul>"},{"location":"topicindex/#data-analysis-and-instrumentation-programming","title":"Data analysis and instrumentation programming","text":"<ul> <li>Data analysis programming: General info for using Python/MATLAB/Octave/R for data analysis</li> <li>Python notes, Numpy notes, Using IPython</li> <li>Matlab notes</li> <li>Shell scripts, AWK, Vim, and assorted textfile notes</li> <li>mercurial, and git: 2 version control systems.</li> <li>Tips for analyzing sensor timeseries</li> <li>Workflow for data analysis</li> <li>Geospatial data analysis</li> </ul>"},{"location":"topicindex/#research-writing","title":"Research writing","text":"<ul> <li>Text file notes</li> <li>Pandoc document converter and markup syntax.</li> <li>LaTeX notes</li> </ul>"},{"location":"topicindex/#math","title":"Math","text":"<p>Pages covering statistical and mathematical concepts, or numerical methods used in data analysis. Some of these pages are composed primarily of links to further information on the subject.</p> <ul> <li>Computation toolboxes - tools for computing math and statistics.</li> <li>Time series analysis - assorted methods for analyzing time series data to extract meaningful statistics.</li> <li>ANOVA - analysis of variance for various experimental designs.</li> <li>Reverse calibration equations - using the quadratic equation to back out sensor output from calibrated data (if calibration is known).</li> <li>Principal components analysis and Principal components regression</li> <li>Correlation</li> <li>Testing for normality</li> <li>Linear regression - Bivariate (one independent variable) and multiple regression (&gt;1 ind. variable).</li> <li> Multilevel/Mixed/Heirarchical statistical models</li> <li>Integration - of functions, or sampled data such as a timeseries</li> <li>Decay and turnover functions.</li> </ul>"},{"location":"topicindex/#instruments","title":"Instruments","text":"<p>Contain documentation for operating, calibrating, and maintaining field and lab instruments, sensors, etc. There are also notes on programs, calibration constants, and conversion coefficients for instruments currently in use.</p> <ul> <li>Li-Cor Quantum sensor (LI-190.md)</li> <li>Notes, conventions, and code for EDLOG programming (Campbell datalogger control)</li> <li>Li-Cor 6400 portable soil respiration system (in use at Hidden Canyon)</li> <li>Campbell cr10 dataloggers (several old ones in Red Butte Canyon)</li> <li>Sentek EnviroSCAN soil moisture profiles (there are a couple at Hidden Canyon).</li> <li>EA-IRMS protocol at the SIRFER lab.</li> </ul>"},{"location":"topicindex/#research-sites","title":"Research Sites","text":"<p>Research sites (or networks) have their own namespace. Pages in a site namespace should have descriptive information about the sites themselves, research instrumentation, or researcher activities there. For example: site maps, research plot layouts, technical info and schematics for research installations (weather stations, etc.), activity/maintenance logs, etc.</p>"},{"location":"topicindex/#hidden-canyon","title":"Hidden Canyon","text":"<ul> <li>Site description, Tree location data</li> <li>Instrumentation: communicationsystem, dataloggers, mettowers, soilprofiles </li> <li>Fieldwork log (2009-2013), data analysis log</li> </ul>"},{"location":"topicindex/#red-butte-canyon","title":"Red Butte Canyon","text":"<ul> <li>Above the University of Utah campus. There is a weather station network there.</li> <li>RBC Weather Station info, WS maintenance log, Using cr10 dataloggers</li> </ul>"},{"location":"topicindex/#nrcs-snow-survey-sites","title":"NRCS Snow Survey sites","text":"<ul> <li>A network of mountain sites in the western U.S. where manual (snow courses) and automated (SNOTEL sites) measurements of snowpack (SWE) are taken for water supply forecasting. Manual measurements date back to the 1930's.</li> <li>Official site</li> </ul>"},{"location":"topicindex/#niwot-ridge-lter","title":"Niwot Ridge LTER","text":"<ul> <li>A high elevation LTER and Ameriflux site in Colorado.</li> <li>Extensive research on tundra and forest ecology has been in progress here for a long time. The Bowling lab has instrumentation that measures biosphere-atmosphere interactions (CO~2~ mainly).</li> </ul>"},{"location":"topicindex/#inactive-research-projects","title":"Inactive research projects","text":"<p>These are either completed or on hold.</p> <ul> <li> Niwot Ridge label decomposition - overview ON HOLD --- //Greg Maurer 2010/08/09 13:11//</li> </ul>"},{"location":"topicindex/#ion-deposition-in-snow-snowdep","title":"Ion deposition in snow (snowdep)","text":"<ul> <li>Methods: Snow sampling and cleanliness procedures, Processing snow samples for analysis, Mixing ion standards</li> <li>Data analysis log</li> <li>Activity log (not used)</li> </ul>"},{"location":"topicindex/#western-us-soil-climate-analysis-west_stationdata","title":"Western U.S. soil climate analysis (west_stationdata)","text":"<ul> <li>Overview</li> <li>Methods: Data collection and prep, Data programming documentation, Data QC (cleaning), Statistical approach &amp; methods</li> <li>Logs: Data analysis log</li> <li>Publication outline</li> <li>Paper at Water Resources Research</li> </ul>"},{"location":"computing/awk/","title":"AWK Notes","text":"<p>Awk is great for editing delimited text files, and can be easily used in shell scripts.</p> <p>Back to general programming page</p>"},{"location":"computing/awk/#basic-stuff","title":"Basic stuff","text":"<p>Code blocks, which are executed for each line of the text file, are in curly braces:</p> <p>~~~{.bash}  awk '{ print }' AZ_soilstations.csv</p>"},{"location":"computing/awk/#this-will-print-the-az_soilstationscsv-file-to-the-shell-assuming-it-is-in-the-current-working-directory","title":"this will print the AZ_soilstations.csv file to the shell (assuming it is in the current working directory).","text":"<pre><code>\nAwk can operate each line of the file as a whole, or in fields\nseparated by a special character:\n\n~~~{.bash}\nawk '{ print $0 }' AZ_soilstations.csv\n# this prints the whole line\n\nawk '{ print $1 }' AZ_soilstations.csv\n# this prints the line up to the first field separator (default is one or more spaces)\n</code></pre> <p>The field separator (FS) can be set prior to running the code block:</p> <pre><code>$ awk -F\",\" '{ print $1 \" \" $3 }' AZ_soilstations.csv\n# prints the first and third column of this comma delimited field\n</code></pre> <p>Expressions, such as pattern matching, can be added before the code block. The following will output the network, station id, state, and name of all scan sites in the file:</p> <pre><code>awk -F\",\" '/scan/ { print $1 \" \" $2 \" \" $9 \" \" $12 }' AZ_soilstations.csv\n# search expressions should be surrounded with forward slashes.\n# outputs: scan 2026 AZ WALNUT GULCH #1\n</code></pre> <p>Comparison expressions (==, &lt;, &gt;, &lt;=, &gt;=, !=, plus ~ (matches) and !~) can also be used:</p> <pre><code>awk -F\",\" '$1==\"snotel\" { print $1 \" \" $2 \" \" $9 \" \" $12 }' AZ_soilstations.csv\n\n# Outputs:\nsnotel 310 AZ BALDY\nsnotel 1121 AZ FORT VALLEY\nsnotel 969 AZ HAPPY JACK\nsnotel 1125 AZ MORMON MTN SUMMIT\nsnotel 861 AZ WHITE HORSE LAKE\n</code></pre> <p>As can logical expressions such as AND (&amp;&amp;) and OR (||):</p> <pre><code>awk -F \",\" '( $1 == \"snotel\" ) &amp;&amp; ( $7 &gt; 7000 ) { print }' AZ_soilstations.csv\n# prints only the lines of AZ snotel sites above 7000 ft elevation\n</code></pre>"},{"location":"computing/awk/#other-features","title":"Other features:","text":"<ul> <li>Can use arithmetic operators (+-*/%^)</li> <li>Can use conditional statements for control flow:</li> <li>if ( //expression// ) //statement1// else //statement2//</li> <li>while ( //expression// ) //statement//</li> <li>for ( //expression1//; //expression// ; //expression2// ) //statement//</li> <li>do //statement// while( //expression// )</li> <li>Some important environmental variables:</li> <li>NF (number of columns) </li> <li>NR (the current line that awk is working on)</li> <li>END (true if awk reaches the EOF)</li> <li>BEGIN (true before awk reads anything)</li> </ul>"},{"location":"computing/awk/#built-in-functions","title":"Built in functions","text":"<ul> <li>gsub(r,s)       substitutes s for r globally in current input line, returns the  number of substitutions </li> <li>gsub(r,s,t)     substitutes s for r in t globally, returns number of substitutions </li> <li>index(s,t)      returns position of string t in s, 0 if not present </li> <li>length(s)       returns length of s </li> <li>match(s,r)      returns position in s where r occurs, 0 if not present </li> <li>split(s,a)      splits s into array a on FS, returns number of fields </li> <li>split(s,a,r)    splits s into array a on r, returns number of fields </li> <li>sprintf(fmt, expr-list) returns expr-list formatted according to format string  specified by fmt </li> <li>sub(r,s)        substitutes s for first r in current input line, returns number of substitutions </li> <li>sub(r,s,t)      substitutes s for first r in t, returns number of substitutions </li> <li>substr(s,p)     returns suffix s starting at position p </li> <li>substr(s,p,n)   returns substring of s length n starting at position p  </li> </ul>"},{"location":"computing/awk/#command-line-usage","title":"Command line usage","text":"<p>Note that:</p> <ul> <li>Input can come from files or be piped in from shell commands.</li> <li>Output can be redirected into files or piped to bash, etc.</li> <li>Lots of these are from here or here</li> <li>Also see examples in the shell scripting page.</li> </ul> <p>Convert Windows/DOS newlines (CRLF) to Unix newlines (LF) from Unix. Removes the carriage return (\\r) at the end of the line ($ at end of search pattern), leaving linefeed: </p> <pre><code> awk '{ sub(/\\r$/,\"\");  print }' filename.txt\n</code></pre> <p>This would remove one or more (+ in pattern) leading (\\^ in pattern) spaces for each line:</p> <pre><code> awk '{ sub(/^ +/,\"\"); print }' filename.txt\n</code></pre> <p>This would remove one or more leading spaces or tabs (brackets join multiple search terms) in the fifth column only (the $5 marks column 5):</p> <pre><code>awk -F\",\" ' BEGIN{OFS=\",\"} { gsub(/\\^ +/,\"\", $5); print }' AZ_soilstations.csv\n\n# Prints the entire .csv file, spaces are removed from column 5.\n# Because this is a .csv the field separator must be set (-F)\n# To preserve comma delimiters on output the output field separator (OFS) must be set in a begin block.\n</code></pre> <p>To do the same operation on several fields just add another statement to the code block:</p> <pre><code>awk -F\",\" ' BEGIN{OFS=\",\"} { gsub(/^ +/,\"\", $5); gsub(/^ +/,\"\", $7); print }' AZ_soilstations.csv\n</code></pre> <p>This command will print a textfile containing sitenumbers for each of a sites SNOTEL files in a data directory with filenames like 828_ALL_WATERYEAR=2002.csv (all files begin with an integer site code and there are files for multiple years in the directory).:</p> <pre><code> ls *.csv | awk -F\"_\" '{print $1}' &gt;  sitelist.txt\n</code></pre> <p>This nifty bash command moves and renames an entire directory of files with awk:</p> <pre><code>ls junk\\* | awk '{print \"mv\"$0\" ../trashdir/\"$0\".dat\"}' | bash\n\n# No pattern to match, so for each line of input piped in by ls, prints mv junk1 ../trashdir/junk1.dat ....etc to bash\n</code></pre>"},{"location":"computing/awk/#executing-saved-scripts","title":"Executing saved scripts","text":"<p>BEGIN blocks allow initialization code (such as setting variables) before running the code block on each line of the input file:</p> <pre><code>BEGIN { \n        FS=\":\" \n} \n{ print $1 }\n# Setting the field separator is best done in a BEGIN block before running the code block \n</code></pre> <p>End blocks do end of script reporting or calculations:</p> <pre><code>BEGIN { x=0 } \n/^$/  { x=x+1 } \nEND   { print \"I found \" x \" blank lines. :)\" }\n# Prints out the number of blank lines in the file \n</code></pre> <p>If saved in a file the script above could be run with:</p> <pre><code>awk -f myfile.awk AZ_soilstations.csv\n</code></pre>"},{"location":"computing/data_analysis_workflow/","title":"Workflow for data analysis (Greg)","text":"<p>Greg's notes on how data moves from collection, to the filesystem, then through the data analysis process.</p> <p>See also: General programming page.</p>"},{"location":"computing/data_analysis_workflow/#filesystem","title":"Filesystem","text":"<ul> <li>~/data/ - on its own partition, regularly backed up</li> <li> <p>current/  - contains directories, organized by project, storing metadata, initial field/lab measurements (usually in spreadsheet form), reports, papers in progress, etc. Each project directory may also contain a data_analysis directory storing data analysis functions and scripts, symlinks to the project's rawdata directory, and any other processed data needed for data analysis. THIS DIRECTORY MUST HAVE VERSIONING</p> <ul> <li>project_1/</li> <li>Lab and field spreadsheets...</li> <li>Maps of plot locations, etc</li> <li>Documentation files for dataloggers, etc</li> <li>Papers in progress</li> <li>data_analysis/<ul> <li>symlink to ../rawdata/project_1/</li> <li>m/</li> <li>matlab scripts and functions</li> <li>py/</li> <li>python scripts and functions</li> <li>processed_data/</li> <li>textfiles with (for example) data exported from spreadsheets in parent directory, data from processing scripts, plot locations converted to UTM (from map), etc.</li> </ul> </li> <li>project_2/</li> <li>...</li> <li>data_analysis/<ul> <li>symlink to ../rawdata/project_2/</li> <li>m/</li> <li>...</li> </ul> </li> <li>archive/ - same as above but for projects that are completed/not maintained</li> <li>oldproject_1/</li> <li>rawdata  - contains directories for each project, typically storing raw data downloads, datalogger files, or data that has been minimally processed from the initial measurements. This data is usually large in size, is suitable for sharing among many projects, or comprises some contextual information not directly measured in the current project (environmental monitoring data for example).</li> <li>project_1/</li> <li>textfile1 (long-term climate measurments from a nearby weather station)</li> <li>textfile2 (datalogger .dat file)</li> <li>project_2/</li> <li>...</li> </ul> </li> </ul>"},{"location":"computing/data_analysis_workflow/#general-workflow","title":"General Workflow","text":"<p>This refers to current projects, each of which is stored in a data/current/projectname/ directory.</p>"},{"location":"computing/data_analysis_workflow/#data-collected-in-the-field-or-lab","title":"Data collected in the field or lab","text":"<ul> <li>In general, this data must be transcribed from field sheets, downloaded from instruments, formatted, etc, and this is most often done using spreadsheets or text files. </li> <li>Some formatting, editing, or summary creation of textfiles can be done with shell scripts, awk, etc, but be sure to document and save the scripts in the appropriate projectname/data_analysis/ directory.</li> <li>Processed data can be exported as a textfile to the projectname/data_analysis/processed_data/ directory, or put in the rawdata/ directory (if it fits criteria above).</li> <li>Data is then processed and analyzed with python, matlab, or other scripts located in the projectname/data_analysis/ directory</li> <li>This analysis generates outputs(datafiles, figures) that can be stored in projectname/data_analysis/processed_data/ for further use in analysis, or in the projectname/ directory, the wiki, or elsewhere for interpretation, publication, etc.</li> </ul>"},{"location":"computing/data_analysis_workflow/#the-generalized-workflow-looks-like-this","title":"The generalized workflow looks like this:","text":"<p>Field/Lab measurements =&gt; projectname/ =&gt; Process         to text =&gt; (QC/data munging =&gt;)         projectname/processed_data/ =&gt; Analysis with         matlab/python/r (calculation, summary data, plots) =&gt;         projectname/processed_data/ (OR projectname/, papers, wiki,         reports, etc for finished products)</p>"},{"location":"computing/data_analysis_workflow/#data-from-elsewhere","title":"Data from elsewhere","text":"<ul> <li>Often this is downloaded as text data (ie, from SNOTEL, etc) in (hopefully) quality-checked form. However some munging may be necessary to trim, format, or otherwise make the data easier to use </li> <li>Probably use shell scripts, awk, etc, and again, be sure to document and save the scripts in the appropriate projectname/data_analysis directory.</li> <li>Data can then be directly placed in the ~/data/rawdata/projectname/ directory</li> <li>Follow steps 3 &amp; 4, as above.</li> </ul>"},{"location":"computing/data_analysis_workflow/#the-generalized-workflow-looks-like-this_1","title":"The generalized workflow looks like this:","text":"<p>Download data =&gt; (QC/Data munging =&gt;)         rawdata/projectname/ =&gt; Analysis with matlab/python/r         (calculation, summary data, plots) =&gt;         /projectname/data_analysis/processed_data (OR /projectname/,         wiki, paper, etc for finished products)</p>"},{"location":"computing/data_analysis_workflow/#coding-and-version-control","title":"Coding and version control","text":""},{"location":"computing/data_analysis_workflow/#code-pieces","title":"Code pieces","text":"<p>In terms of actual pieces of code, there are generally several types that recur in each project.</p> <ul> <li>a loading function</li> <li>a data cleaning function</li> <li>the function(s) that actually do the analysis (calculating, transforming, selecting data)</li> <li>the script(s) that produce reports or plots using the analysis functions above</li> </ul> <p>In practice, this means that in each data analysis project folder, there will be (at least) one version of each of these types of functions. Depending on the language in use and the maturity of the project, there may be many function files and reporting/plotting scripts. See more discussion of about these types of issues here, here, here and here</p>"},{"location":"computing/data_analysis_workflow/#versioning","title":"Versioning","text":"<p>Within each project folder in data/data_analysis there is a versioning repository (.hg or .git). These repositories are initialized early in the development of the project and commits are made to the repository when substantial changes are made to the codebase. For example, if a new plotting script is written one day, it should be added and committed to the repository. If a function is changed to use a different equation or constant, the change should be committed. Branching is also possible for testing new analyses or generating publication subsets of the analysis, and these branches can later be merged into the main, or kept separate. In most cases, data is not versioned, it is stored in the rawdata directory and changes to that directory are logged in a README file.</p> <p>Versioning is currently done with mercurial, but git has a page here too.</p>"},{"location":"computing/debian_config/","title":"DebianConf.markdown","text":"<p>This documents my maintenance routines and personal configurations of Debian and other software for my Debian laptop. Useful web documents are listed in each section, and additional information is available in the main reference and other support documents.</p> <p>Specific softwares (R, Python, LaTex, etc.) have thier own pages in the notebook.</p>"},{"location":"computing/debian_config/#debian-references","title":"Debian references","text":"<ul> <li>Ref manual: http://www.debian.org/doc/manuals/debian-reference/index.en.html</li> <li>Debian FAQ: http://www.debian.org/doc/FAQ</li> <li>Debian wiki: http://wiki.debian.org</li> <li>Index of other support: http://www.debian.org/support</li> </ul>"},{"location":"computing/debian_config/#sources","title":"SOURCES","text":"<p>Currently tracking testing. Not using the name (jessie)</p> <p>Other entries in sources.list:</p> <pre><code># The liquorix kernel\ndeb http://liquorix.net/debian sid main - for liquorix kernel\n\n# The experimental debian repo - used for a few packages\ndeb http://ftp.debian.org/debian/ experimental main contrib non-free\n</code></pre> <p>REMEMBER: After updating sources.list, run <code>apt-get update</code></p>"},{"location":"computing/debian_config/#kernels-liquorix-and-others","title":"KERNELS, LIQUORIX and others","text":"<p>Excellent resources for Debian Kernels:</p> <ul> <li>http://kernel-handbook.alioth.debian.org</li> <li>http://www.debian.org/doc/FAQ/ch-kernel.en.html</li> <li>http://wiki.debian.org/KernelFAQ - for general info</li> </ul> <p>The kernel installed by default in the netinst installation is a somewhat generic(not optimized) kernel, so it may be useful to replace it. Though the debian installer installed the correct kernel for my architecture, in some cases one may want to look at other options in the repositiories.</p> <p><code>apt-cache search linux-image</code> (or <code>-headers</code>) should list the available kernels for your achitecture, as well as additional kernels with special featuresets. Install both the linux-headers and the linux-image package you desire.</p> <p>The Debian kernel was replaced with the \"high performance\" liquorix kernel package. Liquorix is a Debian repository for Zen-kernel builds.</p> <p>See: http://www.liquorix.net &amp; http://zen-kernel.org</p> <ol> <li>Place <code>deb http://liquorix.net/debian/ sid main</code>in sources.list</li> <li>Run <code>apt-get update</code> and install liquorix-keyrings package.</li> <li>Look at current kernel version: <code>uname -r</code></li> <li>To find newer liquorix kernel: <code>apt-cache search liquorix</code></li> <li>Install headers and image for 2.6.xx-0.dmz-liquorix-amd64 (should take a while).</li> </ol> <p>The first time I did this binutils, gcc, and a couple of libs were pulled in.</p> <p>Liquorix kernels can also be installed and upgraded with the smxi script (smxi.org)</p> <p>Kernels from Debian or other repos can be customised using the kernel-package program. See http://www.debian.org/doc/FAQ/ch-kernel.en.html</p>"},{"location":"computing/debian_config/#vpn-configuration","title":"VPN configuration","text":"<p>OpenConnect is the open source project allowing connection to Cisco AnyConnect networks, but there are other vpn options. Installing the package below installes the openconnect package and should make it available in the NetworkManager plugin applet. </p> <p>https://github.com/cboettig/berkeley-linux-config/blob/master/vpn.md</p> <pre><code>apt-get install network-manager-openconnect-gnome\n</code></pre> <p>You can also use openconnect on the commandline, for example:</p> <pre><code>openconnect ucbvpn.berkeley.edu\n</code></pre>"},{"location":"computing/debian_config/#kernel-modules","title":"KERNEL MODULES","text":"<p>Upon startup, UDEV detects hardware and loads needed modules into the kernel. These loaded modules can be viewed with <code>lsmod</code> and loaded manually (removed) with \"modprobe (-r)\"</p> <p>It is possible to force or prevent the loading of a module by listing the name of these modules in one of these files:</p> <ul> <li>/etc/modules-version (version is the kernel version)</li> <li>/etc/modules-major (major is the major kernel version - 2.6)</li> <li>/etc/modules</li> </ul> <p>Modules can also have parameters and arguments passed to them with these files. The SMXI script also has a procedure for adding and removing particular modules.</p>"},{"location":"computing/debian_config/#installed-kernel-modules","title":"INSTALLED KERNEL MODULES:","text":"<p><code>hdaps</code> - advanced harddrive protection for thinkpads. (tp-smapi no longer supported by new thinkpads, see: http://www.thinkwiki.org/wiki/Tp_smapi)</p> <ol> <li> <p>Liquorix kernels come with <code>tp_smapi</code> and <code>hdaps</code> modules compiled</p> </li> <li> <p>Load now with <code>modprobe -a tp_smapi hdaps</code>, OR</p> <pre><code>$echo 'tp_smapi' &gt;&gt; /etc/modules\n$echo 'hdaps' &gt;&gt; /etc/modules\n</code></pre> </li> <li> <p>Add these lines to /etc/modprobe.conf/local.conf</p> <pre><code>options thinkpad_ec force_io=1\noptions hdaps invert=1\n</code></pre> </li> <li> <p>Set charging threshholds, <code>tp_smapi</code> needs sysfsutils</p> <pre><code>$ apt-get install sysfsutils\n</code></pre> </li> <li> <p>AND add these lines to sysfs.conf:</p> <pre><code>devices/platform/smapi/BAT0/start_charge_thresh = 30\ndevices/platform/smapi/BAT0/stop_charge_thresh = 85\n</code></pre> </li> <li> <p>Also install hdapsd to run HDAPS from userspace. This will pull in <code>tp-smapi-dkms</code> and <code>dkms</code> (debian versions of these modules), but I subsequently removed <code>tp-smapi-dkms</code>. </p> </li> </ol>"},{"location":"computing/debian_config/#updates-and-upgrades","title":"UPDATES and UPGRADES","text":"<p>Generally, use apt:</p> <pre><code>apt update\napt upgrade  # OR dist-upgrade (same as full-upgrade)\n</code></pre> <p>The reference suggests that apt dist-upgrade may be more suited to major upgrades (between releases, stable to testing, etc) http://www.debian.org/doc/FAQ/ch-uptodate.en.html</p>"},{"location":"computing/debian_config/#google-drive","title":"Google Drive","text":"<p>Use the drive package here: https://github.com/odeke-em/drive</p> <p>Someone has made a Debian package for this, see: https://github.com/odeke-em/drive/blob/master/platform_packages.md</p>"},{"location":"computing/debian_config/#smxiorg-maintenance-scripts","title":"SMXI.org MAINTENANCE SCRIPTS","text":"<p>See SMXI.org and http://techpatterns.com/forums/forum-34.html</p> <p>Install scripts using:</p> <pre><code>cd /usr/local/bin &amp;&amp; wget -Nc smxi.org/smxi.zip &amp;&amp; unzip smxi.zip &amp;&amp; smxi\n</code></pre> <p>Upon installing and running the script it asks a bunch of configuration questions. Add these options:</p> <ol> <li>Use liquorix kernel</li> <li>Use apt-get</li> <li>Set distribution to testing </li> <li>Use full-upgrade instead of safe</li> </ol> <p>These options can be reset by erasing <code>/etc/smxi.conf</code></p> <p>Upon reaching the main menu (after upgrades), there are several options:</p> <ol> <li>Install packages (non-free stuff, large packages, etc)</li> <li>Remove packages (remove unneeded packages  - bluetooth, gnomemeeting, etc)</li> <li>Clean up stuff</li> <li>Miscellaneous tweaks (tweaks to mozilla, change script config, etc)</li> <li>Virtual machine installer</li> <li>Kernel options (install new kernels, add/remove modules)</li> <li>Continue to graphics installer</li> <li>Restart X/desktop</li> <li>Stop Script </li> </ol> <p>So far have done these actions:</p> <ul> <li>apt cleanups</li> <li>removed unneeded xorg modules</li> <li>installed google-earth</li> </ul>"},{"location":"computing/debian_config/#sudo","title":"SUDO","text":"<p>Sudo is installed by default. Privileges are granted through the <code>/etc/sudoers</code> file, which must be edited with visudo. I added these lines to the sudoers file:</p> <pre><code>User_Alias  ADMINS = greg   # Under 'User alias specification'\nADMINS ALL = ALL            # Under 'User privilege specification'\n</code></pre> <p>To create a list of administrator accounts and grant myself sudo access to all commands.</p>"},{"location":"computing/debian_config/#other-options","title":"Other options","text":"<ul> <li>Aliases to individual commands can be added with <code>Cmnd_Alias</code> statements and these can be added to User privilege specifications also (instead of ALL)</li> <li> <p>To maintian syntax highlighting when using sudo from the terminal I used 2 approaches:</p> <ol> <li> <p>Add these to sudoers file (with visudo):</p> <pre><code>Defaults        env_keep += \"HOME COLORFGBG LS_COLORS\"\nDefaults        env_reset\n</code></pre> </li> <li> <p>create <code>.bash_aliases</code> file and add <code>alias sudo='sudo '</code> to it.</p> </li> </ol> </li> </ul>"},{"location":"computing/debian_config/#connect-to-data-partition-sda5","title":"CONNECT TO DATA PARTITION (sda5)","text":"<p>Add this line to fstab:</p> <pre><code># mount /dev/sda5 as /home/greg/data\nUUID=79faee16-4164-4fcc-b19c-499084c3764f /home/greg/data ext4 defaults 0 2\n</code></pre>"},{"location":"computing/debian_config/#connect-to-network-drive-nas","title":"CONNECT TO NETWORK DRIVE (NAS)","text":"<p>A NAS drive can be browsed and edited a variety of ways. Note that there are a number of users using the  share. Mounting the drive with CIFS can be done with these steps: <ol> <li> <p>Create the <code>/media/nas_name/</code> directory</p> </li> <li> <p>This command (as root) should mount the drive:</p> <pre><code>mount -t cifs //&lt;ip-address&gt;/&lt;share-name&gt; /media/&lt;mntpt-name&gt; -o nounix,user=greg,file_mode=0777,dir_mode=0777\n</code></pre> </li> <li> <p>There is a script (<code>~scripts/mount_nas.sh</code>) that can be run with sudo to mount the drive.</p> </li> <li> <p>Unmount with:</p> <pre><code>sudo umount /&lt;mntpt-name&gt;/\n</code></pre> </li> <li> <p>This drive is also available by ssh using:</p> <pre><code>ssh -l &lt;username&gt; &lt;ip-address&gt;\n</code></pre> </li> </ol> <p>Network drives can also be mounted with gvfs, as long as <code>gvfs-backends</code> is  installed (this contains a gvfs-smb package).</p> <p>Rsyncs to this drive can be done through ssh for backups.</p>"},{"location":"computing/debian_config/#printing-to-network-printers","title":"PRINTING to NETWORK PRINTERS","text":"<p>See: http://wiki.debian.org/SystemPrinting, cups help screens in http://localhost:631</p> <ol> <li>Install cups, cups-client, and foomatic printer driver packages (these should most likely already be here)</li> <li>Start cups with <code>/etc/init.d/cups start</code></li> <li> <p>Navigate to http://localhost:631/</p> <p>There should be a web interface to cups here where it should be possible to add a printer.</p> <ol> <li>Administration(login as root)--&gt;Add Printer--&gt;</li> <li>HP LaserJets are usually on AppSocket protocol using port 9100, so select this option from the bottom of this list (below network printers).</li> <li>Enter <code>socket://&lt;ip-address&gt;:9100/</code> for the URI connection</li> <li>Name the printer with human readable identifiers.</li> <li>Select make and driver (HP LaserJet4250 Foomatic/Postscript for BowlingLJ).</li> <li>Add printer.</li> <li>Set Default options (two sided, etc)</li> <li>You can now set this printer as default if desired.</li> </ol> </li> </ol>"},{"location":"computing/debian_config/#ssh","title":"SSH","text":"<p><code>openssh-client</code> should be installed for ssh access to remote hosts. For added security, it is best to use public/private keys during ssh sessions. To do this:</p> <ol> <li> <p>Generate a public/private ssh key pair with <code>ssh-keygen -t rsa -b 4096 -C \"email address\"</code></p> <p>This will put a public/private key pair in the <code>~/.ssh</code> directory. You will be asked for a passphrase, which will be needed when sending the public key to a host computer.</p> </li> <li> <p>Copy the public key to a remote host using <code>ssh-copy-id user@hostid</code>, or by manually appending the public key to the hosts <code>.ssh/authorized_keys</code> file, like:</p> <p>cat ~/.ssh/id_rsa.pub | ssh user@server 'cat &gt;&gt; .ssh/authorized_keys'</p> </li> <li> <p><code>ssh-agent</code> can save the passphrase for your public key, just use <code>ssh-add ~/.ssh/id_rsa</code>. Ive never gotten this to work in XFCE. Needs to be done at the creation of the desktop or bash session, but I'm not sure how this works. Try using <code>keychain</code>, <code>seahorse</code>, etc for this. Or read this -&gt; https://wiki.archlinux.org/index.php/SSH_Keys or the GitHub tutorial.</p> </li> <li> <p>Make a list of hostnames and ips in <code>/etc/hosts</code> for easier access to frequenly accessed hosts.</p> </li> </ol>"},{"location":"computing/debian_config/#thunderbird","title":"THUNDERBIRD","text":"<ol> <li>Install thunderbird</li> <li>run <code>icedove -profilemanager</code>, create a profile and point it to the profile in <code>~/data/thunderbird-profiles/pu3xvoh</code><ul> <li>alternatively, just move the profile folder into <code>~/.thunderbird</code> and edit the <code>profiles.ini</code> file to point to that folder (pu3xvoh.default)</li> </ul> </li> </ol>"},{"location":"computing/debian_config/#gdal","title":"GDAL","text":"<p>May be required for rgdal and other stuff (qgis should bring this in I think).</p> <p>sudo apt-get install libproj-dev libgdal-dev</p>"},{"location":"computing/debian_config/#wifi","title":"WIFI","text":"<p>See http://wiki.debian.org/WiFi , http://wiki.debian.org/iwlagn , and http://wiki.debian.org/WiFi/HowToUse</p> <ol> <li>Need iwlwifi modlule loaded into kernel</li> </ol> <p>This requires firmware (non-free package is <code>firmware-iwlwifi</code>) to be installed</p> <ol> <li>Also install <code>wireless-tools</code> package</li> <li>Verify device is accessible with <code>iwconfig</code></li> <li>Raise interface with <code>ifconfig wlan0 up</code></li> <li>Configure wireless interface with network-manager or wicd</li> <li>Be sure to point wicd to wlan0</li> </ol>"},{"location":"computing/debian_config/#other-steps","title":"Other steps","text":""},{"location":"computing/debian_config/#virtualbox","title":"VirtualBox","text":"<p>There are a couple ways to install it - covered here: https://wiki.debian.org/VirtualBox</p> <p>I use the oracle ppa, and add it to /etc/apt/sources.list.d/virtualbox.list</p>"},{"location":"computing/debian_config/#console-beeps","title":"CONSOLE BEEPS","text":"<ol> <li>Stop console beeps while in X with (as root) <code>xset b off</code></li> <li> <p>Stop console beep on gdm login screen using <code>gdmsetup</code> program</p> <p>OR uncomment <code>set bell-style none</code> in <code>/etc/inputrc</code> for a permanent solution in consoles.</p> <p>OR blacklist the kernel module by adding `blacklist pcspkr\" to /etc/modprobe.d/blacklist.conf</p> </li> </ol>"},{"location":"computing/debian_config/#screen-management","title":"SCREEN MANAGEMENT","text":"<p>arandr provides screen management (frontend for `xrandr\" commands). VGA 1 is the VGA port on the side of the laptop, LVDS1 is the laptop's screen</p>"},{"location":"computing/debian_config/#urxvt-and-other-x-configs","title":"URXVT and other X configs","text":"<p>Setting preferences for X and X applications can be done using .Xdefaults and .Xresources files. See https://wiki.archlinux.org/indes.php/Xdefaults. After adding something to these files use `xrdb -merge ~/.Xdefaults/Xresources\".</p> <p>urxvt is a nice light terminal to replace xterm, but it pays to configure its default colors:</p> <p>Create ~/.Xdefaults Add a bunch of lines with color definitions and other URxvt config options I used the wiki at http://crunchbanglinux.org/wiki/urxvt Also see wiki.archlinux.org/index.php/Rxvt-unicode</p>"},{"location":"computing/debian_config/#vim","title":"VIM","text":"<p>Installed vim with vim-gtk3 Configured in .vimrc using the Brad Moolenar example from vim website Added `set nobackup\" to keep annoying backup files (file.txt~) away.</p>"},{"location":"computing/debian_config/#bittorrent-sync","title":"BITTORRENT SYNC","text":"<ol> <li>Download binary from http://labs.bittorrent.com/experiments/sync.html</li> <li>Extract binary and move to desired folder (probably /usr/local/installed)</li> <li>Run with './btsync'</li> <li>Access user interface at http://localhost:8888/gui/</li> </ol> <p>May want to autostart this</p>"},{"location":"computing/debian_config/#digital-camera","title":"DIGITAL CAMERA","text":"<p>I installed Shotwell as a photo organizer This relies on libgphoto2 to communicate with my Canon cameras, it all seems to work as long as this is installed. Gphoto2 can be used to access cameras on the command line.</p>"},{"location":"computing/debian_config/#skype","title":"SKYPE","text":"<p>See here: https://wiki.debian.org/skype</p> <p>There is a skype for linux deb package to download at:</p> <p>https://www.skype.com/en/get-skype/</p>"},{"location":"computing/debian_config/#signal","title":"SIGNAL","text":"<p>https://signal.org has instructions for the desktop app</p>"},{"location":"computing/debian_config/#other-stuff","title":"OTHER STUFF","text":"<p>Installing .deb files: Debian packages (.deb files) can be downloaded from various sources. They can be unpacked and installed with `dpkg -i ...\" If the .deb fails to install read the dependency output and install the required dependencies and then reinstall.</p> <p>Getting information about packages: <code>apt-cache showpkg ...\" gives very detailed info about packages, dependencies, etc.</code>apt-cache policy ...\" gives info about the repositories a package is in <code>apt-get show ...\" gives general info about packages, good for prior to install\"apt-cache search ...\" searches for a package</code>apt-get install -s ...\" simulates the install of a package</p> <p>Viewing logs:  All logs are in /var/log/. They can be viewed in a terminal using less, more, etc. `tail /var/log/...\" shows the last 10 lines of a log. \"tail -f /var/log/... follows the log and updates the display as it is appended.</p>"},{"location":"computing/debian_config/#other-packages-to-install","title":"OTHER PACKAGES to install","text":"<ul> <li>xorg</li> <li>openbox</li> <li>obconf</li> <li>xfce4</li> <li>zotero extension - point to ~/data/literature/zotero in prefs</li> <li>gnumeric</li> <li>abiword</li> <li>libreoffice writer, calc, impress</li> <li>libreoffice-gtk (helps with desktop integration)</li> <li>htop</li> <li>powertop</li> <li>vim-gtk - set prefs with .vimrc (use moolenaar example)</li> <li>google-earth - installed with smxi</li> <li>gimp</li> <li>shotwell</li> <li>rdesktop (remote desktop for windows access)</li> <li>mpd (music player daemon)</li> <li>gmpc (client for music player daemon)</li> <li>hpodder (podcatcher)</li> </ul>"},{"location":"computing/geospatial_tools/","title":"Geospatial data tools","text":"<p>This page is a placeholder for procedures that involve geospatial data. There are a few links. That is all.</p>"},{"location":"computing/geospatial_tools/#general-tools","title":"General tools","text":"<ul> <li>Open Source Geospation Foundation (OSGeo)- provides a portal to many open source geospatial software projects, like:</li> <li>GRASS desktop GIS- geospatial data management and analysis, image processing, graphics/maps production, spatial modeling, and visualization.</li> <li>Qgis- a more user friendly desktop GIS that supports vector, raster, and database formats and offers integration with GRASS.</li> <li>GDAL libraries- translator library for many raster geospatial data formats (Geospatial Data Abstraction Library)</li> <li>OGR libraries- translator library for simple features vector data (nowadays a subset of GDAL).</li> <li>NCO- a collection of utilities that aid manipulation and analysis of gridded scientific data (like climate/circulation data)</li> </ul>"},{"location":"computing/geospatial_tools/#python-tools","title":"Python tools","text":"<ul> <li>GDAL/OGR bindings for Python - the bindings to allow Python to call GDAL and OGR (syntax mostly follows the C++ class definitions)<ul> <li>This site provides some examples of how to use the bindings)</li> </ul> </li> <li>Basemap - a library for 2D mapping (in Anaconda)<ul> <li>NOTE - the current version of Basemap in Anaconda requires an old version of GEOS that causes some GDAL import issues. A workaround is to install basemap (with conda or from the conda-forge package) and let <code>conda</code> downgrade GDAL, libgdal, and GEOS. May need to make symbolic links to any missing dynamic libraries in the correct environment path (See here)</li> </ul> </li> </ul>"},{"location":"computing/git/","title":"Version control with Git","text":"<p>A distributed version control system.</p> <p>See also: Mercurial DVCS</p>"},{"location":"computing/git/#resources","title":"Resources:","text":"<ul> <li>Git docs</li> <li>Atlassian DVCS guide</li> <li>GitMagictutorial</li> <li>git ready- learn git one commit at a time</li> <li>Other documentationlinked at the official git site.</li> </ul>"},{"location":"computing/git/#setting-up-git","title":"Setting up Git","text":"<p>After installation you will want to set up the user name, email, editor/diff preferences, etc. Directions can be found in the git documentation.</p>"},{"location":"computing/git/#basics-of-using-git","title":"Basics of using Git","text":"<p>Most of the basic commands for creating and managing a Git repository are very similar to Mercurial.</p>"},{"location":"computing/ipython/","title":"IPython","text":"<p>Assorted notes on using the ipython interactive shell. Official documentation is at http://ipython.org.</p> <p>See also: General programming, Python notes pages.</p>"},{"location":"computing/ipython/#basics","title":"Basics","text":"<ul> <li> <p>Pylab - Starting ipython with the <code>--pylab</code> option imports         numpy and matplotlib libraries into the workspace.</p> </li> <li> <p>Inline help - Following any object of function name with a         \"?\" gives info or documentation about the item.</p> </li> <li> <p>Shell access - to run a command in the system shell prefix         it with an exclamation point: <code>!ping www.bbc.co.uk</code></p> </li> <li> <p>ipdb - ipython has an enhanced debugger (not sure if this is         enabled by default :!:)</p> </li> </ul>"},{"location":"computing/ipython/#magic-commands","title":"Magic commands","text":"<p>IPython \"magic\" commands operate only within IPython, and are prefaced by %. If the flag %automagic is set, then magic commands can be called without the %. (%automagic is on by default.). These commands include:</p> <ul> <li>Functions that work with code: %run, %edit, %save, %macro, %recall, etc.</li> <li>Functions which affect the shell: %colors, %xmode, %autoindent, etc.</li> <li>Other functions such as %reset, %timeit or %paste.</li> <li>To see all the available magic functions, call %lsmagic.</li> <li>Thislists all the core magic functions.`</li> </ul>"},{"location":"computing/ipython/#run","title":"%run","text":"<p>Run any python script and load all of its data directly into the interactive namespace. Since the file is re-read from disk each time, changes you make to it are reflected immediately (unlike imported modules, which have to be specifically reloaded). %run has special flags for timing the execution of your scripts (-t), or for running them under the control of either Python\u2019s pdb debugger (-d) or profiler (-p).</p> <p>You can step through a program from the beginning by calling <code>%run -d theprogram.py</code></p>"},{"location":"computing/ipython/#who-whos","title":"%who, %whos","text":"<p>Print all interactive variables, with minimal (%who) or extensive (%whos) formatting. If any arguments are given, only variables whose type matches one of these are printed. For example:</p> <pre><code>#This will list functions and strings, excluding all other types of variables\n%who function str\n</code></pre>"},{"location":"computing/ipython/#edit","title":"%edit","text":"<p>Gives a reasonable approximation of multiline editing, by invoking your favorite editor on the spot. IPython will execute the code you type in there as if it were typed interactively.</p>"},{"location":"computing/ipython/#debug-pdb-prun","title":"%debug, %pdb, %prun","text":"<p>After an exception occurs, you can call <code>%debug</code> to jump into the Python debugger (pdb) and examine the problem. Alternatively, if you call <code>%pdb</code>, IPython will automatically start the debugger on any uncaught exception. <code>%prun</code> will run a statement through the python code profiler.</p>"},{"location":"computing/latex_notes/","title":"LaTeX notes","text":"<p>Notes for using Tex and related things like Latex. In practice I access latex through <code>pandoc</code> commands.</p> <p>See also: textfiles, pandoc</p>"},{"location":"computing/latex_notes/#general-resources","title":"General Resources","text":"<ul> <li>TexLive distribution</li> <li>TeX Stackexchange site</li> <li>Short tutorial geared to math students.</li> <li>Harvard math dept. reference</li> <li>A simple LaTeX pre-viewer that can be used for code on this page.</li> <li>An SE question with lots of tips and best practices for LaTeX.</li> </ul>"},{"location":"computing/latex_notes/#installing","title":"Installing","text":""},{"location":"computing/latex_notes/#debian","title":"Debian","text":"<p>The full TexLive (<code>texlive_full</code>) distribution is very big, so installing the subset that includes latex, the <code>texlive</code> package, is probably wise. When using funny unicode symbols when making pdfs with pandoc Xetex (<code>texlive-xetex</code> package) is very helpful</p>"},{"location":"computing/latex_notes/#citations","title":"Citations","text":"<p>Insert a citation with <code>cite{citationID}</code>, where <code>citationID</code> is the identifier in a BibTeX (.bib) bibliography file. The <code>.bib</code> is defined somewhere in the LaTeX file with <code>bibliography{~\\user...\\bibfile.bib}</code>.</p> <p>There are other, improved implementations for citations, such as natbib.</p> <p>Currently there is an error in the Zotero export filter for .bib files (parentheses and brackets are sometimes reversed when abbreviations and special characters are present). Open bibtex in vim and do:</p> <pre><code>:%s/)\\}/\\})/c\n</code></pre>"},{"location":"computing/latex_notes/#citation-resources","title":"Citation Resources","text":"<ul> <li>A natbib reference sheet.</li> <li>Some tips on using Zotero with BibTeX</li> </ul>"},{"location":"computing/latex_notes/#making-tables","title":"Making Tables","text":"<p>It may be easier to make and format the table in a spreadsheet first, and some spreadsheets can export tables directly to LaTeX format. Gnumeric, and Libre Calc (using Calc2LaTeX) do a decent job, but there will still be formatting to make it look nice. Table data can also be pasted into this website to generate latex formatted tables. Once you have some data to format into a LaTeX table, this is the traditional way to do it:</p> <pre><code>\\begin{tabular}{|c|c|c|}\n\\hline\nA &amp; B &amp; C\\\\\n\\hline\\hline\nfoo &amp; bar &amp; baz\\\\\n\\hline\nzab &amp; rab &amp; oof\\\\\n\\hline\n\\end{tabular}\n</code></pre> <p>Depending on the table, a nicer/easier way could be to use the Booktabs package, and do something like this:</p> <pre><code>\\begin{tabular}{ccc}\n\\toprule\nA &amp; B &amp; C\\\\\n\\midrule\nfoo &amp; bar &amp; baz\\\\\nzab &amp; rab &amp; oof\\\\\n\\bottomrule\n\\end{tabular}\n</code></pre>"},{"location":"computing/latex_notes/#mathematical-typesetting","title":"Mathematical typesetting","text":"<p>TeX/LaTeX excels at this. Mathematical statements or formulas are generally placed between dollar signs. Here are a few resources:</p> <ul> <li>http://www.math.uiuc.edu/~hildebr/tex/course/intro2.html</li> </ul>"},{"location":"computing/latex_notes/#comments-todos-and-other-document-annotations","title":"Comments, TODOs, and other document annotations","text":"<p>Comments can be added to a Tex or latex document with the <code>%</code> signifying the beginning of a comment. If a document is reviewed by another and they add these types of comments, they will be picked up by a diff between the original and the patch.</p>"},{"location":"computing/latex_notes/#a-couple-other-ideas","title":"A couple other ideas","text":"<ul> <li>The todonotes package places callouts (Word style) in the compiled document.</li> </ul>"},{"location":"computing/latex_notes/#converting-to-wordlo-formats","title":"Converting to Word/LO formats","text":"<p>Pandoc should do this, or...</p> <ul> <li>This tex.se question has some great ideas and links.</li> <li>There are other ideas in this SE question</li> </ul>"},{"location":"computing/matlab_notes/","title":"Matlab notes","text":"<p>Notes for using Matlab effectively for data analysis. See also: General programming, Sensor data notes pages</p>"},{"location":"computing/matlab_notes/#installation-on-linux","title":"Installation on linux","text":"<p>Matlab installer for linux is pretty straightforward these days. MATLAB can be installed in /usr/local/ unless there is not enough space (3-6 GB depending on toolboxes).</p> <p>To get a desktop launcher in the menu (with icon), do:</p> <pre><code>sudo wget http://upload.wikimedia.org/wikipedia/commons/2/21/Matlab_Logo.png -O /usr/share/icons/matlab.png\n</code></pre> <p>then make a matlab.desktop file in /usr/share/applications/ that points to the icon and executes <code>matlab -desktop</code></p>"},{"location":"computing/matlab_notes/#put-date-ticklabels-on-an-x-axis","title":"Put date ticklabels on an X-axis","text":"<p>When plotting a timeseries it is often hard to tell when the data is from because it is easier to plot against a sequential or decimal day, rather than month, day, year, etc. This puts useful tickmarks and labels on a timeseries plot:</p> <pre><code>% datenum_h is a vector of sequential datenum days - divide up this series into 20 tickmarks\nticklocations = linspace(min(datenum_h), max(datenum_h), 20);\n% plot against the datevec_h sequence \nplot(datenum_h, sm(:, i), 'k');\n% after plotting put the tick marks and labels in the correct locations\nset(gca, 'XTick', ticklocations);\nset(gca, 'XTickLabel', ticklocations);\n% Call the datetick function with 'keepticks' to convert the label on each tick \n location to a date.\ndatetick('x', 12, 'keepticks');\n</code></pre>"},{"location":"computing/mercurial/","title":"Version control with Mercurial","text":"<p>WARNING: This page is out of date</p>"},{"location":"computing/mercurial/#resources","title":"Resources","text":"<ul> <li>Mercurial: The Definitive Guide - an online copy of Sullivan book published by O'Reilly.</li> <li>Mercurial wiki</li> <li>HgInit tutorial</li> </ul>"},{"location":"computing/mercurial/#installing-and-configuring","title":"Installing and configuring","text":"<ul> <li>Install in debian with <code>apt install mercurial</code></li> <li>Create ~/.hgrc and add the following lines:`</li> </ul> <pre><code>[ui]\nusername = Firstname Lastname &lt;username@somewhere.com&gt;\nverbose = True\n</code></pre>"},{"location":"computing/mercurial/#creating-a-repository","title":"Creating a repository","text":"<p>First enter the directory that needs version control and initialize the repository</p> <pre><code>cd ~/projectdirectory\nhg init\n</code></pre> <p>This will create the .hg directory containing the repository in <code>~/projectdirectory</code></p> <p>Next, add the files in the <code>~/projectdirectory</code> to the repository</p> <pre><code>hg add  #schedule all files in ~/projectdirectory to be added to the repository\nhg add file1, file2  #or add files to the repository individually`\n</code></pre> <p>The queue of files added to the repository can be viewed with <code>hg status</code>.</p> <p>Then, commit the initial version of these files</p> <pre><code>hg commit  #this will pop up an editor for you to add a commit description\nhg commit -m \"This is the initial commit of these files\"  # or add the message directly from the commandline`\n</code></pre> <p>Use <code>hg log</code> to view a history of commits to the repository (most recent commits are at the top).</p>"},{"location":"computing/mercurial/#make-and-commit-changes","title":"Make and commit changes","text":"<p>Commits should occur whenever substantial changes are made to a file. This would include new features, creation of new scripts or functions, changes in data sources, or other substantial changes in the function or output of data analysis code.</p> <p>Once changes are made to a file in the directory, <code>hg status</code> will show which files are modified</p> <pre><code>greg@gm-thinkpad:~/data/data_analysis/project1$`vim`m/file1.m`\ngreg@gm-thinkpad:~/data/data_analysis/project1$`hg`status`M`\nm/file1.m\n</code></pre> <p>The M at the left indicates that the file has been modified. Other possible markers here are \"!\", meaning the file is missing, or \"?\", meaning the file is unknown (is newly created or not added).</p> <p>When these files are commited, <code>hg log</code> will display the changes</p> <pre><code>greg@gm-thinkpad:~/data/data_analysis/project1$ hg commit\n-m 'Added some new stuff to file1.m' m/file1.m committed changeset\n1:0048e1a6cd8b greg@gm-thinkpad:~/data/data_analysis/project1$ hg log\nchangeset: 1:0048e1a6cd8b tag: tip user: Firstname Lastname\n&lt;username@somewhere.com&gt; date: Thu Dec 29 14:35:06 2011 -0700 files:\nm/file1.m description: Added some new stuff to file1.m\n\nchangeset: 0:d0eea9c4d2d5 user: Firstname Lastname\n&lt;username@somewhere.com&gt; date: Wed Dec 28 14:10:55 2011 -0700 files:\nm/file1.m m/file2.m otherdatadir description: Initial commit of files\n</code></pre>"},{"location":"computing/mercurial/#tracking-removing-renaming-or-ignoring-files","title":"Tracking, removing, renaming, or ignoring files","text":"<p>hg add //file1//</p> <ul> <li>Adds a new file to the repository On the next commit this file will be added to the repository and tracked from then on. `</li> </ul> <p>hg remove //file1//</p> <ul> <li>Will delete the file and cause it to stop being tracked by mercurial. If a file has been deleted without using <code>hg</code> <code>remove</code> the file will be considered missing (has a ! under <code>hg</code> <code>status</code>) by mercurial. Removing files does not remove their history, so they will reappear if you return to a changeset where they were still tracked. If you remove a file without <code>hg</code> <code>remove</code> and <code>hg</code> <code>status</code> reports it as missing, you can run <code>hg</code>remove<code></code>--after<code>to tell Mercurial about it after the fact.</code></li> </ul> <p>hg addremove</p> <ul> <li>This is a combination command that adds untracked files and marks missing files as removed.`</li> </ul> <p>hg copy //file1 new_file1//</p> <ul> <li>Makes a new copy of a file. When you copy a file using this command, Mercurial makes a record of the fact that the new file is a copy of the original file. It treats these copied files specially when you merge your work with someone else\u2019s.`</li> </ul> <p>hg rename //file1 file2//</p> <ul> <li>Mercurial makes a copy of each source file, then deletes the original and and marks it as removed. Using <code>hg</code> <code>status</code> <code>-C</code> will show the origin of the renamed file (since deleted). You can also tell Mercurial about a rename after the fact with <code>hg</code> <code>rename</code>--after<code>.</code></li> </ul>"},{"location":"computing/mercurial/#the-hgignore-file","title":"The .hgignore file","text":"<p>The working directory of a Mercurial repository will often contain files that shouldn't be tracked (backup copies, binary files, etc). These files can be listed in a plain textfile within that directory called .hgignore, and mercurial will ignore these files.</p> <p>An example .hgignore file: <code> <ol> <li>use glob syntax (Shell style).</li> </ol> <p>syntax: glob</p> <ul> <li>.elc</li> <li>.pyc</li> <li> <p>*~</p> </li> <li> <p>switch to regexp syntax (Regular expression as in Perl/Python).</p> </li> </ul> <p>syntax: regexp \\^\\.pc/ ~~~</p>"},{"location":"computing/mercurial/#viewing-or-changing-between-previous-revisions","title":"Viewing or changing between previous revisions","text":""},{"location":"computing/mercurial/#revert","title":"Revert","text":"<p>The command hg revert will revert the directory back to the version at the last commit. This will restore any files that were deleted, and will restore changed files to their state at the last commit. When files are reverted, the current working version will be resaved with a .orig extension. <code>greg@gm-thinkpad:~/data/data_analysis/project1$</code>vim<code>m/file1.m</code>#<code></code>edit<code>the</code>file<code>greg@gm-thinkpad:~/data/data_analysis/project1$</code> <code>hg</code>revert<code>--all</code>saving<code>current</code>version<code>of</code>m/file1.m<code>as</code> <code>m/file1.m.orig</code>#<code>edits</code>since<code>the</code>last<code>commit</code>will<code>be</code> <code>saved</code>in<code>the</code>.orig<code>version</code>reverting<code>m/file1.m</code>#<code></code>file1.m<code>goes</code>back<code>to</code>its<code>previous</code>stateNote that it is also possible to revert individual files (change --all to desired filename).</p>"},{"location":"computing/mercurial/#update","title":"Update","text":"<p>The hg update command allows movement between the various revisions of the directory. Revisions are easily viewed in the log as the first number of the changeset. ~~~ greg@gm-thinkpad:~/data/data_analysis/project1$ hg update -r 0 resolving manifests getting m/file1.m 1 files updated, 0 files merged, 0 files removed, 0 files unresolved</p> <ol> <li>this command will update the directory to its state at the     original commit. Only m/file1.m will change</li> <li></li> </ol> <p>hg update -r 1 # takes us back to the current commit version (see log) - hg update without arguments will also do this ~~~</p>"},{"location":"computing/mercurial/#rollback","title":"Rollback","text":"<p>The hg rollback command has the capability to undo one commit, as long as that commit hasn't been pushed to anyone else.</p>"},{"location":"computing/mercurial/#examine-changes","title":"Examine changes","text":"<p>Specific changes between revisions can be viewed with the diff command, or by serving up the repository's history on mercurial's built in webserver.</p>"},{"location":"computing/mercurial/#using-diff","title":"Using diff","text":"<p>~~~ greg@gm-thinkpad:~/data/data_analysis/project1$ hg diff -r 0 m/file1.m # -r 0 means compare current version to original commit version diff -r d0eea9c4d2d5 m/file1.m --- a/m/file1.m Wed Dec 28 14:10:55 2011 -0700 +++ b/m/file1.m Thu Dec 29 16:14:30 2011 -0700 @@ -4,6 +4,9 @@</p> <p><code>%</code></p> <p>-% data is from CNisotopedata files in rawdata folder. # this line removed -% # and this one + # this and the next 2 lines added +I added this line +</p> <p><code>clear;          % clear memory</code>close all;      % clear any figures <code>fignum=0;       % used to increment figure number for plots</code></p> <pre><code>\n### Use an external diff program\n\nMake sure *extdiff* extension is enabled in *~/.hgrc*, then:\n\nhg extdiff -p vimdiff -r 63 processed_data/wyear_climatesummary.txt`\n\n### The webserver\n\n~~~ greg@gm-thinkpad:~/data/data_analysis/project1$ hg serve\nlistening at &lt;http://gm-thinkpad.pronghorns.net:8000/&gt; (bound to\n\\*:8000)\n\n1.  directing a webserver to this\n    address (http://gm-thinkpad.pronghorns.net:8000/) will show a web\n    interface where all revisions\n2.  visible in different formats (diff, changeset, etc)\n\n</code></pre>"},{"location":"computing/mercurial/#working-with-multiple-repositories","title":"Working with multiple repositories","text":"<p>There are several use-cases in which the following commands are useful:</p> <ul> <li>A team of developers working on the same project must share changes between their local repositories and a central repository.</li> <li>In solo development it may be useful to make an experimental clone of the repository for testing changes. If successful, these changes can be pushed back to the main.`</li> </ul>"},{"location":"computing/mercurial/#commands","title":"Commands","text":"<ul> <li>hg clone makes a copy of an entire repository (including all files and directory structures).</li> <li>Results in a testing branch for solo development, or local working copies of a team project.</li> <li>hg outgoing lists changes in current repository waiting to be pushed</li> <li>hg incoming lists changes in remote repository waiting to be pulled (new changesets in central repo since clone, or last pull)</li> <li>hg paths shows a list of all known remote repositories, including a default for pushing changesets.</li> <li>hg push will push new changes from one repository to another.</li> <li>This could be a central or local repository used to clone the current working repository</li> <li>hg pull pulls in changesets from a central or local repository (the parent of the current one)</li> <li>hg parent shows which changeset(s) you are working off of in your current repository.</li> <li>hg merge merges the changes in two heads (ie. when there are two competing changesets added to the repository)</li> </ul>"},{"location":"computing/mercurial/#example-1","title":"Example 1","text":"<p>In this example, a solo developer makes a clone of a project repository to test changes. Some changes are then pushed back to the original repository, while some remain in the testing branch, which will have some kind of different functionality.</p> <p><code>greg@gm-thinkpad:~/data/data_analysis/project1$</code>hg<code>clone</code>.<code></code>../project1_testing<code>#</code>clone<code>the</code>current<code>repository</code>into<code></code>a<code>testing</code>repository<code>updating</code>to<code>branch</code>default<code></code>resolving<code>manifests</code>getting<code>datadir</code>getting<code>m/file1.m</code> <code>getting</code>m/file2.m<code>getting</code>otherdatadir<code>4</code>files<code>updated,</code> <code>0</code>files<code>merged,</code>0<code>files</code>removed,<code>0</code>files<code>unresolved</code> <code>greg@gm-thinkpad:~/data/data_analysis/project1$</code>ls<code>../</code> <code>hiddencanyon</code>SNOTELdata<code>project1</code>project1_testing<code>#</code>there<code></code>is<code>now</code>another<code>repository</code>(project1_testing)alongside<code>the</code> <code>original</code>greg@gm-thinkpad:~/data/data_analysis$<code>cd</code> <code>../project1_testing/</code> <code>greg@gm-thinkpad:~/data/data_analysis/project1_testing$</code>ls<code>#</code>it<code></code>is<code>identical</code>to<code>the</code>original<code>m</code>datadir`otherdatadirNow, changes can be freely made in the testing branch, and those that work can be pushed back to the original. ~~~ greg@gm-thinkpad:~/data/data_analysis/project1_testing$ vim m/file1.m greg@gm-thinkpad:~/data/data_analysis/project1_testing$ hg st M m/file1.m greg@gm-thinkpad:~/data/data_analysis/project1_testing$ hg diff diff -r ed43a5a8e664 m/file1.m --- a/m/plot_SNOTELsurvey_CN.m Fri Dec 30 06:46:04 2011 -0700 +++ b/m/plot_SNOTELsurvey_CN.m Fri Dec 30 07:02:05 2011 -0700 @@ -3,6 +3,9 @@</p> <p><code>% Plots of C, N, and isotope data for SNOTEL survey sites</code>% <code>% Uses isotope data in soildata/CNisotopedata15sites.csv</code></p> <ul> <li>+This is my new testing code +it does cool stuff</li> </ul> <p><code>clear;          % clear memory</code>close all;      % clear any figures`</p> <p>greg@gm-thinkpad:~/data/data_analysis/project1_testing$ hg commit -m \"added a cool new feature\" m/file1.m committed changeset 5:569e890e2f49 greg@gm-thinkpad:~/data/data_analysis/project1_testing$ hg outgoing comparing with /home/greg/data/data_analysis/project1 searching for changes changeset: 5:569e890e2f49 tag: tip user: Greg Maurer greg@pronghorns.net date: Fri Dec 30 07:02:25 2011 -0700 files: m/file1.m description: added a cool new feature</p> <p>greg@gm-thinkpad:~/data/data_analysis/project1_testing$ hg push pushing to /home/greg/data/data_analysis/project1 searching for changes 1 changesets found adding changesets adding manifests adding file changes added 1 changesets with 1 changes to 1 files ~~~</p> <p>If, in the meantime, commits have been made in the original repository, this push will result in two \"heads\" to the repository. The changesets in the original repo and the \"testing\" changeset that has just been pushed will have to be merged.</p>"},{"location":"computing/mercurial/#example-2","title":"Example 2","text":"<p>In this example clones of a central repository are made by a team. Mercurial helps to exchange and merge changes that are made between the various local repos and the central repository.</p> <p>Covered here</p>"},{"location":"computing/mercurial/#pushing-to-a-git-repository-github","title":"Pushing to a Git repository (GitHub)","text":"<p>The hg-git plugin is needed for this.</p> <pre><code>$ easy_install hg-git   #first install hg-git\n</code></pre> <p>Create a repository on GitHub and save an SSH Key there using these instructions</p> <pre><code>$ cd repositoryname # (a Mercurial repository)\n$ hg bookmark -r default master # make a bookmark of master for default, so a ref gets created\n$ hg push git+ssh://git@github.com/gremau/repositoryname.git\n$ hg push\n</code></pre>"},{"location":"computing/numpy_notes/","title":"NumPy notes","text":"<p>NumPy gives array classes and other tools useful in data analysis. Official documentation is here.</p> <p>See also: Python notes, General programming pages.</p>"},{"location":"computing/numpy_notes/#datetime-objects-and-arrays","title":"Date/time objects and arrays","text":""},{"location":"computing/numpy_notes/#python-datetime-module","title":"Python datetime module","text":"<p>Python's datetime module suplies the core functionality for handling datetime objects. Descriptions of its core classes, and their attribute and methods are briefly covered here, or see Python's more extensive Datetime library reference.</p>"},{"location":"computing/numpy_notes/#inputoutput-datetime-arrays-with-numpy","title":"Input/output datetime arrays with NumPy","text":"<p>The datetime.strptime() class method creates a datetime object from a string representing a date and time and a corresponding format string. date.strftime(format) returns a string representing the date, controlled by an explicit format string. To format these inputs and outputs use the \"%field\" formatting codes. A list of the codes is here. When reading a datafile, a column of formatted dates can be read in in the following way. </p> <pre><code># First define a function that converts the formatted date to a datetime object\ndef datestr2num(s):\n    return datetime.datetime.strptime(s, \"%Y/%m/%d\")\n\n#then, this function can be passed as a converter argument in a genfromtxt() call\ndatetxt = genfromtxt(datapath + 'swe.txt', skiprows=2, delimiter=',',\n                     usecols=(0,), converters={0: datestr2num})\n</code></pre> <p>:!: cant seem to get the same thing to work with loadtxt for some     reason</p>"},{"location":"computing/numpy_notes/#datetimes-in-numpy-itself","title":"Datetimes in NumPy itself","text":"<p>New in NumPy 1.7 is a core datatype for datetime objects called <code>datetime64</code>. It should implement similar functionality as above, but a little more automatically - seehere.</p>"},{"location":"computing/numpy_notes/#masking-invalidmissing-values-in-arrays","title":"Masking invalid/missing values in arrays","text":"<p>One of the biggest tasks in getting data ready for analysis is identifying and \"masking\" data values that are missing or invalid for some reason. These values can be handled in Numpy arrays using a number of approaches.</p>"},{"location":"computing/numpy_notes/#index-arrays","title":"Index arrays","text":"<p>Numpy arrays can always be indexed with other NumPy arrays (as long as they have valid indices). If the index number of bad or missing data is known, the array can always be indexed using another numpy array that excludes the indices of invalid data.</p> <pre><code>In [0]: arange(10,1,-1)\n\nOut[0]: array([10, 9, 8, 7, 6, 5, 4, 3, 2])\n\nIn [1]: x = arange(10,1,-1)\n\nIn [2]: x\n\nOut[2]: array([10, 9, 8, 7, 6, 5, 4, 3, 2])\n\nIn [3]: x[array([0, 1, 2, 3, 4, 5])]\n\nOut[3]: array([10, 9, 8, 7, 6, 5])\n</code></pre> <p>Boolean arrays can also be used to index data arrays. A boolean array of the same dimension as the data array is created by applying a logical statement. This boolean array can then be used as an index.</p> <pre><code>In [4]: test = x&gt;=5\n\nIn [5]: test\n\nOut[5]: array([ True, True, True, True, True, True, False, False, False], dtype=bool)\n\nIn [6]: x[test]\n\nOut[6]: array([10, 9, 8, 7, 6, 5])\n</code></pre> <p>More on using boolean index arrays here</p>"},{"location":"computing/numpy_notes/#nan-values","title":"Nan values","text":"<p>Adding Nan to an array can mask or mark invalid values, and these values can then be left out of calculations in various ways.</p> <pre><code>#change bad decagon sm sensor data (-6999) to nan\ntest = (m == -6999) m[test] = nan\n</code></pre> <p>isnan can be used to locate Nan values in an array by creating a boolean array of the same dimensions. True values are Nans.</p> <pre><code>In [0]: b = arange(10.)\n\nIn [1]: b[2] = nan\n\nIn [2]: b\n\nOut[2]: array([ 0., 1., nan, 3., 4., 5., 6., 7., 8., 9.])\n\nIn [3]: isnan(b)\n\nOut[3]: array([False, False, True, False, False, False, False, False, False, False], dtype=bool)\n</code></pre> <p>It is important to note that in most calculations on an an array containing Nan values, the Nan value will be propagated to the result. This can be avoided by using boolean arrays (such as from isnan) to remove Nan values from the calculation. There are also a number of functions that will perform operations on an array while leaving out nan values (nansum(), nanmax(), nanmin(), etc.).</p> <pre><code>In [4]: sum(b)\n\nOut[4]: nan # should be 43, but the nan propagates to the answer\n\nIn [5]: nansum(b) # this leaves out the nan \n\nOut[5]: 43.0\n\nIn [6]: sum(b[~isnan(b)]) # this also leaves out the nan\n\nOut[6]: 43.0\n</code></pre>"},{"location":"computing/numpy_notes/#masked-arrays","title":"Masked Arrays","text":"<p>Masked arrays are a separate module that must be imported (numpy.ma). This module allows creation of masked arrays that consist of an ndarray and a boolean \"mask\" array of the same dimensions. Data (in the ndarray) that is marked with a False value in the boolean mask are considered valid, while True values denote a missing or invalid value. Operations on the masked array (np.sum, np.mean, etc) don't take the invalid data into account. In this sense it is similar to using a logical test array to mask values in Matlab, but once the mask is created, the masked array can be used like a normal array (without continual explicit use of the mask).</p> <p>~~~{.python} </p> <p>In [7]: import numpy.ma as ma</p> <p>In [8]: b # start with an array containing one nan - see lines 0 and 1 in section above </p> <p>Out[8]: array([ 0., 1., nan, 3., 4., 5., 6., 7., 8., 9.])</p> <p>In [9]: c = ma.array(b, mask=isnan(b))</p> <p>In [10]: c</p> <p>Out[10]: masked_array(data = [0.0 1.0 -- 3.0 4.0 5.0 6.0 7.0 8.0 9.0],</p> <pre><code>mask = [False False  True False False False False False False False], fill_value = 1e+20)\n</code></pre> <pre><code>\nOnce the masked array is created, the data and the mask are both\naccessible using the built in attributes **.data** and\n**.mask**. **ANY** functions and methods that operate on arrays\noperate the same on masked arrays, but the masked values are left out of\nthe calculation.\n\n~~~{.python}\nIn [11]: c.data\n\nOut[11]: array([ 0., 1., nan, 3., 4., 5., 6., 7., 8., 9.])\n\nIn [12]: c.mask\n\nOut[12]: array([False, False, True, False, False,\nFalse, False, False, False, False], dtype=bool)\n\nIn [13]: c.sum()\n\nOut[13]: 43.0\n\nIn [14]: mean(c)\n\nOut[14]: 4.7777777777777777\n\nIn [15]: sum(c.data) \n\nOut[15]: nan\n</code></pre> <p>There are other ways to construct masked arrays, notably <code>masked_where</code> and its aliases (<code>masked_greater</code>, <code>masked_equal</code>, <code>masked_inside</code>, etc). In addition, the masked or unmasked data can be accessed using the mask itself (or <code>!mask</code>). </p> <pre><code>In [16]: d = ma.masked_where(a&gt;=5, b)\n\nIn [17]: d\n\nOut[17]: masked_array(data = [0.0 1.0 -- 3.0 4.0 -- -- -- -- --],\n\n           mask = [False False  True False False  True  True  True  True  True],\n     fill_value = 1e+20)\n\nIn [18]: d[~d.mask]\n\nOut[18]: masked_array(data = [0.0 1.0 3.0 4.0],\n\n    mask = [False False False False],fill_value = 1e+20)\n\n</code></pre> <p>Other things to note:</p> <p>The mask argument in ma.array must be convertible to a boolean array of the same size as the input data array. Adding the <code>fill_value=x</code> argument will fill in masked values with x. Other arguments for constructing arrays can be found ma.array function description. As a subclass of the numpy.ndarray class, the ma.MaskedArray class inherits all its attributes and methods, plus adds a few others (shown here).</p> <p>Lots more info on working with masked arrays: Masked arrays in the NumPy Reference</p>"},{"location":"computing/numpy_notes/#na-masked-arrays","title":"NA-Masked arrays","text":"<p>There is a new NA-masked array introduced in Numpy 1.7 that puts NA-masking directly in the core (instead of a separate module).</p> <p>Details here.</p>"},{"location":"computing/pandoc/","title":"Pandoc","text":"<p>Pandoc is a utility to convert text markup formats (Markdown, reStructuredText, LaTeX, etc) to a variety of other document formats (.doc, .pdf, HTML, etc). Pandoc also provides its own extension of Markdown syntax (details here).</p> <p>Be sure to install:</p> <ul> <li>pandoc</li> <li>pandoc-data</li> <li>pandoc-citeproc (for citations)</li> </ul>"},{"location":"computing/pandoc/#basic-use-for-creating-pdfs","title":"Basic use for creating PDFs","text":"<p><code>pandoc -o</code> writes output to the file following the <code>-o</code>, with formatting based on the file extension. For example, the following command creates a pdf file from a text file (via pdflatex):</p> <pre><code>pandoc cv_master.txt -o cv_master.pdf\n</code></pre> <p>There are other options for specifying input and output files or formats, but this is the simplest.</p>"},{"location":"computing/pandoc/#pdf-formatting-options","title":"PDF formatting options","text":"<p>The output formatting of a pdf document can be set either at the commandline, or by using a header or template file during the conversion.</p>"},{"location":"computing/pandoc/#passing-variables","title":"Passing variables","text":"<p>The <code>-V</code> (or <code>--variable</code>) option changes formatting, fonts, or other settings during a pandoc conversion.</p> <pre><code>pandoc cover_master.txt -V geometry:margin=1.5in -o  cover_master.pdf\n# create pdf with 1.5in margins\n</code></pre> <p>Other variables that can be passed with <code>-V</code> are here</p> <p>Note that some of these variables, such as geometry, can also be passed in a YAML metadata block. See here for details.</p>"},{"location":"computing/pandoc/#include-in-header","title":"Include in header","text":"<p>When <code>-s</code> (or <code>--standalone</code>) is used, pandoc adds the header and footers needed for a standalone document in the output format. To see the default template used, type <code>pandoc -D FORMAT</code>. There are a couple of ways to change what is in this header. The <code>-H=FILE</code> (<code>--include-in-header=FILE</code>) option can include special formatting information contained in FILE with the default header. This can be useful when adding LaTeX formatting commands to the intermediate .tex file before creating the pdf.</p> <pre><code>pandoc cv_master.txt -H '/path/to/header/' -o cv_master.pdf\n</code></pre>"},{"location":"computing/pandoc/#custom-document-templates","title":"Custom document templates","text":"<p>A custom template for the output document header can also be specified using <code>--template=FILE</code>, where FILE is the custom template. If this option is specified, pandoc first looks in the current directory, then the user template directory (<code>$HOME/.pandoc/templates</code>), and then in the default template directory (<code>/usr/share/pandoc...</code>). Templates should end with an extension for the output format (.latex, .html, etc). To create a new template, copy the default for the given output format using:</p> <pre><code>pandoc -D latex &gt; newtemplate.latex\n</code></pre> <p>This is good practice when starting a new project because default templates are updated in new versions of pandoc. The template can then be modified and then used in converting the document using:</p> <pre><code>pandoc manuscript_1.markdown --template=newtemplate.latex --bibliography=SNOTELsoildata.bib -o manuscript_1.pdf\n</code></pre> <p>You can put useful stuff in these templates, like setting margin widths, using packages, etc. A couple I use for pdfs of journal articles are:</p> <pre><code>\\usepackage[margin=1in]{geometry}\n\\usepackage{textcomp} % provides \\textdegree\n</code></pre>"},{"location":"computing/pandoc/#other-formatting","title":"Other formatting","text":"<pre><code>             Option | Result\n</code></pre> <p>----------------------- | ----------------------------------------------------------------------------------------------------------- <code>--toc</code>                 | Automatically create a table of contents <code>--toc-depth</code>           | Specify the header levels to be used in table of contents (implies --toc) <code>--reference-odt=FILE</code>  | Use the FILE stylesheets as a template for .odt output. Best if FILE was created with pandoc, then modified. <code>--reference-docx=FILE</code> | Use the FILE stylesheets as a template for .docx output. <code>--latex-engine=ENGINE</code> | Choose the pdflatex|lualatex|xelatex interpreters, needed for some formatting in pdf files.</p>"},{"location":"computing/pandoc/#citations-and-bibliographies","title":"Citations and bibliographies","text":"<p>Pandoc is capable of including citations from an associated bibliographic database (usually a BibTex file). Citations in a pandoc markdown file look like this: <code>[@citationid]</code> , where <code>citationid</code> is defined in the associated .bib file. To convert a pandoc file with citations, run:</p> <pre><code>pandoc paper1_draft.markdown -o paper1_draft.pdf --bibliography hiddencanyon.bib\n</code></pre> <p>A bibliography will be automatically written after the References heading, if it is included. Citation and bibliography formatting can be specified with <code>--csl=FILE</code>, where <code>FILE</code> is a .csl file (found at http://citationstyles.org). Natbib and biblatex can be used in LaTeX output (pdf) by including them as commandline options.</p>"},{"location":"computing/pandoc/#working-directly-with-latex","title":"Working directly with LaTeX","text":"<p>Pandoc markdown is a nice way to draft LaTeX documents. Pandoc markdown can be rendered to TeX (<code>-o document.tex</code>), or rendered as a PDF via <code>pdflatex</code> (<code>-o document.pdf</code>). Raw TeX and LaTeX can be included in a markdown document and it will be passed to the pdflatex writer. For more info on LaTeX/Tex systems see this page.</p>"},{"location":"computing/pandoc/#citations","title":"Citations","text":"<pre><code>This is a citation from Smith \\cite{smith.2013}\n</code></pre> <p>The citation should be output in BibTex format FIXME// - havent gotten this to work yet//. Inline TeX placed between \\begin and \\end tags will be interpreted as LaTeX instead of markdown, and will be ignored in non-LaTeX output formats.</p>"},{"location":"computing/pandoc/#math-mode","title":"Math mode","text":"<p>TeX math can be used by putting it between dollar signs. One dollar sign (each side) for inline mode, and two for display math. Most LaTeX math mode symbols are also transferred to other output formats. For example, rendering HTML documents with greek characters (such as $\\theta$) will result in unicode (default) greek characters in the rendered HTML. There are also options for using MathML, MathJax, if this doesn't work.</p>"},{"location":"computing/pandoc/#errors-with-unicode-special-characters","title":"Errors with unicode special characters","text":"<p>Pandoc will pass unicode characters in a document to <code>pdflatex</code> that it may  not know how to display. This is pretty common with greek characters that are  used outside of math mode. In this case it is probably best to use the Xetex interpreter instead. This can be specified by sending <code>--latex-engine=xelatex</code> to pandoc. Of course, Xetex must be installed, which is most easily done by installing the full TexLive distribution (large).</p>"},{"location":"computing/programming/","title":"Data analysis programming: general info","text":"<p>This page collects info and links (to this site or others) useful in data analysis programming, including resources for various languages, editors, or testing tools, and notes/tips for using them effectively.</p>"},{"location":"computing/programming/#general-techniques-and-conventions","title":"General techniques and conventions","text":"<p>Notes about data analysis techniques/conventions, independent of language/interface.</p> <ul> <li>Sensor data notes on working with continuous sensor timeseries (from dataloggers, SNOTEL sites, etc.)</li> <li>Data analysis workflow - Notes on collecting, storing, and moving data through the analysis process.</li> </ul>"},{"location":"computing/programming/#text-editing-and-data-file-handling","title":"Text editing and data file handling","text":"<p>VIM is a great text editor. Below are a few resources on using it effectively.</p> <ul> <li>A fairly complete Vim commands cheatsheet.</li> <li>The Vim tips wiki</li> <li>Seven Habits for effective text editing.</li> <li>My Vim notes</li> </ul> <p>An excellent general overview of text/data file handling in  a Unix environment is provided by Unix for Poets, by Kenneth Ward Church. PDFs of this are all over the internet.</p>"},{"location":"computing/programming/#other-useful-resources-including-some-on-this-wiki","title":"Other useful resources (including some on this wiki)","text":"<ul> <li>My textfile notes - various command-line ways of manipulating text.</li> <li>My shell scripting notes, including Unix shell scripts and useful utilities.</li> <li>BASH hackers site is helpful.</li> <li>Shell scripting tutorial by Greg Goebel/Public Domain</li> <li>sed is a text stream editor great for pattern matching and replacing<ul> <li>See this tutorial</li> <li>This page gives great one-line examples.\\</li> </ul> </li> <li>Awk is also very useful for manipulating text files.<ul> <li>My awk notes</li> <li>The awk gateway</li> <li>Awk one-liners explained part one</li> <li>Awk one-liners explained part two</li> <li>Awk tutorial by Greg Goebel/Public Domain</li> </ul> </li> </ul>"},{"location":"computing/programming/#python","title":"Python","text":"<p>Python is a high-level, open-source programming language that, when combined with some numerical, scientific, and plotting packages, makes a very powerful tool for scientific computing and data analysis (on par with Matlab). Useful Python extensions for scientific computing are:</p> <ul> <li>NumPy - provides n-dimensional array objects and other useful numeric extensions to Python</li> <li>SciPy - provides a number of high-level mathematical tools for use in scientific computing (integration, optimization, fourier transforms...etc</li> <li>Matplotlib - a plotting library that provides publication quality plots and plotting routines that are similar to Matlab's.</li> <li>IPython - an interactive shell that is designed to work well with NumPy, SciPy, and Matplotlib.</li> <li>SciKits - add on toolkits that complement SciPy (various statistical models, timeseries analysis, machine-learning, image processing, etc.</li> <li>The pandas library - provides high-performance, easy-to-use data structures (like data frames) and data analysis tools that sit on top of NumPy. </li> </ul>"},{"location":"computing/programming/#official-python-resources","title":"Official Python resources","text":"<ul> <li>The Python documentation page including tutorials and HowTo's</li> <li>Python Language Reference - describing the syntax and core semantics of the language.</li> <li>Python Standard Library - describing the standard library (modules, functions, etc) distributed with Python.</li> <li>Coding in python should follow the Python Style Guide.</li> <li>Official NumPy/SciPy documentation<ul> <li>Additional NumPy/SciPy documentation</li> </ul> </li> <li>PyPlot documentation for the Matlab-like plotting framework in matplotlib.</li> <li>Python package index - an index of many add-on tools discussed in this wiki.</li> </ul>"},{"location":"computing/programming/#python-forums","title":"Python forums","text":"<p>Python (and its scientific extensions) have a large user/developer community supporting them. These are some forums that might be helpful:</p> <ul> <li>http://www.scipy.org/Mailing_Lists</li> <li>http://old.nabble.com/Scipy-User-f33045.html</li> </ul>"},{"location":"computing/programming/#my-python-notes","title":"My Python notes","text":"<p>Collected notes, tips, and tricks for using any of the Python tools above.</p> <ul> <li>General Python notes on debugging, code structure, and other aspects of development.</li> <li>Ipython</li> <li>NumPy notes - Various notes on using the NumPy package.</li> </ul>"},{"location":"computing/programming/#other","title":"Other","text":"<ul> <li>The Python Wiki Vim page and this blog entry give some interesting tips about using vim as a python source editor.</li> </ul>"},{"location":"computing/programming/#matlab-and-clones","title":"MATLAB (and clones)","text":"<p>MATLAB is a proprietary programming language and IDE that is widely used in scientific and engineering computing.</p>"},{"location":"computing/programming/#resources","title":"Resources","text":"<ul> <li>Official MathWorks documentation</li> <li>Function reference</li> <li>MATLAB Central - the official user/developer community, including a file exchange.</li> <li>Kluid forums has matlab and octave forums.</li> <li>My MATLAB notes</li> </ul>"},{"location":"computing/programming/#clones-of-matlab","title":"Clones of Matlab","text":"<p>There are a bunch of free/open-source clones of Matlab that have various levels of syntax compatibility.</p> <ul> <li>GNU Octave - generally very compatible with Matlab, though some functions are missing.\\</li> <li>SciLab</li> <li>FreeMat</li> </ul>"},{"location":"computing/programming/#r","title":"R","text":"<p>R is a free, open-source software environment for statistical computing and graphics.</p> <ul> <li>R-project homepage</li> <li>R manuals</li> <li>R wiki</li> <li>knitr - a nice report generating engine for R</li> <li>My R notes</li> </ul>"},{"location":"computing/programming/#math-and-stats-tools","title":"Math and Stats tools","text":"<p>Many toolboxes are available, either standalone or in Python, R, and Matlab, for math and statistical applications. See the math toolbox page page.</p>"},{"location":"computing/programming/#testing-data-analysis-functions","title":"Testing data analysis functions","text":"<p>Code used in data analysis can perform fairly complex operations on datasets and generate output that may be significantly changed from the original data. The code itself can also be fairly complex and its actual function may be difficult to discern just by reading the code or looking at the data. It is important to verify that the result of running this code is what is expected and that the output is accurate. Writing test functions that call data analysis code and analyze their output is a useful way to do this.</p>"},{"location":"computing/python_notes/","title":"Python tips","text":"<p>Python is a very powerful, high-level programming language that, when combined with some numerical, scientific, and plotting packages, makes a very powerful tool for scientific computing and data analysis (similar to Matlab). Useful python extensions for this application are NumPy, SciPy, Matplotlib, IPython, and SciKits. This page collects some general tips on developing Python code.</p> <p>See also: NumPy notes, the IPython page, and the Python section of the general programming page.</p>"},{"location":"computing/python_notes/#installing-python-packages","title":"Installing Python packages","text":"<p>Download miniconda from here: https://conda.io/miniconda.html and run with</p> <pre><code>bash &lt;miniconda-file-name&gt;\n</code></pre> <p>Conda is the package manager and <code>conda install &lt;package-name&gt; will install packages. To keep conda install small by cleaning out tarballs and old packages use</code>conda clean -tp`.</p> <p>The anaconda distribution also includes pandoc, which plays well with the system Tex distribution.</p> <p>The main Python package repository is PyPI (the Python Package Index) and installation from PyPI is generally done using <code>pip</code></p>"},{"location":"computing/python_notes/#the-datetime-module","title":"The datetime module","text":"<p>Python's datetime module suplies the core functionality for handling datetime objects. Descriptions of its core classes, and their attribute and methods are briefly covered below, or see Python's more extensive Datetime library reference.</p> <ul> <li><code>dt.date(year, month, day)</code></li> <li><code>dt.date.min</code> and <code>dt.date.max</code> give the minimum and maximum possible dates</li> <li><code>dt.date.year</code>, <code>dt.date.month</code> and <code>dt.date.day</code> return the attributes</li> <li><code>date.strftime(format)</code> - returns a string representing the date, controlled by an explicit format string</li> <li><code>date.toordinal()</code> - returns the proleptic Gregorian ordinal of the date, where January 1 of year 1 has ordinal 1.</li> <li><code>dt.time(hour[,minute[,second[,microsecond[,tzinfo]]]])</code></li> <li>all the attributes are optional and the <code>tzinfo</code> attribute (which defines timezone, time offsets, etc) can be \"None\"</li> <li>similar methods to those in <code>dt.date</code> </li> <li><code>dt.datetime(year,month,day[,hour[,minute[,second[,microsecond[,tzinfo]]]]])</code></li> <li>Contains all the info from a <code>date</code> and a <code>time</code> object.</li> <li>year, month, day are required, tzinfo can be 'None'.</li> <li>two class method contstructors are important: </li> <li><code>datetime.combine(date, time)</code> - combines a date and a time object into a datetime.</li> <li><code>dt.datetime.strptime(date_string,format)</code> - returns a datetime corresponding to date_string, parsed according to format.</li> <li>Similar instance methods to <code>date</code> and <code>time</code> objects (can call strftime, toordinal, etc.)</li> <li><code>dt.timedelta([days[,seconds[,microseconds[,milliseconds[,minutes[,hours[,weeks]]]]]]])</code></li> <li>Represents a duration, the difference between two dates or times.</li> </ul>"},{"location":"computing/python_notes/#using-strptimeand-strftime","title":"Using strptime()and strftime()","text":"<p>The datetime.strptime() class method creates a datetime object from a string representing a date and time and a corresponding format string. date.strftime(format) returns a string representing the date, controlled by an explicit format string. To format these inputs and outputs use the \"%field\" formatting codes. A list of the codes is here</p>"},{"location":"computing/python_notes/#debugging-code","title":"Debugging code","text":""},{"location":"computing/python_notes/#python-debugger-pdb","title":"Python debugger (pdb)","text":"<p>More on this module here.</p> <p>The Python library page is here.</p> <pre><code>\n# import the pdb module\nimport pdb\n\n# insert this into code at line x to start debugger at line x\npdb.set_trace()\n\n# as the code runs it will stop and enter the debugger at line x\n# leaving you at the pdb prompt\n\n(Pdb)\n\n# once in interactive mode several commands can be issued\nn # execute the next line\np var1, var2, var3 # print the value of variables\nc # exit interactive debugger and continue running program\nl # list area of program where you currently are\ns # step into a subroutine (function call on that line)\nr # if in a subroutine, continue until returned to original subroutine call\nq # crash out of debugging\n\n# if there are variable assignments or other changes to be made they can be\n# made at the debugger prompt and then tested by continuing the code\n\n(Pdb) var1 = \"bbb\"\n\n# An exclamation point tells pdb that what follows is a Python statement, not a pdb command.\n\n(Pdb) !b = \"BBB\"\n\n# It is also possible to set a breakpoint, (then continue to this breakpoint).\n(Pdb) break function.py:4\n</code></pre> <p>Other useful Pdb classes:</p> <pre><code># run a file or statement interactively with the debugger\npdb.run('mymodule.test()')\n\n# similar to *pdb.run* but returns the value of the expression\npdb.runeval(expression)\n\n# calls the specified function and passes any specified arguments to it:\npdb.runcall(function[, argument, ...])\n\n# performs postmortem debugging of the specified traceback\npdb.post_mortem(traceback)\n\n# performs postmortem debugging of the traceback contained in sys.last_traceback:\npdb.pm()\n</code></pre>"},{"location":"computing/python_notes/#other-useful-debugging-modules","title":"Other useful debugging modules","text":""},{"location":"computing/python_notes/#ipdb","title":"IPdb","text":"<p>The IPython shell has its on debugging module, similar to Python's. See the IPython page for more.</p>"},{"location":"computing/python_notes/#codeinteract","title":"Code.interact()","text":"<p><code>code.interact()</code> stops execution and gives you an interactive console to examine the current state of your program. To use this function, simply embed the following at the line were you want the console to start:</p> <pre><code>import code; code.interact(local=locals())\n</code></pre> <p>The resulting console inherits the local scope of the line on which code.interact() is called. This enables you to check the current state of your program to understand its behavior and make any necessary corrections.</p>"},{"location":"computing/python_notes/#logging","title":"Logging","text":"<p>Python has a logging module that allows the collection of various log messages from within a running program. To start logging debug messages it must be imported and configured to log DEBUG level messages:</p> <pre><code>import logging\n# Log everything, and send it to stderr.\nlogging.basicConfig(level=logging.DEBUG)\n\n# Now, in the body of the program, you can log events like this:\nlogging.exception(\"Something bad happened!\")\nlogging.debug(\"Finishing for loop\")\n</code></pre>"},{"location":"computing/r_notes/","title":"R notes","text":""},{"location":"computing/r_notes/#installing-and-updating","title":"Installing and updating","text":"<p>On Debian</p> <p>To install R:</p> <pre><code>sudo apt-get install r-base r-base-dev\n</code></pre> <p>On MacOS</p> <pre><code>brew install r\n</code></pre>"},{"location":"computing/r_notes/#package-management","title":"Package management","text":"<p>Install packages using <code>install.packages()</code>. Whenever R has a new update in the distribution (3.4 to 3.5, for example), packages will generally need to be reinstalled also. The location they are installed to can vary and R may ask.</p> <p>Often R complains about missing Debian packages (curl, ssl) and may fail if miniconda/anaconda is already installed (may want to change dir name).</p> <p>On Debian</p> <p>Packages are installed from a CRAN repository to a local library directory. The default users library must be created at <code>~/R/x86_64-pc-linux-gnu-library/{version#}</code>, but packages can also be installed to <code>/usr/local/lib/R/site-library</code> if permissions allow.</p> <p>On MacOS</p> <p>Packages are installed to <code>/Library/Frameworks/R.framework/Versions/3.6/Resources/library</code></p>"},{"location":"computing/r_notes/#package-list","title":"Package list","text":"<p>My usual list of packages to update is:</p> <p>install.packages(c('tidyverse', 'xts', 'rgdal', 'data.table', 'automap', 'forecast', 'ggmap', 'cowplot', 'raster', 'SPEI', 'lubridate'))</p> <p>Sometimes the core R packages on Debian go out of date and need to be updated. Start R with sudo and run:</p> <p>update.packages()</p>"},{"location":"computing/r_notes/#jupyter-r-notebooks","title":"Jupyter R notebooks","text":"<p>To run R in Jupyter notebooks install IRKernel using the linux source method (libzmq3-dev first).</p>"},{"location":"computing/sensordata_notes/","title":"Notes on working with sensor data","text":"<p>Collected tips for analyzing data in datalogger output files, SNOTEL site datasets, or other continuous timeseries data.</p> <p>See also: General programming,         Matlab tips, and Python         tips pages.</p>"},{"location":"computing/sensordata_notes/#normalizing-sensor-timeseries","title":"Normalizing sensor timeseries","text":"<p>I am experimenting with a number of ways to normalize the data from sensors:</p> <ul> <li>max-min normalization</li> <li>minimum normalization</li> <li>datetime minimum normalization`</li> </ul> <p>An example MATLAB function to normalize the data several ways:</p> <pre><code>% 0-1 normalization (0=min and 1=max for series)\n% if normtype == 1\n\nin_max = max(in);\nin_min = min(in);\nnorm = (in - in_min)./(in_max-in_min);\n\n% subtract min value, all series have a zero point (problem if there are\n% negative values) elseif normtype == 2\n\nin_min = min(in);\nnorm = (in - in_min);\n\n% normalize by setting min to value at specific time\n% (index given as arg3) elseif normtype == 3\n\nin_min = in(varargin{1});\nnorm = (in - in_min);\n</code></pre>"},{"location":"computing/sensordata_notes/#creating-decimal-day-arrays-from-datalogger-output","title":"Creating decimal day arrays from datalogger output","text":"<p>Campbell dataloggers typically give several date/time columns in a datlogger file (depending of output commands), a year, a sequential day (doy), and a 4 digit timestamp. Here is a way to convert that into a decimal day array:</p> <pre><code># create decimal day\nyear = m[:,1] day = m[:,2]\n\n# a sequential day of 1-365 (or 366 in leapyear)\nhhmm = m[:,3] # timestamp formatted as hhmm\nhh = floor(hhmm / 100) mm = hhmm - hh * 100 dd = day + (hh + (mm / 60)) / 24\n\n# create a cumulative decimal day\nyear_cum = year - 2009 # the starting year is arbitrary\ndd_cum =(year_cum * 365) + dd\n</code></pre>"},{"location":"computing/sensordata_notes/#data-filtering-interpolation-and-moving-window-statistics","title":"Data filtering, interpolation, and moving window statistics","text":"<p>Many techniques for filtering and smoothing timeseries data are explicitly mathematical using moving means, variances, standard deviations or other statistics.</p> <p>See the timeseries analysis page for more, and for examples.</p>"},{"location":"computing/shellscripts/","title":"Shell scripting notes","text":"<p>Notes on using the Unix shell (bash, ash, et. al) to manipulate files, text, etc.</p> <p>See also: General programming page, the Awk page, Vim notes, Text files.</p>"},{"location":"computing/shellscripts/#basic-stuff","title":"Basic stuff","text":"<p>Assign variables like this:</p> <pre><code>VAR=\"foo\"  # don't use any spaces around the = sign\n</code></pre> <p>Expand variables with $:</p> <pre><code>echo $VAR # prints \"foo\" to stdout\n</code></pre> <p>If the location of the variable is unclear in the input string use curly braces:</p> <pre><code>echo $VARbar # prints nothing\necho ${VAR}bar # prints \"foobar\"\n</code></pre> <p>Single quoted strings are interpreted literally:</p> <pre><code>echo '$VAR' # prints \"$VAR\" to stdout\n</code></pre> <p>Double quoted strings are interpreted literally except for $, ` (backquote), and \\ (escape).</p> <pre><code>echo \"$VAR\" # prints \"foo\" to stdout \n</code></pre> <p>Double quotes also group space separated words or variable together and can be used to append/prepend:</p> <pre><code>VAR2=\"bar foobar\"\nVAR3=\"foo$VAR2\"\necho $VAR3 # prints \"foobar foobar\"\n</code></pre> <p>Backquoted strings are executed by your shell and are then replaced by the output of that execution, so:</p> <pre><code>echo The current date and time is: `date`\n</code></pre> <p>Outputs:</p> <pre><code>The current date and time is: Thu Feb 24 23:19:17 MST 2011\n</code></pre> <p>The backslash escapes the metacharacter ($,`,',\") just after it so it is interpreted as a string\"</p> <pre><code>echo $VAR # prints \"$VAR\" to stdout \n</code></pre>"},{"location":"computing/shellscripts/#redirection-and-piping","title":"Redirection and piping","text":"<p>The <code>&gt;</code> operator redirects the standard output (or stderr sometimes) of a command to a file.</p> <pre><code>ls -l &gt; ls-l.txt # Writes the output of the ls command to ls-l.txt\n</code></pre> <p><code>&gt;&gt;</code> will append to a file, rather than clobber the existing file with a new one. You can also set a \"noclobber\" option as the default in bash.</p> <p>Redirecting in the opposite directions, <code>&lt;</code>, can be used to feed the contents of a file into a command.</p> <pre><code>cat &lt; textfile1 &gt; textfile2 # Same as \"cp textfile1 textfile2\"\n</code></pre> <p>The <code>|</code> operator \"pipes\" the standard output of a command to the standard input of another command or process.</p> <pre><code>ps axl | grep zombie # select and show zombie processes from the full list of processes\n</code></pre>"},{"location":"computing/shellscripts/#command-line-scripts","title":"Command line scripts","text":"<p>Simple shell scripts can be run from the command line, separating commands with a semicolon.</p> <p>Nice utilities to put in shell commands and scripts:</p> <ul> <li>The usual file commands <code>cp</code>, <code>mv</code>, <code>rm</code>, etc.</li> <li><code>grep</code>: prints text (from a file or standard input) matching a sequence of characters.</li> <li><code>cut</code>: cuts columns out of delimited files</li> <li><code>tr</code>: (translate) deletes (-d) or replaces characters in a file</li> <li><code>tail</code>: prints the last //n// lines in a given file (good for removing headers).</li> <li><code>sed</code> commands (see this tutorial)</li> <li>awk oneliners</li> <li><code>find</code>: ????</li> </ul> <p>For example in a particular directory, a list of files with the .csv extension can be generated with a simple loop:</p> <pre><code>greg@gm-thinkpad:~/Downloads/station_inventory$ for i in *.csv; do echo \"$i\"; done\n# Outputs:\nAZ_soilstations.csv\nCO_soilstations.csv\nID_soilstations.csv\nMT_soilstations.csv\nNM_soilstations.csv\nNV_soilstations.csv\nUT_soilstations.csv\nWY_soilstations.csv\n</code></pre> <p>Using the same loop format, copies and substitutions to the filename can be made:</p> <pre><code>greg@gm-thinkpad:~/Downloads/station_inventory$ for i in *.csv; do cp \"$i\" \"${i/.csv}2.csv\"; done\n# Outputs\ngreg@gm-thinkpad:~/Downloads/station_inventory$ ls\nAZ_soilstations2.csv  MT_soilstations2.csv  UT_soilstations2.csv\nAZ_soilstations.csv   MT_soilstations.csv   UT_soilstations.csv\nCO_soilstations2.csv  NM_soilstations2.csv  WY_soilstations2.csv\nCO_soilstations.csv   NM_soilstations.csv   WY_soilstations.csv\nID_soilstations2.csv  NV_soilstations2.csv\nID_soilstations.csv   NV_soilstations.csv\n</code></pre> <p>A similar effect to the substitution could be done with basename.</p> <p>Sed, Awk, and tr all replace characters or patterns in textfiles. To remove all spaces from a directory of textfiles:</p> <pre><code>for i in  .csv; do tr -d ' ' &lt; \"$i\" &gt; \"${i/.csv}2.csv\"; done\n</code></pre> <p>To replace each line's first occurrence of a given phrase with another pattern, either of these will work:</p> <pre><code>for i in  *.csv; do sed 's/station id/station_id/' &lt; \"$i\" &gt;  \"${i/.csv}2.csv\"; done\n</code></pre> <p>OR  </p> <pre><code>for i in  .csv; do awk -F \",\" '{ sub(/station id/, \"station_id\"); print }' \"$i\" &gt; \"${i/.csv}2.csv\"; done\n</code></pre> <p>To replace all occurrences of the pattern use global replacement - add a <code>g</code> after the sed statement, or use <code>gsub</code> instead of <code>sub</code> in awk.</p> <p>This command will take each .csv file, cut out columns 13-15, and redirect this output to a new file.</p> <pre><code>for FILE in  .csv; do cut -d\",\" -f-12,16- \"$FILE\" &gt; \"${FILE/.csv}2.csv\"; done \n</code></pre>"},{"location":"computing/shellscripts/#executing-saved-scripts","title":"Executing saved scripts","text":"<ul> <li>Start scripts with the path to bash (#!/bin/bash)</li> <li>Make sure they are executable</li> <li>From the current working directory they can be run with \"bash scriptname\" OR \"./scriptname</li> </ul>"},{"location":"computing/textfiles/","title":"Text files","text":"<p>Text files are perhaps the simplest and most interoperable format for storing and exchanging data. Here are some tips and utilities for manipulating and effectively using text files in a UNIX-like environment.</p> <p>See also: General programming page, the Awk page, Vim notes, the shell scripting page.</p>"},{"location":"computing/textfiles/#text-operations","title":"Text operations","text":""},{"location":"computing/textfiles/#find-grep-text-in-the-contents-of-multiple-files","title":"Find (grep) text in the contents of multiple files","text":"<pre><code>grep -Hrn 'search term' path/to/files\n</code></pre> <p>where <code>-H</code> prints the filename (default if called on a dir), <code>-r</code> is recurse (search in subdirectories), and <code>-n</code> prints the line number where the search term occurs in the file. To match a particular filename or extension add <code>--include=file.*</code> (or any regexp). <code>--exclude-dir=dir</code> is useful for excluding directories like <code>.svn</code> and <code>.git</code>.</p> <p><code>find</code>, followed by a call to <code>grep</code> is also an option.</p> <pre><code>find /path -type f -exec grep -l \"string\" {} +\n</code></pre> <p>where <code>-type f</code> means search only files, <code>-exec grep...</code> specifies the <code>grep</code> search to run for <code>{}</code> (each file found).</p>"},{"location":"computing/textfiles/#searchreplace-text-in-multiple-files","title":"Search/replace text in multiple files","text":"<p>Using just sed:</p> <pre><code># Replace \"phrase1\" with \"phrase2\" in all \"file*.txt\" files in the current directory, \n# and append \".bak\" to the original files.\nsed -i.bak 's/phrase1/phrase2/g' file*.txt \n</code></pre> <p>Using find, and piping to sed - no backup copy made</p> <pre><code>  find . -type f -print0 | xargs -0 sed -i 's/phrase1/phrase2/g'`\n</code></pre>"},{"location":"computing/textfiles/#diffmerge","title":"Diff/merge","text":"<p>There are plenty of diff-merge tools, including vimdiff. For diffing files with long lines (paragraphs in documents), wdiff should help.</p>"},{"location":"computing/textfiles/#markup-languages","title":"Markup languages","text":"<p>When drafting text documents, markup languages can be used to add formatting or other information to plain text. This makes text more readable, and the markup text can be parsed by an interpreter to render the content in other document formats (ie. HTML, LaTeX documents, .docx files, etc.).</p> <ul> <li>Lightweight markup languages</li> <li>Markdown</li> <li>reStructuredText</li> <li>ASCIIdoc</li> <li>Markdown extensions/supersets</li> <li>Pandoc Markdown</li> <li>MultiMarkdown</li> <li>kramdown</li> </ul> <p>I tend to use Pandoc to create documents.</p>"},{"location":"computing/vim_notes/","title":"Vim","text":"<p>Notes on using Vim effectively.</p> <p>See also: General programming page, shell scripting page, the Awk page, Textfile notes.</p>"},{"location":"computing/vim_notes/#vimrc","title":".vimrc","text":"<p>The <code>~/.vimrc</code> file is where vim looks first for settings when starting up. <code>~/.gvimrc</code> can be used to set options in the GUI version of Vim. <code>.vimrc</code> is read first.</p>"},{"location":"computing/vim_notes/#setting-vim-defaults","title":"Setting VIM defaults:","text":"<ul> <li>https://github.com/tpope/vim-sensible</li> </ul>"},{"location":"computing/vim_notes/#encrypting-files","title":"Encrypting files","text":"<p>Files can be encrypted using the <code>-x</code> flag. Typing:</p> <pre><code>vim -x &lt;filename&gt;\n</code></pre> <p>will prompt for a password and then open a new or existing file in encrypted mode. This file will subsequently be saved in encrypted form and will require a password to open it again. An already open file can also be encrypted with <code>:X</code>. The default level of encryption is not so secure (zip), so there are options to change that.</p> <pre><code>:set cryptmethod?         # Display the current encryption algorithm\n:set cryptmethod=blowfish # Set to the much more secure blowfish cipher.\n</code></pre>"},{"location":"computing/vim_notes/#diffing-files-vimdiff","title":"Diffing files (Vimdiff)","text":"<p>Vim (gvim) can be used to diff two or more files and merge changes between them (see vimdiff docs). Use:</p> <pre><code>vimdiff &lt;file1&gt; &lt;file2&gt;\n</code></pre> <p>The files will show up side by side (vertical split, use -o for horizontal), with changes highlighted in pink/red, and they can then be edited. <code>viewdiff</code> or <code>gviewdiff</code> is used to open a read-only diff view. Diff view can also be started from within a vim session (see docs). Once started, these are the basic commands for merging changes:</p> <ul> <li><code>do</code> - Get changes from other window into the current window.</li> <li><code>dp</code> - Put the changes from current window into the other window.</li> <li><code>]c</code> - Jump to the next change.</li> <li><code>[c</code> - Jump to the previous change.</li> <li><code>Ctrl W + Ctrl W</code> - Switch to the other split window.`</li> </ul>"},{"location":"computing/vim_notes/#searchreplace-strange-unicode-characters","title":"Search/Replace strange unicode characters","text":"<p>Sometimes there are funky non-standard characters in file that don't display, print, or are simply undesired. For example, hyphens, minus signs, and dashes have a number of permutations and it is best to use them consistently. They frequently get mixed up in text files, especially when copying from strange formats (i.e. Word, Excel, etc.).</p> <ul> <li>Place cursor over the funky character and type <code>ga</code>. This gives ascii value in decimal, hexidecimal, and octal format.</li> <li>Form a search/replace command using the ascii value, prepended by some regexp syntax:.</li> <li><code>%dNN</code> is the symbol for a decimal ascii code, where <code>NN</code> is the first code given by <code>ga</code>.</li> <li><code>%xNN</code> is the symbol for a hexidecimal ascii code.</li> <li><code>%oNN</code> is the symbol for an octal ascii code.</li> <li>Use <code>%uNN</code> for multibyte characters.</li> <li>Don't forget to escape this (with a )!</li> </ul> <p>For example, say there is a funky minus sign in my file. I ga over it and it has the hex value of 2212. I can then search and replace it with regular hyphen-minus using:</p> <pre><code>:%s/\\%x2212/-/g\n</code></pre>"},{"location":"computing/vim_notes/#tabs-and-tabkey-behaviour","title":"Tabs and tabkey behaviour","text":"<p>I prefer 4 columns of whitespace for each indent in a file, and I prefer to use spaces for this, not tabs. This means pressing the  should give a defined number of spaces. This can be set in <code>~.vimrc</code>. <pre><code>set expandtab       When &lt;tab&gt;key is pressed, insert spaces, not hard tabs.\nset tabstop=4       Number of columns per tab\nset softtabstop=4   Match to tabstop unless expandtab is unset, then it may set to use tabs + spaces\nset shiftwidth=4    Number of columns for indent and autoindent`\n</code></pre> <p>When editing a file with different tab settings, <code>retab</code> will change all tabs to the settings in <code>.vimrc</code>. A hard tab can be inserted by pressing <code>&lt;Ctrl-V&gt;&lt;Tab&gt;</code>.</p>"},{"location":"data_and_models/MsTMIP_reanalysis/","title":"MsTMIP","text":"<p>This is a global reanalysis product of a large number of terrestrial biome models.</p> <p>For more information about the MsTMIP project:</p> <ul> <li> <p>Main website (won't open in FF!!!???)</p> </li> <li> <p>Overview/experimental design</p> </li> <li> <p>Simulation outputs</p> </li> <li> <p>Driving data (environmental data) for MsTMIP</p> </li> <li> <p>Model structure and characteristics</p> </li> </ul>"},{"location":"data_and_models/MsTMIP_reanalysis/#reference","title":"Reference","text":"<p>Huntzinger, D.N., C.R. Schwalm, Y. Wei, R.B. Cook, A.M. Michalak, K Schaefer, A.R. Jacobson, M.A. Arain, P. Ciais, J.B. Fisher, D.J. Hayes, M. Huang, S. Huang, A. Ito, A.K. Jain, H. Lei, C. Lu, F. Maignan, J. Mao, N. Parazoo, C. Peng, S. Peng, B. Poulter, D.M. Ricciuto, H. Tian, Xiaoying Shi, W. Wang, N. Zeng, F. Zhao, and Q. Zhu (in press). NACP MsTMIP: Global 0.5-deg Terrestrial Biosphere Model Outputs (version 1) in Standard Format. Data set. Available on-line [http://daac.ornl.gov] from Oak Ridge National Laboratory Distributed Active Archive Center, Oak Ridge, Tennessee, USA. DOI: 10.3334/ORNLDAAC/1225.s</p>"},{"location":"data_and_models/MsTMIP_reanalysis/#getting-the-data","title":"Getting the data","text":"<ol> <li>Go here and fill out the little data use form.</li> <li>Once filled out the links below will be accessible.</li> <li>Follow links and download netCDF files.</li> </ol>"},{"location":"data_and_models/century_soil/","title":"Century Soil Model","text":"<p>The CENTURY model is an ecosystem model used to simulate carbon and nutrient dynamics (N, S, P) for different types of ecosystems including grasslands, agricultural lands, forests and savannas. </p> <p>The manual for the model is here. Be sure to see the manual addendum, tutorial, and parameterization workbook also.</p> <p>Miscellaneous links:</p> <ul> <li>Simple slideshow</li> <li>DAYCENT (daily timestep version)</li> <li>Version 5 of Century</li> <li>A gridded model product has been created</li> </ul>"},{"location":"data_and_models/century_soil/#basic-steps-for-a-century-simulation","title":"Basic steps for a Century simulation","text":"<ol> <li> <p>Download the model and build the components in the CENTURY, EVENT100, FILE100, and LIST100 subdirectories (using <code>make</code>, see README files in each subdir)</p> </li> <li> <p>Create a simulation folder that contains the appropriate files. Basically this will include the century, list100, file100, and event100 executables and all the parameter files. See README's and manual for a complete list of files.</p> </li> <li> <p>Parameterize the model for your site.  There are twelve data files that store these parameters that are used by Century during a simulation. These files can be created and updated using the FILE100 program. The most important is the <code>&lt;site&gt;.100</code> file that is specific to your site.</p> <ul> <li>Note that I cannot get the <code>file100</code> executable to work on my linux system unless it is within the original FILE100 directories (so parameter files must be moved there for editing, or edited with text editor)</li> </ul> </li> <li> <p>Establish the simulation time and schedule events to occur using the EVENT100 program. This will create a .sch file that can be used to run a simulation</p> </li> <li> <p>Run the model with <code>century -s &lt;schedule.file&gt; -n &lt;binary.output.file&gt;</code></p> </li> <li> <p>Model will output a binary file that should be converted to ascii with LIST100 program. Run <code>list100</code>, choose options, then the output variables you want to print in the new file.</p> </li> <li> <p>Evaluate the output.</p> </li> </ol>"},{"location":"data_and_models/century_soil/#useful-papers","title":"Useful papers","text":"<p>Parton, William J., Melannie Hartman, Dennis Ojima, and David Schimel. 1998. \u201cDAYCENT and Its Land Surface Submodel: Description and Testing.\u201d Global and Planetary Change 19 (1): 35\u201348. link</p> <p>Parton, W. J., David S. Schimel, C. V. Cole, and D. S. Ojima. 1987. \u201cAnalysis of Factors Controlling Soil Organic Matter Levels in Great Plains Grasslands.\u201d Soil Science Society of America Journal 51 (5): 1173\u20131179.link</p> <p>Parton, W. J., D. S. Ojima, C. V. Cole, and D. S. Schimel. \"A general model for soil organic matter dynamics: sensitivity to litter chemistry, texture and management.\" SSSA special publication (USA) (1994).link</p> <p>Burke, Ingrid C., C. M. Yonker, W. J. Parton, C. V. Cole, D. S. Schimel, and K. Flach. 1989. \u201cTexture, Climate, and Cultivation Effects on Soil Organic Matter Content in US Grassland Soils.\u201d Soil Science Society of America Journal 53 (3): 800\u2013805. link</p>"},{"location":"data_and_models/hdf_ncdf_files/","title":"Using HDF and NetCDF files","text":""},{"location":"data_and_models/hdf_ncdf_files/#hdf-eos-files","title":"HDF-EOS files","text":"<p>MODIS data, and other NASA data comes packaged as .hdf files. These are HDF-EOS files, specified by NASA and based on the HDF4 specification. More info here, and there are code examples for using HDF-EOS files with different languages here. For a MOD17 example see here.</p> <p>Since HDF5 is the current and most supported HDF format, it may be easiest to first convert HDF-EOS files to HDF5 files using a conversion tool. Download and unpack then <code>cd</code> into that directory and run <code>./h4toh5 ~/path/to/file.hdf</code>. </p>"},{"location":"data_and_models/hdf_ncdf_files/#reading-with-python","title":"Reading with python","text":"<p>If using a HDF5 files, h5py or PyTables can be used to access the data in the file.</p> <p>If using HDF4 data with python, check these resources:</p> <ul> <li>PyHDF info</li> <li>In anaconda, you may want the the conda-forge package</li> <li>hdf4 also seems to work</li> </ul> <p>Also - maybe check this out:</p> <ul> <li>http://www.pymodis.org/</li> </ul>"},{"location":"data_and_models/hdf_ncdf_files/#reprojecting-the-data","title":"Reprojecting the data","text":"<p>Usually these come in the a standard sinusoidal projection and there may or may not be lat/lon data provided in the file. If there is no lat/lon data it must be created using the file metadata (corner coordinates of the tile, cell size, etc). It is possible to do this and reproject with GDAL (which can find the file metadata using GetGeoTransform) and Basemap in Python (see examples here)</p> <p>If using only pyhdf, this:</p> <p>https://lpdaac.usgs.gov/tools/modis_reprojection_tool</p> <p>or this may help:</p> <p>http://hdfeos.org/software/eos2dump.php</p>"},{"location":"data_and_models/hdf_ncdf_files/#netcdf-files","title":"NetCDF files","text":""},{"location":"data_and_models/hdf_ncdf_files/#reading-the-file","title":"Reading the file","text":"<p>Some of this cribbed from:</p> <p>http://www.hydro.washington.edu/~jhamman/hydro-logic/blog/2013/10/12/plot-netcdf-data/</p> <p>They can be opened by GDAL (though potentially a little tricky)</p> <pre><code>ds = gdal.Open(mstmip_dir + 'BIOME-BGC_BG1_Monthly_NEP.nc4')\n</code></pre> <p>Or you can just use the ncdf-python module directly</p> <pre><code>ncdf2 = Dataset(mstmip_dir + 'BIOME-BGC_BG1_Monthly_NEP.nc4')\n# Then pull out relevant variables\nnep = ncdf2.variables['NEP'][-1] # data for one day month\nlats = ncdf2.variables['lat'][:]\nlons = ncdf2.variables['lon'][:]\nncdftime = ncdf2.variables['time'][:]\nnep_units = ncdf2.variables['NEP'].units\n</code></pre>"},{"location":"data_and_models/hdf_ncdf_files/#timestamps","title":"Timestamps","text":"<p>You can use the time converter in the ncdftime module</p> <pre><code>time_conv = utime('days since 1700-01-01 00:00:00')\ntimes = time_conv.num2date(ncdftime)\nprint(times[num])\n</code></pre> <p>Or just create a numpy array     td = np.array([np.timedelta64(int(i), 'D') for i in ncdftime ])     times = td + np.datetime64('1700-01-01 00:00:00')     print(times[-1])</p>"},{"location":"data_and_models/modis_NDVI/","title":"MODIS NDVI","text":"<p>Smoothed dataset for coterminous US : https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1299</p>"},{"location":"data_and_models/modis_NPP/","title":"MODIS NPP (MOD17)","text":"<p>It seems there are 2 versions of this product: 1) The original datasets produced by NASA, and 2) an improved version by the U. Montana NTSD group that reduces the amount of \"cloud contamination\" and improves the underlying climate dataset.</p> <ul> <li> <p>Main NTSD website, see also the NASA site</p> </li> <li> <p>User guide</p> </li> <li> <p>Original, uncorrected data</p> </li> <li> <p>Version 55, available in several time formats:</p> <ul> <li>Annual (A3, MOD17A3 directory) data</li> <li>Monthly (Monthly A2, Monthly_MOD17A2 directory), in year and month subdirectories</li> <li>8 day (A2, MOD17A2 directory) in year and day of year subdirectories</li> <li>In each subdirectory there is one .hdf file for each tile (see projection and tiling info below)</li> </ul> </li> </ul>"},{"location":"data_and_models/modis_NPP/#references","title":"References","text":"<p>Improvements of the MODIS terrestrial gross and net primary production global data set</p> <p>Sensitivity of Moderate Resolution Imaging Spectroradiometer (MODIS) terrestrial primary production to the accuracy of meteorological reanalyses</p> <p>Drought-Induced Reduction in GlobalTerrestrial Net Primary Production from 2000 Through 2009 and Supplemental Material</p>"},{"location":"data_and_models/modis_NPP/#getting-the-data","title":"Getting the data","text":"<ol> <li>Go to the links listed above and navigate to the correct directory</li> <li>MODIS data comes in spatial extent tiles following the \"MODIS Sinusoidal tiling system\"<ul> <li>For New Mexico the correct tiles are is H8V5 and H9V5</li> </ul> </li> </ol>"},{"location":"data_and_models/northam_climate/","title":"Interesting or useful climate data for North America","text":"<ul> <li>The PRISM group homepagewith access to datasets (Oregon State Univ.).</li> <li>PRISM data- available at the NRCS website</li> </ul>"},{"location":"data_and_models/soil_data/","title":"Soil data sources","text":""},{"location":"data_and_models/soil_data/#coterminous-us","title":"Coterminous U.S.","text":"<p>SSURGO description here</p> <p>It exists in shapefile and gridded form (SSURGO and gSSURGO)</p> <p>STATSGO - description here</p> <p>There are a variety of ways to get these data:</p> <ol> <li> <p>Web Soil Survey is a web-based download system that allows users to select a region of interest and download soil data layers. ROIs have to be pretty small.</p> </li> <li> <p>Geospatial Data Gateway allows downloads of gSSURGO, SSURGO, STATSGO2, by state or more specific areas.</p> </li> <li> <p>Soil Data Access is sort of an API to the NRCS database. Info on how to query this database can be found here. There are also some projects to allow access from R:</p> <ul> <li>Algorithms for Quantitative Pedology project page has created multiple R packages for soil data access and analysis</li> <li>Tutorial 1</li> <li>Tutorial 2</li> </ul> </li> <li> <p>NASIS may be another option</p> </li> </ol>"},{"location":"hiddencanyon/analysislog_1/","title":"Hidden Canyon data analysis","text":"<p>This log mainly covers analysis of Hidden Canyon ecohydrology experiment data (Project overview).</p> <p>See also: The Data program documentation, and the Hidden Canyon site description.</p> <p>03-20-2012</p> <p>Re-analyzing photos from 4/24/2012 (swe locations before leafout) again. In addition to the standard settings (see canopy photo procedure I am using these settings:</p> <ul> <li>This time am using growing season Mar. 1 to June 30.</li> <li>Using HCfullmask.msk.</li> <li>Using the blue color plane filter.</li> <li>Pixel count around 140-190, usually around 160</li> </ul> <p>12-14-2012</p> <ul> <li>Have been working on analyzing the snowmelt data from Hidden Canyon using a linear mixed effects model. Here are a few items to remember to try when I come back to this after Xmas:</li> <li>Plot \"\u0394SWE\" for individual SWE measurement locations, and for control/dust-addition blocks (in python or R). I'm curious as to whether these are linear over time, and whether my linear model might need to be adjusted (should it be non-linear?).</li> </ul> <p>May 16</p> <ul> <li>Went through calibration data and removed bad peaks. There were many for 2-21 injections. This lowered the error for the runs, but I wonder if it really makes the data any better. If the calibration injections were off, then it is likely that the sample injections are too. Would it be best to just remove 2-21 injection and only use the 3-13 injections for the same samples?</li> <li>I5 caltank injections still seem to be calculated as high ppm values, and I'm not sure why this is. Could it be the linear fit equation? Maybe the fit isn't linear across the 15000ppm range and the calculated values at low ppm are affected more?</li> <li>Indeed - ran some injection data with a second order polynomial (in process_injectiondata.m) and the fit is much better (3-28 data - first order = 19.7rms, second order = 3.8rms). So, change all or some of the injection runs to second order fits?</li> <li>Made the call to use first or second order for each run based on what gave a lower rms error. Currently this data is the data in processed_data/snowinlets.csv.</li> <li>Also removed inlet samples for 2/21 run from analysis and am relying on the 3/13 repeats (because 2/21 run was so bad). However, I left the atmospheric samples in for calculating fluxes.</li> </ul> <p>May 15</p> <ul> <li>Inlet data is now plotted with plot_inlets.py</li> <li>Make sure all datalogger data (Met1 at least) is up to date.</li> <li>One bad peak for 3-13 run: [50]</li> <li>Something is up with calibrations, especially for I5 tank, and looking at the qc plots generated with process_injectiondata.m, it seems like several injection dates are pretty noisy. Check this and figure out why. </li> <li>Probably would be a good approach to plot calibration data over the entire 2011 and 2012 year in plot_inlets.py, then mark bad calibration injections in the datasheet/csv.</li> </ul> <p>March 14, 2012</p> <ul> <li>Added a way to remove bad peaks from while running process_injectiondata.m. It is now necessary to look at the second plot and figure out which peaks are erroneous and then enter them at the command line. Still not sure why they are there, but it looks to have something to do with my injection technique.</li> <li>Need to do some serious data/error checking in the injection to ppm to respiration rate calculations.</li> <li>Last set of injections (2-21) look pretty bad, so I reran the samples from that day and will hopefully have better numbers.</li> <li>When I run the 2-21 samples, the bad peaks that need to be selected are [22, 24, 35, 42, 43, 47, 51]</li> </ul> <p>Feb 21, 2012</p> <ul> <li>Did a new run on the injection system and then ran the data through process_injectiondata.m and it failed. Apparently there are little mini-peaks after the injection peaks that Matlab is reading as separate injections. Will need to fix this and run the data again.</li> <li>The mini peaks occur after the highest standard injections (14,000 ppm) and may be the result of a faulty syringe (though I have no idea why...).</li> <li>The culprit is the CV method of discovering peaks vs inter-peak readings (which are flat).</li> </ul> <p>Feb 9, 2012</p> <ul> <li>Created scripts to convert Forest1 v3 and v4 datafiles to a format compatible with v6 (in py/fileconverters/), and put the resulting files in the rawdata folder for the project.</li> <li>Created a parsefile for both dataloggers that provides variable names based on the datalogger program output (Met1, and Forest1 v3, 4, and 6). This is used in file converters and the check_hc_datalogger.py programs (and others in the future).</li> <li>Updated loaddata/loadDatalogger program to load Forest1 v6 data, as well as the new converted files correctly. Also tweaked the way it removes bad data.</li> <li>check_hc_dataloggers.py has also been rewritten to plot output from all sensors, and it has better datetime handling (using a new function in loaddata module.</li> <li>TO DO:</li> <li>Make Sentek sensors at profile 10 work - will probably require a tweaked datalogger program.</li> <li>Locate and remove more bad data - probably need a \"bad data log\" file somewhere to read.</li> <li>Make a function to convert Decagon outputs to actual temp and vwc numbers (in loaddata?).</li> <li>Convert snowpack CO2 measurements to fluxes.</li> </ul> <p>Jan 31, 2012</p> <ul> <li>Have compiled all data from the dataloggers into a couple of files that correspond to the datalogger programs they come from. There is a v3 and v4 datafile from Forest1, and the current data from v6 program is being appended to HC_F1_append.dat.</li> <li>Now trying to figure out how to view this data effectively with all these different versions.</li> </ul> <p>Jan 9, 2012</p> <ul> <li>Wrote process_injectiondata.m to process datafiles and calibrate peak areas from the new CO~2~ injection system.</li> <li>New datafiles from injection runs will go in rawdata/co2injections. Once processed, the data from these files must be merged into the injection datasheets.</li> <li>data_analysis/hiddencanyon is now fairly well organized, except for trees/ directory.</li> </ul>"},{"location":"hiddencanyon/communicationsystem/","title":"Communications system at Hidden Canyon","text":"<p>Communication with the two dataloggers at Hidden Canyon is made possible using a wireless modem and three spread-spectrum radios.</p>"},{"location":"hiddencanyon/communicationsystem/#devices-and-connectivity","title":"Devices and connectivity","text":""},{"location":"hiddencanyon/communicationsystem/#sierra-wireless-raven-xtg-modem-1","title":"Sierra Wireless Raven XTG Modem (1)","text":"<ul> <li>Antenna is a dual band (800-1900 Mhz, with 5.12 and 6.12dB signal gain respectively) Wilson \"Trucker\" whip antenna mounted on the Met tower.</li> <li>Connected to the internet via the AT&amp;T wireless network</li> <li>Fixed IP address</li> <li>Modem can be configured with Sierra's AceManager program on a Windows machine.</li> </ul>"},{"location":"hiddencanyon/communicationsystem/#digi-xbee-pro-pkg-rf-modems-3","title":"Digi XBee-PRO PKG RF Modems (3)","text":"<ul> <li>A base radio (connected to Raven modem) communicates with each datalogger radio (2) on a separate radio channel.</li> <li>Configuration of modems is through Digi's X-CTU software on a Windows machine (lab computer or laptop in the field).</li> <li>All 3 use firmware version 10CD</li> <li>All 3 use standard antennas</li> <li>Command Sequence Character (CC) for all is 2D (\"-\")</li> <li>PAN ID (ID) for all is 3171</li> <li>Sleep mode (SM) for all radios is 1 (hibernation mode) - this means that when idle they should enter low-power hibernate mode. This doesn't seem to work on the 2 radios connected to dataloggers due to constraints in how the dataloggers' RS-232 ports operate.</li> </ul>"},{"location":"hiddencanyon/communicationsystem/#connections-and-configuration-settings","title":"Connections and configuration settings","text":"<ul> <li>Base radio</li> <li>Connected to Raven XTG modem with RS-232 cable and null modem adapter</li> <li>MAC Address: 0013A2004063B85A</li> <li>Coordinator enable (CE) = 0</li> <li>Channel (CH) = C</li> <li>Source Address (MY) = 1</li> <li>Destination High (DH) = 0</li> <li> <p>Destination Low (DL) = Switched between 2(Met radio), 3 (Forest radio), and 0 (Idle)</p> </li> <li> <p>Met1 radio</p> </li> <li>Connected to Met1 datalogger with RS-232 cable and null modem adapter</li> <li>MAC Address: 0013A2004063B8D6</li> <li>Coordinator enable (CE) = 0</li> <li>Channel (CH) = C</li> <li>Source Address (MY) = 2</li> <li>Destination High (DH) = 0</li> <li> <p>Destination Low (DL) = 1</p> </li> <li> <p>Forest1 radio</p> </li> <li>Connected to Forest1 datalogger with RS-232 cable and null modem adapter</li> <li>MAC Address: 0013A200406AA06E</li> <li>Coordinator enable (CE) = 0</li> <li>Channel (CH) = C</li> <li>Source Address (MY) = 3</li> <li>Destination High (DH) = 0</li> <li>Destination Low (DL) = 1</li> </ul>"},{"location":"hiddencanyon/communicationsystem/#loggernet-software-configuration","title":"LoggerNet software configuration","text":"<p>Communication and data transfer with the Hidden Canyon network over the internet is initiated from a workstation in the Bowling lab using LoggerNet software (Campbell Scientific Inc). The configuration for this to work is as follows:</p> <ul> <li>IPPort configured for the static IP of the Raven, port 3001.</li> <li>Two Generic devices below the IPPort are the radios - configured to Raise DTR</li> <li>One of the CR-23x is connected to each Generic device</li> <li>When connecting to each datalogger, LoggerNet runs a unique dial script to change the channel of the base radio (using the ATDL command) to the address of the appropriate endpoint radio.</li> <li>An end script changes the base radio back to an idle channel.</li> <li>Communication system is available for three half-hour periods per day (see below).</li> </ul>"},{"location":"hiddencanyon/communicationsystem/#dial-script","title":"Dial script","text":"<pre><code>D3000                    # wait 3000 ms\nT\"---\" R\"OK\"4000         # enter command mode\nT\"ATDL2^m\" R\"OK\"4000     # change destination to 2 (or 3 for Forest logger)\nT\"ATCN^m\" R\"OK\"4000      # exit command mode\n</code></pre>"},{"location":"hiddencanyon/communicationsystem/#end-script","title":"End script","text":"<pre><code>D3000\nT\"---\" R\"OK\"4000\nT\"ATDL0^m\" R\"OK\"4000    # change destination to 0 for idle\nT\"ATCN^m\" R\"OK\"4000\n</code></pre>"},{"location":"hiddencanyon/communicationsystem/#power-relays-and-program-control","title":"Power relays and program control","text":"<p>Running all three radios and the modem at all times would drain too much power, so in addition to running in low-power mode when they are on, the entire communication system operates from relay controllers (one at each datalogger). These relays are switched on or off at programmed time intervals using the datalogger control ports. Table 2 in the EDLOG programs running on each datalogger is devoted to operating the relays. Essentially all devices are switched on for thirty minutes every eight hours, so the dataloggers should be available from 00:00 to 00:30am, 8:00 to 8:30am, and 16:00 to 16:30pm.</p>"},{"location":"hiddencanyon/communicationsystem/#table-2-edlog-code","title":"Table 2 EDLOG code","text":"<pre><code>; run table every half hour\n*Table 2 Program\n02: 1800    Execution Interval (seconds)\n\n; set control port 1 high every 8 hours\n1:  If time is (P92)\n 1: 0        Minutes (Seconds --) into a\n 2: 480       Interval (same units as above)\n 3: 41       Set Port 1 High\n\n; set control port 1 low 30 minutes later\n2:  If time is (P92)\n 1: 30        Minutes (Seconds --) into a\n 2: 480       Interval (same units as above)\n 3: 51       Set Port 1 Low\n</code></pre>"},{"location":"hiddencanyon/dataloggers/","title":"Hidden Canyon dataloggers","text":"<p>There are two Campbell CR23x dataloggers at Hidden Canyon:</p> <ul> <li>Met1 is located on the met towerand it logs meteorological data and powers the modem and the Base and Met1 radios.</li> <li>Forest1 is located on the forest towerand it logs meteorological data and soil profiledata (via 3 multiplexers).</li> </ul> <p>Both of these dataloggers are connected via radio to the communications system.</p>"},{"location":"hiddencanyon/dataloggers/#met1","title":"Met1","text":"<ul> <li>Current wiring spreadsheet: </li> <li>Current EDLOG program: </li> </ul>"},{"location":"hiddencanyon/dataloggers/#sensors-with-start-date","title":"Sensors (with start date)","text":"<ul> <li>Vaisala temp/RH sensor, Dec 20, 2009</li> <li>Met One windspeed and direction sensor, Jan 7, 2010</li> <li>Texas Electronics rain gauge, Dec 20, 2009</li> <li>Upward looking PAR (LI-190), Nov 11, 2009</li> <li>Downward looking PAR (LI-190), Jan 25, 2010</li> <li>REBS Net Radiation sensor, Jan 7, 2010 (day windset started working), Jan 25, 2010 Negative correction factor changed</li> <li>Judd Snow Depth sensor, Jan 14, 2010, offest changed to 340cm Aug 26, 2010.</li> <li>Apogee IR radiometer (snow surface below tower), Dec 20, 2009</li> <li>Setra atmospheric pressure, Dec 20, 2009</li> </ul>"},{"location":"hiddencanyon/dataloggers/#peripherals","title":"Peripherals","text":"<ul> <li>Switch: Campbell A6REL-12 (switches power to modem and radios)</li> <li>Modem1: Sierra Wireless RavenXT</li> <li>Base Radio: Digi Xbee radio connected to the modem</li> <li>Met1 Radio: Digi Xbee radio connected to Met1 datalogger</li> </ul>"},{"location":"hiddencanyon/dataloggers/#change-log","title":"Change log","text":"<ul> <li>8/26/10 - connected A6REL-12, Base radio, Met1 radio, and modem to this datalogger and changed program to v3.</li> </ul>"},{"location":"hiddencanyon/dataloggers/#old-edlog-programs","title":"Old EDLOG Programs","text":""},{"location":"hiddencanyon/dataloggers/#forest1","title":"Forest1","text":"<ul> <li>Current wiring spreadsheet: </li> <li>Current EDLOG program: </li> </ul>"},{"location":"hiddencanyon/dataloggers/#met-sensors-with-start-date","title":"Met Sensors (with start date)","text":"<ul> <li>Vaisala temp/RH sensor, Dec 18, 2009</li> <li>Met One windspeed and direction sensor, Dec 18, 2010</li> <li>Apogee IR radiometer (Control snow surface temp), Dec 18, 2009, fixed EDLOG problem on Feb 18, 2010.</li> <li>Apogee IR radiometer (Treatment snow surface temp), Jan 7, 2010, repositioned on March 4, 2010.</li> <li>Apogee IR radiometer (Canopy surface temp), Dec 18, 2009, repositioned on Jan 14, 2010</li> </ul>"},{"location":"hiddencanyon/dataloggers/#soil-profile-sensors-via-mux1-and-mux2","title":"Soil profile sensors (via MUX1 and MUX2)","text":"<p>see the soil profile page for more details</p> <ul> <li>12 Decagon EC-TM sensors</li> <li>18 Campbell CS-616/615 sensors</li> <li>6 Campbell 107 thermistors</li> </ul>"},{"location":"hiddencanyon/dataloggers/#peripherals_1","title":"Peripherals","text":"<ul> <li>MUX1: Campbell AM16/32B connected to 4 soil profiles</li> <li>MUX2: Campbell AM16/32 connected to 4 soil profiles</li> <li>Crydom D1D07 relay connected to 5v power and control port</li> <li>ForestRadio: Digi Xbee radio connected to RS-232 port</li> </ul>"},{"location":"hiddencanyon/dataloggers/#change-log_1","title":"Change log","text":"<ul> <li>10/16/2010: New datalogger program (v4) added, rewired MUX 1 &amp; 2, added MUX 3 and profiles 5 &amp; 7 to network. Appears to be collecting data as of 2:30pm.</li> </ul>"},{"location":"hiddencanyon/fieldwork_log/","title":"Hidden Canyon Logbook","text":"<p>Visits to Hidden Canyon site:</p>"},{"location":"hiddencanyon/fieldwork_log/#2013","title":"2013","text":"<p>** 7-8-2013 **</p> <ul> <li>Removed last soil profiles (destroyed 2 Decagon sensors in process)</li> <li>Broke down sensors and wiring on towers.</li> <li>Pulled out the rest of the forest instrumentation and put in pile.</li> </ul> <p>** 6-20-2013 **</p> <ul> <li>DOSE revisit - couldn't complete coring</li> <li>Cleaned up pinflags and soil resp collars (should do another sweep tho)</li> </ul> <p>** 6-11-2013 **</p> <ul> <li>Collected remaining litterbags - had to dig plot 3, litterbag 2 out of 10cm of snow.</li> <li>Shut down F1 datalogger.</li> <li>Removed profiles 5, 6, 7, and 8</li> </ul>"},{"location":"hiddencanyon/fieldwork_log/#2012","title":"2012","text":"<p>** Oct 22, 2012 **</p> <ul> <li>Reconnected wiring at F1 datalogger.</li> <li>Removed profiles 2 and 4.</li> </ul> <p>** Oct 19, 2012 **</p> <p>Site breakdown with Allison, Laurel, and Pataki lab (Tara, Meghan, LaShaye).</p> <ul> <li>Removed profiles 1, 1d, 3, 3d, 9 and 10.</li> <li>Removed Mux1</li> <li>Removed sensors on Forest tower.</li> <li>Cleanup of flagging, ropes, signage.</li> <li>Brought down extra enclosure/battery from solar tower.</li> </ul> <p>** Oct 17, 2012 **</p> <ul> <li>Canopy photos at all soil profiles - leaves on aspen are now gone.</li> <li>Collected litterbags</li> </ul> <p>** Oct 8, 2012 **</p> <ul> <li>Sapling pre-dawn xylem water potential</li> <li>Fixed wiring at MUX 1 and it looks like everything is working again.</li> </ul> <p>** Oct 6, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars.</li> <li>Sapling mid-day xylem water potential</li> <li>Surface VWC measurements at all locations.</li> <li>There is a problem with MUX1 - wiring is chewed up and it is not working.</li> </ul> <p>** Sept 12, 2012 **</p> <ul> <li>Returned with the repaired LiCor and measured LiCor respiration at all collars.</li> </ul> <p>** Sept 9, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars, but there is a problem with the instrument. These fluxes will need to be tossed.</li> <li>Sapling pre-dawn and mid-day xylem water potential.</li> <li>Surface VWC measurements at all locations.</li> </ul> <p>** Aug 26, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars.</li> <li>Sapling pre-dawn and mid-day xylem water potential, mature pre-dawn only.</li> <li>Surface VWC measurements at all locations.</li> </ul> <p>** Aug 9-10, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars.</li> <li>Sapling pre-dawn and mid-day xylem water potential</li> <li>Surface VWC measurements at all locations.</li> <li>2nd DOSE measurement/sample collection (peak drought)</li> </ul> <p>** July 26-27, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars.</li> <li>Sapling and mature tree pre-dawn and mid-day xylem water potential</li> <li>Surface VWC measurements at all locations.</li> </ul> <p>** July 17, 2012 **</p> <ul> <li>Hemispherical photos of all litterbag/collar and soil profile locations - with Tasha.</li> </ul> <p>** July 11, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars.</li> <li>Sapling pre-dawn and mid-day xylem water potential.</li> <li>Mature tree pre-dawn xylem water potential.</li> <li>Surface VWC measurements at all locations.</li> </ul> <p>** June 25, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars.</li> <li>Sapling (only) pre-dawn and mid-day xylem water potential</li> <li>Surface VWC measurements at all locations.</li> </ul> <p>** June 18, 2012 **</p> <ul> <li>Did all DOSE measurements</li> </ul> <p>** June 4, 2012 **</p> <ul> <li>Measured LiCor respiration at all collars - Note that the IRGA was spanned in the lab last week.</li> <li>Mature/sapling pre-dawn and mid-day xylem water potential</li> <li>Surface VWC measurements at all locations.</li> <li>Collected the last litterbag (P3-2)</li> </ul> <p>** May 31, 2012 **</p> <ul> <li>Collected all litterbags.</li> <li>Logged snowcover at all grid points (one remaining - 30E, 20N).</li> <li>Moved and reinserted all respiration collars (&lt;1m from last years location).</li> </ul> <p>** May 21, 2012 **</p> <ul> <li>Measured SWE at all plots and reps.</li> <li>Logged snowcover at all grid points</li> <li>Snow cover is effectively gone, except from 2 measurement locations (bags/collars)</li> </ul> <p>** May 14, 2012 **</p> <ul> <li>Measured SWE at all plots, all reps.</li> <li>Logged snowcover at all grid points (and other points).</li> </ul> <p>** May 8, 2012 **</p> <ul> <li>Measured SWE in all plots. all 6 measurements.</li> <li>Logged snowcover at all grid points (and other points).</li> <li>Messed with datalogger</li> <li>Collected full column cores from each plot for dust loading analysis.</li> </ul> <p>** Apr 30, 2012 **</p> <ul> <li>Fourth dust deposition - all 6 plots, ~3000g each</li> <li>Messed with datalogger</li> <li>Logged snow at all grid points</li> </ul> <p>** Apr 27, 2012 **</p> <ul> <li>Measured SWE in all plots, 4 measurements each</li> <li>Sampled all inlets</li> <li>3 full column samples taken for dust loading</li> <li>Messed with datalogger</li> </ul> <p>** Apr 24, 2012 **</p> <ul> <li>Took hemispherical canopy photos at all SWE measurement points</li> <li>Fiddled with forest datalogger</li> <li>Logged clear grid points - there were several.</li> </ul> <p>** Apr 19, 2012 **</p> <ul> <li>Measured SWE in all plots, 5 measurements each</li> <li>Third dust deposition - all 6 plots, ~3000g each</li> <li>Messed with datalogger again</li> </ul> <p>** Apr 15, 2012 **</p> <ul> <li>Sampled all inlets</li> <li>Measured SWE in plots 1-4 (2 measurements each)</li> <li>Messed with datalogger again...</li> </ul> <p>** Apr 10, 2012 **</p> <ul> <li>Second dust deposition - all 6 plots, ~3000g each</li> <li>Measured SWE in all plots. n=6 in all 6 plots.</li> <li>Messed with datalogger again - seems to be the wiring below the box, was working ok when I left.</li> </ul> <p>** Apr 3, 2012 **</p> <ul> <li>Visited to try and fix sensor problems again - not sure if it worked.</li> <li>Dust is still visible on the treatment plots, despite the new snow from a few days ago.</li> </ul> <p>** Mar 30, 2012 **</p> <ul> <li>First dust deposition - all 6 plots, ~3000g each</li> <li>Measured SWE in all plots. n=2 in plots 1-4, n=4 in 5&amp;6 - figured I could count some of the measurements from earlier in the week for plots 5 &amp; 6.</li> </ul> <p>** Mar 27, 2012 **</p> <ul> <li>Sampled all inlets</li> <li>Measured SWE in plots 1-4 (3 measurements each)</li> <li>Messed around with datalogger wiring, but was not able to restore measurements of soil sensors.</li> </ul> <p>** Mar 21, 2012 **</p> <p>Made a quick trip up to try to fix soil moisture datalogging again. Still having trouble identifying the problem.</p> <ul> <li>Excavated snow around base of forest datalogger post and checked wires.</li> <li>Retaped some chewed (lightly chewed) wires and tried to put more slack in them - didn't seem to do much though.</li> <li>Disconnected treatment IR radiometer, and forest Vaisala (not sure if this will help or not, but these sensors are malfunctioning anyways.)</li> <li>When I left all sensors were working, though sometimes intermittently it seemed.</li> <li>Took a snow sample from the board (2 cores, 37cm depth).</li> </ul> <p>** Mar 12, 2012 **</p> <ul> <li>Sampled all inlets</li> <li>Measured SWE in all plots (3 measurements each)</li> <li>The bad soil profile data problem returned, so I excavated the snow at the post and left everything uncovered. There are some chewed up wires there (since retaped) but not sure if this is where the problem is or not. All soil moisture sensors are failing to read, not just the ones with chewed wires. Hopefully this will keep the data coming in for a while.</li> </ul> <p>** Mar 8, 2012 **</p> <p>Noticed soil profiles were not working (all data was -6999) yesterday, went up to fix this.</p> <ul> <li>Fixed one loose wire on MUX3 (Sentek sensor profile 9)</li> <li>There seems to be some loose wiring at the base of the Forest1 datalogger post, but it is difficult to identify. Excavating the snow and adjusting the wire caging seemed to help. </li> <li>Re-covered the base with snow.</li> </ul> <p>** Feb 18, 2012 **</p> <ul> <li>Sampled all inlets</li> <li>Measured swe in all plots (4 measurements each)</li> <li>Lowered ropes a bit</li> <li>Modified Forest1 datalogger program to get Sentek sensor at profile 10 running finally (had to remove loops in program).</li> <li>Put out stormboard.</li> </ul> <p>** Jan 17, 2012 **</p> <ul> <li>Went to Hidden Canyon and sent v6 program to Forest1 datalogger manually</li> <li>As of the 4pm data collection this appears to be working (except for profile 10 sentek sensor) - there is new data starting at 2pm.</li> </ul> <p>** Jan 8, 2012 **</p> <ul> <li>Rewrote and sent a datalogger program to Forest1 datalogger. Program includes sentek sensors and new CS-616s in measurements and final storage table (averages).</li> <li>Sent at ~4pm after downloading all data and archiving it</li> <li>resent at 12am (9th) - doesn't appear to be sending the program, probably because the radio is turning off.</li> </ul> <p>** Jan 4, 2012 **</p> <ul> <li>Sampled all inlets</li> <li>SWE measurements in plot 1, 2, 3, 4 (4 measurements each)</li> <li>Rewired Sentek sensors (12V power and Gnd are now on the terminal strips with the 616's)</li> </ul>"},{"location":"hiddencanyon/fieldwork_log/#2011","title":"2011","text":"<p>** Oct 28, 2011 **</p> <ul> <li>9:15am - Sent new program (v5) to Forest datalogger that includes profiles 6 and 8.</li> <li>EDIT (1-6-2012): apparently I forgot to average the new sensor readings and add them to the output table, so it will have to be resent.</li> </ul> <p>** Oct 26, 2011 **</p> <ul> <li>Wired up all new sensors to MUX3 - P6, 8, 9 &amp; 10.</li> <li>Fixed IR radiometer on met tower</li> <li>Datalogger working - not sure what this intermittent problem is about - will look at data</li> </ul> <p>** Oct 22, 2011 **</p> <ul> <li>Installed Sentek sensors in profiles 9 and 10 using slurry method</li> <li>Mid-day water potential measurements (skipped big trees)</li> <li>Wired sensors</li> <li>Datalogger not working?</li> </ul> <p>** Oct 16, 2011 **</p> <ul> <li>Predawn (6:30-7:30am) water potential measurements (skipped large trees).</li> <li>Measured surface soil moisture w/ campbell handheld probe at all locations (morning).</li> <li>Soil respiration measurements (10-1pm)</li> </ul> <p>** Oct 15, 2011 **</p> <ul> <li>Collected litterbags</li> <li>Installed 2 new soil moisture profiles (6 and 8) in plots 3 and 4</li> <li>There is a problem with the datalogger again - doesnt appear to be getting good data. Maybe the datalogger needs replacing?</li> </ul> <p>** Oct 11, 2011 **</p> <ul> <li>Had to resend datalogger program again</li> </ul> <p>** Sept 23, 2011 **</p> <ul> <li>Midday(1:30pm-2:30pm) and predawn (6:10-7:10am) water potential measurements (skipped large trees due to lack of pressure gas).</li> <li>Measured surface soil moisture w/ campbell handheld probe at all locations (morning).</li> <li>Soil respiration measurements (10-1)</li> </ul> <p>** Sept 9-10, 2011 **</p> <ul> <li>Midday(3:30pm-5pm) and predawn (6-7:30am) water potential measurements (including large trees).</li> <li>Measured surface soil moisture w/ campbell handheld probe at all locations (morning of 10th).</li> <li>Soil respiration measurements (9th - 1-3pm).</li> <li>Restarted Forest1 datalogger - there may be a gap present in the data around this time.</li> </ul> <p>** Aug 26-27, 2011 **</p> <ul> <li>Midday(3pm-5pm) and predawn (5-7am) water potential measurements (including large trees).</li> <li>Measured surface soil moisture w/ campbell handheld probe at all locations (morning of 27th).</li> <li>Soil respiration measurements (26th - 12-2:30pm).</li> </ul> <p>** Aug 11-12, 2011 **</p> <ul> <li>Midday(2:30pm-4pm) and predawn (5-7am) water potential measurements.</li> <li>Added large trees from Plots 1 &amp; 2 to xylem wp measurements (5 trees in each plot - subset of last year's trees)</li> <li>Measured surface soil moisture w/ campbell handheld probe at all 6 plots, 6 locations each (morning of 12th).</li> <li>Soil respiration measurements (12th - 9-11:30am).</li> </ul> <p>** July 28, 2011 - Mike B. **</p> <ul> <li>Midday(12pm-1:30pm) and predawn (6-7am) water potential measurements.</li> <li>Soil respiration measurements (9-11:30am).</li> </ul> <p>** July 20, 2011 - Davis U. **</p> <ul> <li>Fixed all guywires etc on weather stations with Davis.</li> </ul> <p>** July 13-14, 2011 **</p> <ul> <li>First midday xylem water potential measurements around 3-5pm</li> <li>Hammered in soil collars, 4 per plot, adjacent to litterbags (but randomized)</li> <li>First predawn xylem water potential around 5-7am (14th)</li> <li>First soil respiration at 9-12am (14th)</li> </ul> <p>** July 12, 2011 - Davis U. **</p> <ul> <li>SWE at ALL locations (and all were melted)</li> <li>Took photos of the spatial extent of snowmelt.</li> <li>Logged melted grid points (only one still has snow)</li> <li>Collected litterbags (except for P3-2 which was still snowcovered)</li> </ul> <p>** June 29, 2011 **</p> <ul> <li>Measured SWE at ALL locations</li> <li>Took photos of the spatial extent of snowmelt.</li> <li>Logged melted grid points</li> </ul> <p>** June 23, 2011 **</p> <ul> <li>Measured SWE at 12 locations per treatment</li> <li>Took photos of the spatial extent of snowmelt.</li> <li>Collected snow columns for total dust loading measuremnts - 1 per plot</li> <li>Logged melted grid points</li> </ul> <p>** June 15, 2011 **</p> <ul> <li>6th dust application</li> <li>Measured SWE at 12 locations in each plot</li> <li>Took photos</li> <li>Met tower IR thermometer has a loose wire (EX terminal)</li> <li>Plot 2 has melted out in the middle of the plot, and litterbag set 3 is exposed.</li> <li>30N 30E stake is almost exposed melted out.</li> </ul> <p>** June 7, 2011 **</p> <ul> <li>Measured SWE at 12 locations in each plot</li> <li>Took gas samples from all inlets</li> <li>Collected dust cores in the open from the last few dust events</li> <li>Reconnected Forest1 datalogger to its radio</li> </ul> <p>** June 1, 2011 **</p> <ul> <li>Dust dep 5</li> <li>Fixed MUX1 problem - moving snowpack had pulled the COM terminals loose (at the MUX)</li> <li>Took a few photos</li> </ul> <p>** May 27, 2011 **</p> <ul> <li>Collected exetainer samples from all inlets (Not measured yet - LiCor may be broken!)</li> <li>Federal sampler SWE measurements in 24 locations (12 per treatment)</li> <li>Stormboard sample</li> <li>Collected dust cores from the May 15-16 event</li> </ul> <p>** May 16, 2011 **</p> <ul> <li>Federal sampler samples in 12 locations (control and treatment)</li> <li>Surface \"after\" samples in forest (3x each in dust and control plots)</li> <li>Surface samples in the open (3x each in dust and control areas near towers)</li> <li>Took some photos</li> <li>Brought back stormboards from BCL, BCM. and BCH.</li> </ul> <p>** May 12, 2011 **</p> <ul> <li>Storm board sample (PCAPS method)</li> <li>Surface samples in the open (3x near towers)</li> <li>Dust application in 3 locations in the open (adjacent to surface samples above)</li> <li>Took albedo photos of Ctl snow surface vs. dusted snow surface (in adjacent slope positions)</li> <li>First set (6 photos) 11:45 to 12:15pm</li> <li>Second set (6 photos) 1:45-2:15pm</li> <li>Surface \"before\" samples in forest (3x each in dust and control plots)</li> <li>Dust deposition 4</li> <li>Federal sampler samples in 7 locations (control and treatment)</li> <li>Collected exetainer samples from all inlets (Measured on the 13th - OK)</li> <li>Took a number of photos in the forest of dust/control areas - 2pm</li> </ul> <p>** May 4, 2011 **</p> <ul> <li>Storm board sample (PCAPS method)</li> <li>Full column sample (PCAPS method)</li> <li>Kelly samples of the entire column (10cm increments)</li> <li>Surface samples in the open (3x near towers)</li> <li>Surface samples in forest (3x each in dust and control plots)</li> <li>This is at essentially peak SWE (according to the Brighton SNOTEL)</li> <li>Dust is still active at the top of the snowpack.</li> </ul> <p>** May 1, 2011 **</p> <ul> <li>Collected exetainer samples from all inlets (Measured these on the 3rd - OK).</li> <li>Measured SWE in plots 1-6 (16 measurements)</li> <li>Dust deposition 3</li> </ul> <p>** Apr 15, 2011 **</p> <ul> <li>RE-collected exetainer samples from all inlets (Measured these the following morning - OK).</li> <li>Measured SWE in plots 1-6 (20 measurements)</li> <li>Dust deposition 2</li> </ul> <p>** Apr 12, 2011 **</p> <ul> <li>Collected exetainer samples from all inlets (Measured these the following morning - :!: all were atmospheric:!:).</li> <li>Measured SWE in plots 1-4</li> <li>Fixed ropes</li> <li>Dug out met tower</li> </ul> <p>** Apr 2, 2011 **</p> <ul> <li>Measured SWE in all plots, these are good numbers this time. Used lube on tubes.</li> <li>Collected \"after\" sublimation samples from N &amp; S facing slopes in HC</li> </ul> <p>** Mar 31, 2011 **</p> <ul> <li>First dust deposition, on all 3 treatment plots</li> <li>Measured SWE in all plots, but sampling was difficult and I don't have too much faith in the numbers.</li> <li>Collected sublimation samples from N &amp; S facing slopes in HC</li> </ul> <p>** Mar 27, 2011 **</p> <ul> <li>Fixed/replaced ropes</li> <li>Wired charging station together and connected battery</li> <li>Dug out solar tower a bit.</li> </ul> <p>** Mar 23, 2011 **</p> <ul> <li>Collected exetainer samples from all inlets (Measured these the following morning).</li> <li>Measured SWE in corridor 1. Conditions made this difficult, and there are only 2-3 reliable measurements.</li> <li>Fixed ropes - one needs to be replaced</li> <li>Collected snow sample from storm board.</li> </ul> <p>** Feb 18, 2011 **</p> <ul> <li>Collected exetainer samples from all inlets, including t1-t3 series for 3 inlets. (Measured these the following morning)</li> <li>Measured SWE in six locations in corridor 1</li> <li>Fixed ropes</li> <li>Collected six snow cores from N of towers for dust sampling</li> <li>Dug out tower bases a bit</li> </ul> <p>** January 2011 **</p> <p>Find dates for these!!</p> <ul> <li>Dug out tower bases</li> <li>Installed caps for inlet tubing</li> <li>First sampling of below snow inlets</li> <li>Restarted measurements at Forest site Jan 11.</li> <li>Collected snow samples at board</li> <li>Maintained ropes and signs, but there has been some traffic.</li> </ul>"},{"location":"hiddencanyon/fieldwork_log/#2010","title":"2010","text":"<p>** Early December 2010 **</p> <ul> <li>Forest 1 datalogger seems to have failed in early November. Began troubleshooting this.</li> <li>Installed signs around perimeter of forest and tightened up ropes</li> <li>There are some tracks through the area already</li> </ul> <p>** 15-16 Oct, 2010 **</p> <ul> <li>Wired MUX 3 to Forest datalogger</li> <li>Installed new datalogger program and made sure it was working (see here)</li> <li>Forest1 measurements became functional at ~2:30pm Oct 16th</li> <li>Deployed all litterbags on the 15th (180 in 36 reps)</li> <li>Final xylem pressure and soil respiration measurements for the season (all on the 16th) but forgot soil temp probe.</li> <li>Tidied all wires and tubing and sealed enclosures for winter.</li> </ul> <p>** Early October visits</p> <ul> <li>Installed profiles 5 and 7</li> <li>Finished installing undersnow inlets and measured all tubing lengths.</li> <li>More litter collected and litter bags made, weighed, and filled.</li> <li>Installed post and enclosure for MUX 3</li> <li>Began rewiring Forest1 datalogger and connections to MUX 1 &amp; 2</li> <li>Downloaded Forest1 data on Oct 8th, up to 11am averages</li> <li>Forest1 down from Oct 8th to the 15th</li> </ul> <p>** 13 - 25 Sept, 2010 **</p> <p>Several visits during this time:</p> <ul> <li>Installed wiring for charging station at solar tower, reoriented 30W panel, and completed caging.</li> <li>Installed undersnow CO~2~ inlets and tubing for plots 1, 2, and the bottom half of 3.</li> <li>Collected some litter, but more will be necessary.</li> <li>Flagged plots 1-6 with pink flagging tape</li> <li>Dug holes for profiles 5 &amp; 7 (new plot and profile scheme detailed here)</li> <li>Measured respiration and xylem psi in plots 1 and 2.</li> </ul> <p>** 8-10 Sept, 2010 **</p> <ul> <li>Installed caging around solar, met, and forest towers and fixed all wiring that was chewed.</li> <li>Replaced 3 malfunctioning Decagon sensors (P1-60, P5-20, P7-60)</li> <li>Forest 1 radio and relay are installed and functioning</li> </ul> <p>** 3-4 Sept, 2010 **</p> <ul> <li>Measured xylem water potential in the afternoon (2-4pm, 3rd).</li> <li>Measured soil respiration at all collars (10:30am-1pm, 3rd)</li> <li>Measured predawn xylem water potential (5-7am, Aug 4th).</li> <li>Downloaded data from both dataloggers.</li> <li>Continued install of Forest 1 radio, and put new program on Forest1 datalogger.</li> <li>Put out tarps for litter collection.</li> <li>Examined potential sites for new SM/ST profiles.</li> <li>Fixed chewed up wiring to Treatment IR radiometer, there is other rodent damage to fix.</li> </ul> <p>** 26 Aug, 2010 **</p> <ul> <li>Installed wireless modem, Base radio, Met1 radio, and associated antennas and relays.</li> <li>New datalogger program to Met1 datalogger (added table 2, changed Judd depth sensor offset)</li> <li>Dowloaded data from both dataloggers (after 5pm average)</li> <li>Partial install of Forest1 radio</li> <li>Can connect to Met1 datalogger as of 8am Friday morning --- //Greg Maurer2010/08/27 11:30//</li> </ul> <p>** 13-14 August, 2010 **</p> <ul> <li>Measured xylem water potential in the afternoon (2-4pm, Aug 13).</li> <li>Measured predawn xylem water potential (5-7am, Aug 14).</li> <li>Measured soil respiration at all collars (Aug 14, 8-11:30am)</li> <li>Downloaded all data from dataloggers.</li> <li>Checked wiring for Profile 6-20cm - OK</li> <li>Brought collars from north side of Hidden Canyon back.</li> <li>Collected Boletes!</li> </ul> <p>** 2-3 August, 2010 **</p> <ul> <li>Measured xylem water potential in the evening (5pm, Aug 2).</li> <li>Measured predawn xylem water potential (4am, Aug 3).</li> <li>Downloaded all data from dataloggers.</li> </ul> <p>** 23 July, 2010 **</p> <ul> <li>Measured soil respiration at all collars - data looks ok.</li> </ul> <p>** 22 July, 2010 **</p> <ul> <li>Downloaded data from dataloggers</li> <li>Soil respiration attempt # 2 FAILED - again due to batteries. Try again tomorrow.</li> </ul> <p>** 16 July, 2010 **</p> <ul> <li>Soil respiration measurements - FAILED - batteries died, need at least 2 more good batteries to complete all 20 collars.</li> </ul> <p>** 12 July, 2010 **</p> <ul> <li>Downloaded data from dataloggers</li> <li>All grid points are melted out.</li> <li>Adjusted lower (30w) solar panel a bit.</li> <li>Labeled collars</li> </ul> <p>** 23 June, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Measured SWE in control plots (n-10), treatment plots (n-10), and on the east side of treatment (n=5)</li> <li>Most of Control and Treatment are melted out, no new dust needed</li> <li>Logged melt progress at grid points.</li> <li>Installed respiration collars, 10 in each swath</li> </ul> <p>** 16 June, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Measured SWE in control plots (n-10), treatment plots (n-10), and on the east side of treatment (n=5)</li> <li>No new dust applied, old dust is still visible.</li> <li>Logged melt progress at grid points.</li> <li>Fixed guywire on solar tower.</li> </ul> <p>** 9 June, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Measured SWE in control plots (n-10), treatment plots (n-10), and on the east side of treatment (n=5)</li> <li>No new dust applied, old dust is still visible.</li> <li>Dug snow pits and measured depth of dust layers.</li> <li>Logged melt progress at grid points</li> </ul> <p>** 3 June, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Measured SWE in control plots (n-10), treatment plots (n-10), and on the east side of treatment (n=5)</li> <li>Sixth dust application - 3.5 mason jars (12000g each)</li> <li>There is bare ground visible in the middle of the treatment plot.</li> </ul> <p>** 25 May, 2010 **</p> <ul> <li>Measured SWE in control plots (n-10), treatment plots (n-10), and on the east side of treatment (n=5)</li> <li>Fifth dust application - 3.5 mason jars (12000g each).</li> </ul> <p>** 14 May, 2010 **</p> <ul> <li>Downloaded data from both dataloggers</li> <li>Measured SWE in control plots (n-10), treatment plots (n-10), and on the east side of treatment (n=5)</li> <li>Fourth dust application - 3.5 mason jars (12000g each).</li> </ul> <p>** 6 May, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Measured SWE in control plots (n=6), treatment plots (n=6), and on the east side of treatment (n=3).</li> <li>Third dust application - 4 mason jars (1200g each).</li> </ul> <p>** 24 April, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Measured SWE in control plots (n=6), treatment plots (n=6), and on the east side of treatment (n=3).</li> <li>Second dust application went well - 4 mason jars (1200g each).</li> </ul> <p>** 8 April, 2010 **</p> <ul> <li>Downloaded data from both dataloggers</li> <li>First dust application. 3 mason jars (~1200g each) covers roughly the entire area.</li> <li>More battery power might help.</li> </ul> <p>** 25 March, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Changed wiring from Forest1 to MUX2 - Power and signal for Campbell sensors was moved to the extra pair of wires in Line 2.</li> <li>Carried up some dust and the vacuum.</li> </ul> <p>** 18 March, 2010 **</p> <ul> <li>Downloaded data from both dataloggers</li> <li>Dust application should commence next week. </li> </ul> <p>** 4 March, 2010 **</p> <ul> <li>Downloaded data from both dataloggers</li> <li>Checked wiring at MUX2 and Forest1 to troubleshoot Campbell sensors --- fixed one potential wiring problem at MUX2</li> <li>Mounted treatment IR radiometer on post</li> <li>Extended rope at top of stand</li> </ul> <p>** 18 Feb, 2010 **</p> <ul> <li>Downloaded data from both dataloggers.</li> <li>Tested 3 malfunctioning Decagon sensors at sensor leads with the testing datalogger. All appear broken.</li> <li>Fixed EDLOG problem with Canopy and Control (input terminal mismatch). They now appear to be working.</li> <li>Measured slope and aspect of Met and Forest towers</li> <li>Hammered in mounting post for Treatment IR radiometer</li> <li>Added ropes at top of stand - still needs signs</li> </ul> <p>** 4 Feb, 2010 **</p> <ul> <li>Gave Tom Painter and Dave a tour the site and talked science.</li> <li>Roped off another 100 feet of forest at the lower east edge of the plot.</li> </ul> <p>** 2 Feb, 2010 **</p> <ul> <li>Downloaded data from both dataloggers</li> <li>Fixed wiring at Mux 2 (one free wire, some terminals a bit loose)</li> <li>Verified that wiring from Mux 2 to Forest 1 datalogger is connected correctly.</li> <li>Looked for shorts, but didn't find anything too bad - some cable shields are probably touching, but it shouldn't matter (right?).</li> <li>Added another 50 feet of rope, but there is still more needed below that.</li> </ul> <p>** 25 Jan, 2010 **</p> <ul> <li>Downloaded data from both dataloggers</li> <li>Reversed wiring for downward looking PAR - should be working now.</li> <li>Changed REBS negative correction factor to a negative value in Met1 EDLOG program.</li> <li>Roped and signed east side of forest, lower paths still need to be blocked</li> </ul> <p>** 14 Jan, 2010 **</p> <ul> <li>Met tower </li> <li>Lowered met tower crossbars - crossbar 1 is at roughly 3.85 m, crossbar 2 is at 3.7m</li> <li>Leveled upward PAR, downward PAR, wind, and Rnet sensors</li> <li>Wind sensor is oriented properly now (arrow facing S) and is unobstructed</li> <li>Reoriented met tower so crossbars are N/S (1) and E/W (2)</li> <li>Connected downward PAR sensor leads, bridged with 560\u03a9 resistor at terminals, and jumpered Diff Low to ground</li> <li>New program (HC_MET1_V2.csi) uploaded. All sensors but downward PAR appear to be working now.</li> <li>Forest tower</li> <li>Pointed Canopy IR radiometer horizontally in the downhill direction (southwest)</li> <li>New program (HC_FOREST1_v3.csi) uploaded. This made all the CS-615s work</li> <li>Collected data from both dataloggers.</li> </ul> <p>** 10 Jan, 2010 **</p> <ul> <li>Tried to troubleshoot the Decagon sensor problems</li> <li>Checked junction to MUX wiring and it appears to be sound. Wires are correctly connected and it looks like there is excitation and a signal at the sensor leads.</li> <li>Don't know why there is -INF  or zeros at the Forest1 datalogger (could be faulty sensors)</li> <li>Also tried and 80 ms delay before powering up sensors - didn't help</li> <li>Pit 7 60cm is still going in and out also.</li> <li>Downloaded data from Forest1</li> </ul> <p>** 7 Jan, 2010 **</p> <ul> <li>Downloaded data and uploaded HC_Forest1_v2.csi to Forest1 (small delay change in serial io)</li> <li>Attached windset to Met1 logger (with extension) and treatment IR radiometer (with extension) to Forest1</li> <li>New signs</li> <li>Tried to troubleshoot soil moisture sensors, but two decagons and the 615s are not working</li> <li>Tightened all MUX terminals (this may have fixed one sensor)</li> <li>Checked juction box 1 and it looks OK</li> <li>Found which Campbell sensors are 615's</li> <li>Problem with remaining decagons is likely a wiring problem from junction box to multiplexer (or bad sensors)</li> </ul>"},{"location":"hiddencanyon/fieldwork_log/#2009","title":"2009","text":"<p>** 20 Dec, 2009 **</p> <ul> <li>Downloaded data from Forest1 and Met1 datalogger</li> <li>Finished attaching sensors to Met1 and Forest1 dataloggers</li> <li>Met1 wind sensor, downward PAR, and forest treatment IR radiometer still not connected</li> </ul> <p>** 18 Dec, 2009 **</p> <ul> <li>Downloaded data from Forest1 datalogger</li> <li>Marked off treatment and control swaths with flagging tapes. Treatment is marked with two flags per branch.</li> </ul>"},{"location":"hiddencanyon/litterbaglog_1/","title":"Litterbaglog 1","text":""},{"location":"hiddencanyon/litterbaglog_1/#2010","title":"2010","text":"<ul> <li>Litterbags were constructed of 10 x 15cm pieces of black nylon and gray fiberglass fabric. The nylon fabric is approximately 0.2mm (No-See-Um mesh), and the fiberglass screen is approximately 1.7mm mesh. Both were purchased from Seattle Fabric Inc. </li> <li>Litter collected between Sept 20th and Oct 10th by leaving tarps out on the ground, and/or shaking needles directly from trees onto tarps</li> <li>Litter was dried in drying oven, mixed well, and litterbags were filled by around October 14th. Between 4.5 and 5g of litter was weighed and placed into each bag before they were sewed shut. Sewed bags were then weighed and prepared for the field (wrapped in sets).</li> <li>First complete set litter bags was deployed Oct 15, 2010. 180 litterbags (6 reps, 5 bags each per plot). Litterbags were affixed to the ground using wire pins in sets of 5 bags. There are six sets of litterbags per plot, arranged every ten meters in a line running from the bottom of the plot to the top.`</li> </ul>"},{"location":"hiddencanyon/litterbaglog_1/#2011","title":"2011","text":"<ul> <li>Plot 2 set #3 was the first litterbag set to melt out</li> <li>July 12 - Collected the first set of litterbags - all but Plot 3 set #2 was melted out.</li> <li>July 14 - Collected plot 3 set 2 litterbag</li> <li>July 18th - Litterbags were kept in the drying oven until the 18th and then weighed.</li> <li>4 sets of bags had gopher activity around them that partially covered the bags. This makes weighing the litter accurately difficult. Litter was sifted away from mineral soil in a plastic tub and then weighed separately, but there is still mineral soil clinging to some litter.</li> <li>Decomposed litter is archived for later analysis. `</li> </ul>"},{"location":"hiddencanyon/mettowers/","title":"Hidden Canyon towers","text":"<p>These towers are responsible for power, aboveground sensor measurements (meteorological), and communication at the Hidden Canyon site.</p> <p>There are three total towers at Hidden Canyon:</p> <ul> <li>A solar tower with solar panels, a battery box, and charge controllers. Located in the clearing.</li> <li>A met tower with meteorological instrumentation, a datalogger, and a modem and antenna for communication with the lab. Located just east of the solar tower.</li> <li>A forest tower with some met instruments, and a datalogger that is connected to the forest soil profilesvia 2 multiplexers.</li> </ul>"},{"location":"hiddencanyon/mettowers/#objectives","title":"Objectives","text":"<ul> <li>Continuous measurement of air temperature, relative humidity, windspeed and direction, soil and snow surface temperatures, snow depth, net radiation, and incoming and reflected PAR.</li> <li>Provide environmental data to support the ecohydrology experimentsat Hidden Canyon.</li> <li>Measurement of weather patterns and longer term climatic trends at Hidden Canyon site. </li> <li>Comparison of Hidden Canyon research results to results from similar studies at other sites with varying environmental conditions.</li> <li>Measure the influence of forest cover on a subset of these measured environmental conditions (Temp/RH, surface temps, wind).</li> </ul>"},{"location":"hiddencanyon/mettowers/#methods-and-instrumentation","title":"Methods and Instrumentation","text":"<p>Tower descriptions and sensor lists are below. Sensors that have unique or difficult configurations may have their own instrumentation page (linked). Greater detail on towers, dataloggers, sensors, and wiring layouts is tabulated in .</p>"},{"location":"hiddencanyon/mettowers/#met-tower","title":"Met tower","text":"<p>![../media/img_3847_scaled_.jpg?200|Met tower enclosure and solar tower (background)] ![../media/img_3825_scaled_.jpg?235|Instruments on Met tower]</p> <p>The Met tower is a mast about 7m tall, with an East/West oriented boom at 3.7m height and a N/S oriented crossbar at 3.8 m height. The tower is located in the open, about 16m east and slightly downhill of the solar tower. Slope angle is 21-22\u00b0 at the base of the tower. Aspect is 197\u00b0 (SSW).</p> <p>Sensors and data collection start dates</p> <ul> <li>Vaisala temp/RH sensor, Dec 20, 2009</li> <li>Met One windspeed and direction sensor, Jan 7, 2010</li> <li>Texas Electronics rain gauge, Dec 20, 2009</li> <li>Upward looking PAR (LI-190), Nov 11, 2009</li> <li>Downward looking PAR (LI-190), Jan 25, 2010</li> <li>REBS Net Radiation sensor, Jan 7, 2010 (day windset started working), Jan 25, 2010 Negative correction factor changed</li> <li>Judd Snow Depth sensor, Jan 14, 2010, offset changed to 340cm Aug 26, 2010.</li> <li>Apogee IR radiometer (snow surface below tower), Dec 20, 2009</li> <li>Setra atmospheric pressure, Dec 20, 2009</li> </ul> <p>Datalogger:</p> <p>The datalogger at this tower is a Campbell CR23x called Met1. For more details see the Hidden Canyon datalogger page.</p> <p>Communications:</p> <p>The Forest1 datalogger powers two radios, and a wireless modem via a Campbell AM6REL12 every 8 hours. This allows communications between the Met1 an Forest1 dataloggers and the Base radio, and communications via the internet with the wireless modem (see communications page).</p>"},{"location":"hiddencanyon/mettowers/#forest-tower","title":"Forest tower","text":"<p>![../media/img_3788_scaled_.jpg?200|Forest tower and enclosure] The forest tower is a fencepost about 2.5m tall with an East/West oriented boom at the top of the post. It is located 40.5m east and downhill of the Met tower. Slope angle is 20-21\u00b0 at the base of the tower. Aspect is 205\u00b0 (SSW).</p> <p>Sensors and start dates</p> <ul> <li>Vaisala temp/RH sensor, Dec 18, 2009</li> <li>Met One windspeed and direction sensor, Dec 18, 2010</li> <li>Apogee IR radiometer (Control snow surface temp), Dec 18, 2009, fixed EDLOG problem on Feb 18, 2010.</li> <li>Apogee IR radiometer (Treatment snow surface temp), Jan 7, 2010, repositioned on March 4, 2010.</li> <li>Apogee IR radiometer (Canopy surface temp), Dec 18, 2009, repositioned on Jan 14, 2010</li> </ul> <p>Datalogger:</p> <p>The datalogger at this tower is a Campbell CR23x called Forest1. For more details see the Hidden Canyon datalogger page.</p> <p>Multiplexers:</p> <p>The Forest1 datalogger is connected to two Campbell AM16/32 multiplexers. Mux1 is uphill of the forest tower and is connected to soil profiles 1-4. Mux2 is downhill of the forest tower and is connected to soil profiles 5-8. More detail on this setup is at the Hidden Canyon soil profiles page.</p> <p>Communications:</p> <p>The Forest1 datalogger powers a radio via a relay every 8 hours. This radio communicates with the base radio (see communications page).</p>"},{"location":"hiddencanyon/mettowers/#solar-tower","title":"Solar tower","text":"<p>The solar tower is located in the clearing about 1m east of CP1. It is a mast 7 m tall with one boom and three solar panels facing south. The enclosure on this tower contains two charge controllers and some terminal strips. Two power cables exit the enclosure, one powering the Met1 datalogger, and one powering the Forest1 datalogger. There is a battery box with 5 batteries at the base of the mast.</p>"},{"location":"hiddencanyon/mettowers/#maintenance-and-data-log","title":"Maintenance and data log","text":"<p>See here.</p>"},{"location":"hiddencanyon/overview/","title":"Hidden Canyon ecohydrology experiments","text":"<p>Hidden Canyon is the site of experiments and measurements designed to elucidate the importance of snowpacks and snowpack variability on the cycling of water through subalpine forest ecosystems. Snowpack meltwater is the dominant source of water availabe for soil moisture recharge and plant water use during the year, but the amount and timing of this water resource is set by some interaction between snowpack size, snowmelt speed and timing, infiltration of meltwater into soil, and plant transpiration. These aspects of snowpack dynamics and ecosystem response have high spatial and interannual variability.</p>"},{"location":"hiddencanyon/overview/#some-research-objectives","title":"Some research objectives","text":"<p>These experiments use artificial perturbations of the snowmelt cycle along with natural spatial and interannual variability to explore:</p> <ul> <li>The influence of dust deposition on forest snowpack dynamics.</li> <li>Seasonal, interannual, and spatial patterns of snow accumulation and melt in a subalpine forest.</li> <li>Seasonal patterns in soil moisture availability and water availability for trees under varied snowpacks and snowmelt cycles. </li> <li>The response of soil organisms to varied snowpack dynamics.</li> </ul>"},{"location":"hiddencanyon/overview/#hypotheses","title":"Hypotheses","text":"<ul> <li>Enhanced dust deposition on a forest snowpack surface leads to an accelerated snowmelt cycle, higher rates of sublimation water loss, and reductions in ecosystem water availability.</li> <li>Early snowmelt (whether from dust or natural variability) in an area will lead to more rapid depletion of soil moisture and longer periods of low tree xylem water potentials during the following growing season.</li> <li>Seasonal patterns in soil moisture and xylem water potentials will be similar in low snowpack and early melt scenarios.</li> </ul>"},{"location":"hiddencanyon/overview/#experimental-design","title":"Experimental design","text":"<p>The main experimental treatment for this experiment is the addition of dust to the snowpack surface with the intention of accelerating the snowmelt cycle. We expect the responses in the hypotheses above to occur as a result. The experiment's plot and sensor designs are desribed in:</p> <ul> <li>Snowmelt experiment design - Control and dust-addition plot layout, dust loading.</li> <li>Hidden Canyon soil profiles - locations, sensors, depths, etc.</li> <li>Hidden Canyon met towers - locations and sensors</li> </ul>"},{"location":"hiddencanyon/overview/#design-changelog","title":"Design changelog","text":"<ul> <li>Only plots 1 and 2 were operational from Nov 2009, to Oct 15, 2010. Plot 1 was a control and plot 2 a treatment plot. See the snowmel design pagefor a detailed layout. Each plot had 4 soil moisture profiles and two soil temperature profiles during this time. </li> <li>Xylem water potential and soil respiration) was measured during the 2010 growing season. Measurements were made on 9 large trees in each treatment. We found little difference between control and treatment and are considering switching to smaller trees for 2011.</li> <li>Moisture and temperature profiles 5 (in lower plot 3) and 7 (lower plot 4) were installed and functional on October 15, 2010.</li> <li>Plots 3-6 were set up and measured for the 2011 snowmelt season. Currently plots 2, 4, and 5 are dust additions and 1,3, and 6 are controls.</li> </ul>"},{"location":"hiddencanyon/overview/#methods","title":"Methods","text":"<ul> <li>Dust-on-snow procedures page: info on dust and how it is applied.</li> <li>SWE measurements: Measuring snowmelt (SWE disappearance) with a Federal Sampler is described here.</li> <li>Measuring xylem water potentialwith a pressure bomb. </li> <li>Measuring sublimation at the snow surface(multiple methods).</li> <li>Snowpack albedo measurements\\</li> <li>Measuring snowpack dust loading(natural or artificial).</li> </ul>"},{"location":"hiddencanyon/overview/#measurement-details-and-logs","title":"Measurement details and logs","text":""},{"location":"hiddencanyon/overview/#swe-and-melt-rate-at-hidden-canyon","title":"SWE and melt rate at Hidden Canyon","text":"<p>Snowpack depth and SWE are measured prior to each application of dust. Measurements are made using a Federal Snow Sampler. 10 or more locations are measured in Control and dust addition plots, and these measurement locations are roughly the same (within 2m) at each measurement date.</p> <ul> <li>Hidden Canyon snowmelt log: dust deposition dates, SWE data, photos, etc.</li> </ul>"},{"location":"hiddencanyon/overview/#melt-timing","title":"Melt timing","text":"<p>Presence or abscence of snow at each 10m grid point in the HC forest (80 x 80m grid) was recorded at each spring visit during the snowmelt season. The proximity of snow was also noted. This gives an approximate date of snow disappearance for each grid point, and these dates may be related to canopy density, slope position, or other spatially linked influences on snowpack energy balance. This method is still being developed.</p> <ul> <li>Hidden Canyon GRID snowmelt log</li> </ul>"},{"location":"hiddencanyon/overview/#regional-swe-and-melt-rate","title":"Regional SWE and melt rate","text":"<p>Weather, climate, and snowpack/snowmelt data from nearby SNOTEL sites or other stations in the region can provide important context for the Hidden Canyon experiments.</p> <ul> <li>Regional snowpack log</li> </ul>"},{"location":"hiddencanyon/overview/#soil-water-and-plant-xylem-pressure","title":"Soil water and plant xylem pressure","text":"<p>Xylem water potentials of are measured using a pressure the pressure bomb method using a PMS instrument. All measurements are made on Abies lasiocarpa twigs from trees in a particular size class (large trees in 2010, probably saplings in the future). Twigs are excised from south-facing (downhill) branches at a consistent height above the ground. These twigs are stripped of phloem at the cut end and measured in the chamber within 10 minutes of being cut. Midday measurements are made between 1 and 5pm, and predawn measurements are made between 4 and 7am on consecutive days. Days immediately following rain events are avoided.</p> <ul> <li>Ecosystem water log: Xylem water potential, soil moisture, and related measurement data.</li> </ul>"},{"location":"hiddencanyon/overview/#snow-surface-measurements","title":"Snow surface measurements","text":"<p>Several other measurements are relevant to snowmelt and ecosystem water availability, and they can demonstrate the influence of added dust on these ecosystem processes. They include sublimation, ambient dust loading, and albedo measurements of the snow surface. Not all of these have well defined measurement methods yet, but development of these methods is important.</p> <ul> <li>Snow surface log</li> </ul>"},{"location":"hiddencanyon/overview/#other-logs","title":"Other logs","text":"<ul> <li>Met measurements: meteorological measurements at Hidden Canyon towers.</li> <li>Soil profiles: soil temperature and soil moisture data from the control and treatment plots.</li> </ul> <p>FIXME</p>"},{"location":"hiddencanyon/overview/#hidden-canyon-carbon-cycling","title":"Hidden Canyon carbon cycling","text":"<p>Experimental manipulations of the below-canopy snowpack are taking place at the Hidden Canyon site and this is expected to influence soil water availability and temperature in the forest in both winter and the growing season. These changes in the biophysical drivers of carbon cycling are expected to influence rates of soil respiration and organic matter decomposition.</p>"},{"location":"hiddencanyon/overview/#some-research-objectives_1","title":"Some research objectives","text":"<ul> <li>To understand seasonal and interannual patterns of soil respiration and decomposition in Wasatch Mountain forests and how these patterns are determined by seasonal snowpacks.</li> <li>To measure differences in soil CO~2~ fluxes and litter decomposition between dust addition (early melt) and control snowpack treatments.</li> <li>To study the response of soil organisms to varied snowpack dynamics.</li> <li>To understand the relative importance of winter vs. growing season periods for soil carbon cycle processes.</li> </ul>"},{"location":"hiddencanyon/overview/#related-projects","title":"Related projects","text":"<ul> <li>The ecohydrology experimentsat Hidden Canyon</li> </ul>"},{"location":"hiddencanyon/overview/#hypotheses_1","title":"Hypotheses","text":"<ul> <li>Soil respiration (CO~2~ efflux) and decomposition (mass loss) are limited by moisture availability for a longer portion of the growing season in low snowpack or early snowmelt years (or areas within a landscape).</li> <li>Areas (or years) with early snowmelt or smaller snowpack accumulations will have reduced growing season soil CO~2~ fluxes and annual litter mass loss compared the opposite case.</li> <li>Accelerated snowmelt will lead to earlier peaks in below-snow soil CO~2~ fluxes during the transition from winter to spring.</li> <li>A shorter snowcovered period will reduce the amount of litter decomposed in winter and the growing season.</li> </ul>"},{"location":"hiddencanyon/overview/#experimental-design_1","title":"Experimental design","text":"<p>The main experimental treatment for this experiment is the addition of dust to the snowpack surface with the intention of accelerating the snowmelt cycle. We expect the responses in the hypotheses above to occur as a result. The experiment's plot and sensor designs are desribed in:</p> <ul> <li>Snowmelt experiment design - Control and dust-addition plot layout, dust loading.</li> <li>Hidden Canyon soil profiles - locations, sensors, depths, etc.</li> <li>Hidden Canyon met towers - locations and sensors</li> </ul>"},{"location":"hiddencanyon/overview/#design-changelog_1","title":"Design changelog","text":"<ul> <li>Only plots 1 and 2 were operational from Nov 2009, to Oct 15, 2010. Plot 1 was a control and plot 2 a treatment plot. See the snowmelt design pagefor a detailed layout. Each plot had 4 soil moisture profiles and two soil temperature profiles during this time.</li> <li>Soil respirationwas measured during the 2010 growing season at 10 collars in each plot (1 &amp; 2). We found little difference between control and treatment and are considering adding more collars for 2011.</li> <li>Moisture and temperature profiles 5 (in lower plot 3) and 7 (lower plot 4) were installed and functional on October 15, 2010.</li> <li>Snow inlets (6 inlets in plots 1 &amp; 2, 3 inlets in plots 3 &amp; 4) and litterbags (6 sets in each of 6 plots) were deployed in early October 2010.</li> <li>Plots 3-6 were set up and measured for the 2011 snowmelt season. Currently plots 2, 4, and 5 are dust additions and 1,3, and 6 are controls.</li> </ul>"},{"location":"hiddencanyon/overview/#methods_1","title":"Methods","text":"<ul> <li>Growing season soil respiration with the Li-Cor 6400 system.</li> <li>Below-snow soil respirationmeasurements made with the inlet/tubing system and exetainers.</li> <li>Litterbag mass loss measurement of winter and summer decomposition. </li> </ul>"},{"location":"hiddencanyon/overview/#measurement-details-and-logs_1","title":"Measurement details and logs","text":""},{"location":"hiddencanyon/overview/#growing-season-soil-respiration","title":"Growing season soil respiration","text":"<p>Growing season soil CO~2~ fluxes are measured at collars using the Li-Cor 6400. Collars are installed in control and treatment (early-melt) plots each spring and are measured roughly every 2 weeks. Measurements are made in the during the morning hours (between 9am and noon) and measurement locations alternate between control and treatment collars so that soil temperature increases during the measurement period occur evenly between treatments.</p> <ul> <li>Hidden Canyon soil respiration log: measurement locations, dates, and preliminary data.</li> </ul>"},{"location":"hiddencanyon/overview/#below-snow-soil-respiration","title":"Below-snow soil respiration","text":"<p>Below-snow soil CO~2~ efflux is measured in winter and spring using an inlet and tubing system. Inlets are placed on the soil surface in control and treatment plots each fall, in association with soil sensor profiles. Each inlet is tubed to a central measurement location between plots. To collect a sample, below-snow air is pumped from the inlet to the sampling assembly with a pump. Because tubing lenghts vary, the timing of pumping must be adequate to clear the tubing of stagnant air and bring the snowpack air to the sampling fitting. Samples are transfered into an evacuated Labco Exetainer with a syrnge, needle, and septa. Three samples of ambient air (above snow) are also collected.</p> <ul> <li>Hidden Canyon soil respiration log: measurement locations, dates, and preliminary data.</li> </ul>"},{"location":"hiddencanyon/overview/#litterbag-decomposition","title":"Litterbag decomposition","text":"<p>Measurements of litter decomposition are measured, as litter mass loss, twice a year at Hidden Canyon. Mass loss measurements are made using litter bags recovered in spring (to measure winter decomposition) and in fall (to measure growing season decomposition). This allows a comparison of winter versus summer decomposition. Measurements are made in both the Treatment (dust addition) and Control (natural dust loading) plots, and the snow-free date is recorded for each set of bags every spring.</p> <ul> <li>Hidden Canyon litterbag log: bag assembly, placement, recovery, and measurement.</li> </ul>"},{"location":"hiddencanyon/overview/#other-logs_1","title":"Other logs","text":"<ul> <li>Met measurements: meteorological measurements at Hidden Canyon towers.</li> <li>Soil profiles: soil temperature and soil moisture data from the control and treatment plots.</li> </ul>"},{"location":"hiddencanyon/programdocs/","title":"Program documentation for Hidden Canyon data analysis","text":""},{"location":"hiddencanyon/programdocs/#data-files","title":"Data files","text":""},{"location":"hiddencanyon/programdocs/#functions","title":"Functions","text":""},{"location":"hiddencanyon/programdocs/#scripts","title":"Scripts","text":""},{"location":"hiddencanyon/programdocs/#process_injectiondatam","title":"process_injectiondata.m","text":"<p>Takes datalogger output files from the CO~2~ injection system, calibrates data using the cal tank values, and then calculates ppm of unknown samples.</p> <ul> <li>File Input: datalogger .dat file from injection (GUI selection by user)</li> <li>User Input: </li> <li>Allows exclusion of one data period (GUI selection by user)</li> <li>Numbered peaks can be excluded (enter integer list at command line) - Fixes issue with bad peaks after high (14000ppm) cal injections. See Feb 21 log entry.</li> <li>Outputs //2 Plots// - One with raw datalogger data, cumsum, and calculated peaks, and one with calibration RMS values. //1 datafile// with integrals and calibrated CO~2~ ppm</li> </ul>"},{"location":"hiddencanyon/sitedescription/","title":"Hidden Canyon (Wasatch Mtns, UT)","text":"<p>Location description</p> <ul> <li>Elevation: 9400 ft</li> <li>Aspect: 197-205\u00b0 (South-Southwest)</li> <li>Slope: 20-22\u00b0 at towers</li> <li>Dominant tree species: Engelmann spruce, Subalpine fir, Aspen</li> <li>There are 2 georeferenced gridsand other markers.</li> <li>Site of ecohydrology experiments(snowmelt, soil carbon cycling, and plant water balance) </li> <li>Established by the Bowling Lab in the Fall of 2009.</li> </ul>"},{"location":"hiddencanyon/sitedescription/#measurements","title":"Measurements","text":"<p>Hidden Canyon has:</p> <ul> <li>A meteorological towerlocated in a clearing to the west of the study forest.</li> <li>A forest met towerthat records aboveground and belowground climate variables in the forest.</li> <li>Eight soil profilesthat measure soil temperature and water content.</li> <li>Some snow measurements, part of the ecohydrology experiments.</li> <li>Soil respiration measurements</li> <li>Decomposition measurements using litterbags</li> <li>Xylem water potentialmeasurements</li> <li>All trees have location, species, and status data here.</li> </ul>"},{"location":"hiddencanyon/sitedescription/#communication","title":"Communication","text":"<p>This site is connected to the internet via a wireless modem. This modem is connected via radio to both dataloggers, so both can be accessed from the Bowling lab with LoggerNet. See this page for information on how this works.</p>"},{"location":"hiddencanyon/sitedescription/#site-logs","title":"Site logs","text":"<p>Activity logs document all site visits (beginning Dec 20, 2009)</p> <ul> <li>2013 activity log</li> <li>2012 activity log</li> <li>2011 activity log</li> <li>2010 activity log</li> <li>HiddenCanyon data analysis</li> <li>Met tower log</li> <li>Soil profile log</li> <li>Snowmelt experiment log</li> </ul>"},{"location":"hiddencanyon/sitedescription/#to-do-list","title":"TO DO list","text":"<ul> <li>~~Remove Sentek sensors~~</li> <li>~~Remove Profiles 1, 1d, 3, 3d and Mux1~~</li> <li>~~Remove sensors on F1 datalogger tower, including   both downward looking IR surface temp sensors~~</li> <li>~~Take down ropes and signs around site~~</li> <li>Remove all flagging.</li> <li>~~Remove second enclosure/battery on solar panel   tower.~~</li> <li>~~Tighten nut on upward PAR sensor~~</li> <li>Remove profiles 2d and 4d</li> <li>Wiring to Mux 3 needs to be pulled/cut out</li> <li>Take down Forest, Met, Solar towers</li> <li>Flagging, stakes, and resp collars should be removed. </li> </ul>"},{"location":"hiddencanyon/snowmeltdesign/","title":"Snowmeltdesign","text":""},{"location":"hiddencanyon/snowmeltdesign/#2010-pilot-project-experimental-design","title":"2010 Pilot project - Experimental design","text":"<p>![media/hc_fieldsite.jpg?600|A schematic of the treatment and control plots and installed profiles in 2010] There are two 10 by ~60 meter plots in the Hidden Canyon forest that are parallel to each other and separated by a 5m corridor. One swath serves as the treatment plot and receives regular dust additions during the spring. The other is a control and receives only natural dust loading. Each plot is instrumented as follows:</p> <ul> <li>One Apogee infrared radiometer to measure snow surface temperature</li> <li>Two soil moisture profiles with Campbell CS-615/16 sensors at 5, 20, and 60cm depths. One is in an upslope position and one is in a dowslope position.</li> <li>Two soil moisture AND temperature profiles, each with 3 Decagon EC-TM sensors at the same depths. Again, one of these profiles is high and one is low.</li> <li>Additional measurements for the soil carbon cycling and water balance parts of the experiment (see project overview) are replicated in each each plot.`</li> </ul>"},{"location":"hiddencanyon/snowmeltdesign/#2011-experimental-design","title":"2011 Experimental design","text":"<p>For the 2011 snowmelt season, the experiment has been scaled up to 3 dust treatment plots, and 3 controls, each around 10 x 70m. Additional soil moisture and temperature profile measurements have been added in two of these plots (3 &amp; 4) and water balance and carbon cycle measurements will also be deployed within these new areas.</p>"},{"location":"hiddencanyon/snowmeltlog_1/","title":"Spring 2010 snowmelt log","text":"<p>![media/treatment2_100424.jpg?250|Just after dust application on April 24th, 2010]</p> <ul> <li>First dust application of spring. Used slightly over three mason jars. Also - forgot to bring the Federal sampler so will have to measure next week and add any losses seen from nearby SNOTEL sites. Photos to come.  --- Greg Maurer 2010/04/08 21:00</li> <li>Second dust application. Used 4 mason jars and measured SWE in control and treatment (n=6) swaths. Also took 3 SWE measurements to the east of the treatment swath in similar stand density areas for additional comparison. --- Greg Maurer 2010/04/24 13:53</li> <li>Third dust application. Used 4 mason jars and measured SWE in control and treatment (n=6) swaths. Also took 3 SWE measurements to the east of the treatment swath in similar stand density areas for additional comparison.  --- Greg Maurer 2010/06/07 15:45</li> <li>Fourth dust application - used 3.5 mason jars --- Greg Maurer 2010/05/14 08:57</li> <li>Fifth dust application - used 3.5 mason jars --- Greg Maurer 2010/05/25 08:41</li> <li>Sixth dust application (3.5 mason jars), and Treatment swath is beginning to melt out. --- Greg Maurer 2010/06/03 14:15</li> <li>Measured SWE - 6/10 Treatment plot SWE points are melted out. 3/10 Control plot SWE points are melted out. --- Greg Maurer 2010/06/09 14:15</li> <li>Our total dust loading for the year was roughly 36g m^-2^</li> <li>This appeared to at least double the total natural snowpack dust loading rate for the Wasatch of 20-36g m^-2^ (from Anne Bryant and Tom Painter's work at Alta).</li> </ul>"},{"location":"hiddencanyon/snowmeltlog_1/#snowmelt-plots","title":"Snowmelt plots","text":"<p>NOTE the presence of the East Control directly east of the treatment swath! It was created to measure spatial variability in SWE from the clearing to the center of the stand. The first measurement was made on April 8th, 2010 and the last measurement was made on June 23rd.</p>"},{"location":"hiddencanyon/snowmeltlog_1/#spring-2011-snowmelt-log","title":"Spring 2011 snowmelt log","text":"<p>![media/hcplot4_110512.jpg?350|Plot 4 on May 12, 2011]</p> <ul> <li>31 March, 2011: First dust application of spring on 31 March, 2011. Used slightly over three mason jars per plot. SWE measurements were made difficult by freezing inside of tube so will have to return and remeasure.  --- Greg Maurer 2011/04/04 12:09</li> <li>April 2: Remeasured SWE using lubricant in the sampling tubes - it worked this time and it looks like measurements on the 31st were off by at least 200mm of SWE.  --- Greg Maurer 2011/04/04 12:09</li> <li>See Hidden Canyon logfor other SWE measurement and dust deposition dates.</li> <li>June 15 First bare ground occurred  at the middle of plot 2 (near 30E, 30N stake) - continued grid measurements at several time periods this year</li> <li>June 23Collected snow columns for total dust loading measurements - 1 per plot</li> <li>July 12 Snowmelt was complete (except for a small area in plot 3 - near litterbag set 2).</li> <li>Aug 8: Tuned up the <code>hc_melt.py</code> program to plot out the 2011 snowmelt season. Looks, in general like dust had no effect on the rate of melt at Hidden Canyon.</li> </ul> <p>![media/hc_snowmelt2011.png?350|Control and Treatment snowmelt in 2011. Red arrows = dust storms, Yellow arrows = dust addition]</p>"},{"location":"hiddencanyon/soilprofiles/","title":"Soilprofiles","text":"<p>FIXME</p>"},{"location":"hiddencanyon/soilprofiles/#hidden-canyon-soil-profiles","title":"Hidden Canyon soil profiles","text":"<p>A number of soil moisture and soil moisture/temperature profiles are replicated throughout the Hidden Canyon site. They provide continuous measurement of soil moisture and soil temperature at three depths. Installation began in the fall of 2009.</p> <p>Measurement start date: 11 November, 2009</p> <p>Relevant links:</p> <ul> <li>These sensor profiles are tied in to the Hidden Canyon towers</li> <li>Plot layouts, snowpack treatments, and profile locations are detailed here</li> <li>Profile data is analysed using procedures described here</li> </ul>"},{"location":"hiddencanyon/soilprofiles/#objectives","title":"Objectives","text":"<ul> <li>Record seasonal and interannual changes in soil moisture and temperature at three depths.</li> <li>Provide soil moisture/temperature data to support the ecohydrology experiments at Hidden Canyon.</li> <li>Measure differences in SM/ST in high and low stand density areas and high and low slope position</li> <li>Measure differences in SM/ST in snowmelt treatment and control plots</li> </ul>"},{"location":"hiddencanyon/soilprofiles/#methods","title":"Methods","text":"<p>![media/hc_profiles1_scaled.jpg?250|Installation of Campbell 616 sensors]</p> <p>Soil moisture and temperature sensor profiles were installed at Hidden Canyon in October of 2009 and 2010. One square meter soil pits were excavated to a depth of 70-80 cm. Soil was excavated in layers, placed on tarps, and three sensors were inserted into the vertical face of the excavated pit horizontally and parallel to the surface of the soil. Sensors were inserted at 5 cm, 20 cm, and 60 cm depths and the layers of excavated soil were replaced in order, and compacted using foot pressure. Several types of sensors are in use, in separate profiles. Four sensor profiles (1d, 2d, 3d, 4d) measure both soil temperature and soil volumetric water content using Decagon EC-TM's (combination sensors. Four profiles (1,2,3,4) measure soil volumetric water content only using CS-616's or Campbell CS-615's sensors. Two sensor profiles (5 &amp; 7) have both water content reflectometers (CS-616s) and thermistors (Campbell 107s) installed in the same pit. Sensors are multiplexed at three separate Campbell AM-16/32 multiplexers and are controlled and read by a Campbell CR23x datalogger (Forest1).</p> <p>Multiplexer, datalogger, and sensor connections are documented in .</p>"},{"location":"hiddencanyon/soilprofiles/#decagon-ec-tms","title":"Decagon EC-TM's","text":"<p>These sensors are digital. They must be turned on, require a 50ms delay, and they then output three numbers in serial format --- Soil moisture, conductivity (always 0 for these particular sensors), and soil moisture. Serial data is read in through a COM port (Instruction P15 in EDLOG) on the datalogger. There have been some problems with these sensors failing at install and the data they give seems to have a small diurnal oscillation in soil moisture that we can't explain. We are unlikely to use them again.</p>"},{"location":"hiddencanyon/soilprofiles/#not-working","title":"Not working:","text":"<ul> <li> <p>Profile 4d-60cm, Profile 3d-20cm, and Profile 1d-60cm were down from install (roughly) until being replaced on Sept 8th.</p> </li> <li> <p>Profile 2d-60cm is reading too low (all readings are negative) but the shape of the response seems good. The consensus among the Decagon people is that this sensor is either in an airgap or near a rock and it needs to be excavated and reinstalled. --- Greg Maurer2010/08/11 12:10</p> </li> </ul>"},{"location":"hiddencanyon/soilprofiles/#campbell-cs-616s","title":"Campbell CS-616's","text":"<p>These are working pretty well, even though none of them are new and some appear to have been in service for a long time. The period is read with instruction P138 (CS616 Water Content Reflectometer), and then converted to soil VWC with a polynomial fit (Instruction P55) on EDLOG dataloggers. They can also be read with instruction P27 (Period Average). They appear to give more reliable data (no diurnal soil moisture pattern) than the Decagons do.</p>"},{"location":"hiddencanyon/soilprofiles/#campbell-cs-615s","title":"Campbell CS-615's","text":"<p>A number of the sensors installed in the soil moisture profiles are CS-615's. This was discovered after they were installed. These sensors have different programming and calibration routines at the datalogger, and the datalogger program had to be written with multiple loops to accommodate the mix of 615's and 616's. They are read with Instruction P27, but the parameters are different, as are the calibration coefficients (in instruction P55) for converting to soil VWC.</p>"},{"location":"hiddencanyon/soilprofiles/#cs-615-sensor-locations","title":"CS-615 sensor locations:","text":"<ul> <li>Profile 1 - 5, 20, and 60cm</li> <li>Profile 2 - 5cm and 20cm</li> <li>Profile 3 - 60cm</li> </ul>"},{"location":"hiddencanyon/soilprofiles/#parameters-for-reading-615-sensors-at-the-dataloggers","title":"Parameters for reading 615 sensors at the dataloggers","text":"<ul> <li>All 615's are configured with calibration coeffients for low EC soils (&lt; 1.0 dS m^-1^). C0 = -0.187, C1 = 0.037, C2 = 0.335.</li> </ul>"},{"location":"hiddencanyon/soilprofiles/#unsolved-campbell-sensor-problems","title":"Unsolved Campbell sensor problems","text":"<ul> <li> <p>All Campbell sensors in profiles 1 and 3 gave an intermittent error reading (6999) intermittently for several months after install. Wiring has been thoroughly checked, and the MUX seems to work OK with the Decagons. Mysteriously, around April 30th (day 120), this intermittent problem got MUCH less frequent and the sensors gave mostly good data. We're not sure what changed. Data recorded between the intermittent errors (which was very sparse up until the end of April) is in the expected range for the sensors.  --- Greg Maurer 2010/08/11 12:04</p> </li> <li> <p>There is one sensor, Profile 3 - 20cm, that is reading too high, though the shape of the response seems to be normal, but reversed. Check this one's wiring. --- Greg Maurer 2010/08/11 12:10</p> </li> </ul>"},{"location":"hiddencanyon/soilprofiles/#sentek-envirosmart-probes","title":"Sentek Envirosmart Probes","text":"<p>In summer of 2010 several Sentek Envirosmart Probes will be installed. More to come on this later.</p>"},{"location":"hiddencanyon/soilprofiles/#multiplexers","title":"Multiplexers","text":"<ul> <li>Profiles 1, 1d, 3, &amp; 3d are multiplexed at Mux 1, a Campbell AM16/32 downhill of the forest tower.</li> <li>Profiles 2, 2d, 4, &amp; 4d are multiplexed at Mux 2, a Campbell AM16/32B uphill of the forest tower.</li> <li>Profiles 5 and 7 are multiplexed at Mux 3, a Campbell AM 16/32 at about 48E and 25N.</li> <li>More on wiring and programming for these instruments is in and on the Hidden Canyon datalogger page.</li> </ul>"},{"location":"hiddencanyon/soilprofiles/#2010-soil-profile-log","title":"2010 Soil profile log","text":"<ul> <li>CS-615 sensors in Profile 2 began working as of 14 Jan, 2010 (New EDLOG program)--- Greg Maurer 2010/01/15 11:03</li> <li>Decagon sensors in Profile 4d-60cm, Profile 3d-20cm, Profile 1d-60cm are not working. Tested these three sensors with a direct connection to a datalogger at the sensor leads and they all appear to be dead (-INF signal in Loggernet). --- Greg Maurer 2010/02/19 08:15</li> <li>Three malfunctioning sensors (4d-60, 3d-20, 1d-60) were replaced Sept 8, 2010. They appear to work.</li> <li>Renumbered site plots and profiles. Plots are 1-6 in order from east to west, profiles are sequential starting downslope in plot 1 (Profile 1), upslope plot 1 (Profile 2), downslope plot 2 (Profile 3), etc... See the experimental design pagefor more details on this.</li> <li>Installed new profiles, numbers 5 and 7, in early October. These have Campbell CS-616s and 107 thermistors at each depth.</li> <li>Rewired and reordered sensors at Mux 1 and 2, installed Mux 3 and wired profiles 5 and 7 to it. Forest 1 datalogger was down from Oct 8th to the 15th. </li> <li>Installed new Forest1 datalogger program (v4) and the new Forest1 measurement scheme became operational at ~2:30pm Oct 16th.</li> <li>Possible loss of data starting around Nov 11, 2010 to Jan 11, 2011. FIXME</li> </ul>"},{"location":"hiddencanyon/soilprofiles/#maintenance-and-data-log","title":"Maintenance and data log","text":"<p>See here</p>"},{"location":"hiddencanyon/soilresplog_1/","title":"2010 Soil Respiration measurements","text":"<p>[[media/hc_2010_rs.png|width=350px|alt=2010 Respiration collars]]</p>"},{"location":"hiddencanyon/soilresplog_1/#growing-season-2010","title":"Growing season 2010","text":"<ul> <li>Replaced chemical traps and bought 2 new batteries in July 2010.</li> <li>Installed collars, 10 in each plot, on July 12.</li> <li>First complete set of measurements (all 20 collars) at Hidden Canyon on July 23.</li> <li>Further sampling days: August 14th, Sept 3rd, Sept 24th, Oct 16th.</li> <li>Replaced collar T1 (0.5m west) on October 15th.</li> <li>Forgot the temperature probes Oct 16th.</li> </ul>"},{"location":"hiddencanyon/soilresplog_1/#fallearly-winter-2010","title":"Fall/Early Winter 2010","text":"<ul> <li>Installed 18 snow inlets in early October</li> <li>6 each in plot 1 &amp; 2 (3 around each profile), and 3 each in plot 3 &amp; 4 (near Profiles 5 &amp; 7).</li> <li>Tubing ends need to be capped and the sampling protocol needs to be developed.</li> </ul>"},{"location":"hiddencanyon/soilresplog_1/#2011-soil-respiration-measurements","title":"2011 Soil Respiration measurements","text":""},{"location":"hiddencanyon/soilresplog_1/#january-march-below-snow","title":"January-March (below snow)","text":"<ul> <li>Early Jan - Have finished capping and otherwise preparing the tubing ends to be accessed.</li> <li>Jan 21 - First inlet samples taken. This was an incomplete set of samples.</li> <li>Feb 18 - Second set of inlet samples (COMPLETE this time) taken. All samples, including Jan 21st set, were measured the next day (19th).</li> <li>Mar 23 - Third set of inlet samples. Added a ~6000 ppm tank to calibration when measuring these (big red tank).</li> <li>Apr 12 - Fourth set of inlet samples - these failed because the collection system was leaking.</li> <li>April 15th, May 1, Mau 12, May 27, and June 7 completed the below snow season.</li> <li>There was some water in the tubing on the last measurement date.</li> <li>Concentrations got pretty high (8000ppm) but I haven't calculated fluxes yet.</li> </ul>"},{"location":"hiddencanyon/soilresplog_1/#july-13-14","title":"July 13-14","text":"<ul> <li>Installed respiration collars on the evening of the 13th</li> <li>Measured respiration on the morning of the 14th</li> </ul> <p>Collar Locations: \\^ Plot \\^ Collar number \\^ Location Desc \\^ Coordinates \\^ Notes \\^ | 1 | 1 | Litterbags 1 | | | | 1 | 2 | Litterbags 2 | | | | 1 | 3 | Litterbags 3 | | | | 1 | 4 | Litterbags 5 | | | | 2 | 1 | Litterbags 2 | | | | 2 | 2 | Litterbags 3 | | | | 2 | 3 | Litterbags 5 | | | | 2 | 4 | Litterbags 6 | | | | 3 | 1 | Litterbags 1 | | | | 3 | 2 | Litterbags 2 | | | | 3 | 3 | Litterbags 4 | | | | 3 | 4 | Litterbags 5 | | | | 4 | 1 | Litterbags 2 | | | | 4 | 2 | Litterbags 3 | | | | 4 | 3 | Litterbags 4 | | | | 4 | 4 | Litterbags 5 | | | | 5 | 1 | Litterbags 1 | | | | 5 | 2 | Litterbags 3 | | | | 5 | 3 | Litterbags 4 | | | | 5 | 4 | Litterbags 6 | | | | 6 | 1 | Litterbags 2 | | | | 6 | 2 | Litterbags 3 | | | | 6 | 3 | Litterbags 4 | | | | 6 | 4 | Litterbags 5 | | |</p>"},{"location":"hiddencanyon/trees/","title":"Trees","text":""},{"location":"hiddencanyon/trees/#treespecies-count-in-the-plot-approximate","title":"Tree/species count in the plot (approximate)","text":"<ul> <li>//Picea engelmanii// = 60</li> <li>//Abies lasiocarpa// = 360</li> <li>//Populus tremuloides// = 153</li> </ul>"},{"location":"hiddencanyon/westerndust/","title":"Westerndust","text":"<p>FIXME - add Milford Flat dust info</p>"},{"location":"hiddencanyon/westerndust/#dust-composition-and-origin","title":"Dust composition and origin","text":"<p>[[media/misc/dustmap.jpg|width=300px|align=left|alt=\"Dust collection sites along the Buckhorn Draw Road (San Rafael Swell, UT\"]] Dust used in this project is derived from the Chinle-Moenkopi formation of the Colorado Plateau. The nutrient and chemical composition has been described in Neff et al, 2006. This dust was collected on 20 Feb, 2010 along the Buckhorn Draw Road in the San Rafael Swell of Utah. This material was wet at the time of collection. It was dried and then sifted to .5mm before application.</p>"},{"location":"hiddencanyon/westerndust/#chinle-1","title":"Chinle 1","text":"<p>Coordinates: 39.112961\u00b0 N 110.680463\u00b0 W</p> <p>The road cuts directly below a Chinle/Moenkopi (probably Chinle) exposure and there is material collected between the road and the base of this wall. Brick red, material is directly below outcrop and is rather shaley still (blocky pebbles mixed with finer material).</p>"},{"location":"hiddencanyon/westerndust/#chinle-2","title":"Chinle 2","text":"<p>Coordinates: 39.111881\u00b0 N 110.679688\u00b0 W</p> <p>Around the corner (SE) the road cuts throuh a fan of soil and rock debris below multiple chinle outcrops, with a wingate cliff towering above it. The fan is lightly vegetated with junipers, rabbitbrush, etc. This material was dug out of the roadcut. It is also brick red, slightly lighter, and is generally finer textured than Chinle 1. There are inclusions of a beige to blue crumbly sandstone within it. It may be a combination of Chinle/Moenkopi and weathered Wingate/Navajo from above.</p>"},{"location":"instruments/cr10dataloggers/","title":"Campbell CR10 dataloggers","text":"<p>Notes on using a very old style datalogger from Campbell Scientific. This requires a bit more effort than newer dataloggers.</p>"},{"location":"instruments/cr10dataloggers/#serial-to-cs-io-interface","title":"Serial to CS-I/O interface","text":"<p>CR10s have only one serial port on the datalogger panel, and it is not a standard RS-232 serial port. This means it needs a special interface to use it rather than just an RS-232 serial connection to an instance of Loggernet running on a computer. This can be done through CS-I/O-only devices, such as a Campbell 10KD keypad (see below), or using a special interface between a computer serial port (standard RS-232) and the datalogger (CS-I/O). The SC32A is what we use as an interface between Loggernet and the CR10, either as a direct connection to the datalogger's CS-I/O port, or connected to it via the 3-ended serial cable between the CR10 and its storage module.</p> <p>Once the SC32A is in place, Loggernet should work as normal and a program can be transferred, data downloaded, input locations monitored, etc.</p>"},{"location":"instruments/cr10dataloggers/#using-the-cr10kd","title":"Using the CR10KD","text":"<p>The CR10KD is a keypad and screen that can interface with the CR10. It is connected to the SC12 cable in series with the datalogger and any attached storage modules. Once attached, key commands can be used to put the datalogger in different modes to recieve commands.</p>"},{"location":"instruments/cr10dataloggers/#cr10kd-keypad-commands","title":"CR10KD keypad commands","text":"<ul> <li><code>*0</code> - Show running table status</li> <li><code>*5</code> - View and set clock</li> <li><code>*6</code> - View input locations</li> <li><code>*7</code> - Display Final Storage data</li> <li><code>*9</code> - Storage module commands (download data, etc)</li> <li><code>*D</code> - Save/load programs to or from storage module</li> <li>Once a mode above has been entered, values in different locations can be accessed with:</li> <li><code>A</code> - advance one location</li> <li><code>B</code> - go back one location`</li> </ul>"},{"location":"instruments/cr10dataloggers/#working-with-campbell-storage-modules","title":"Working with Campbell storage modules","text":"<p>Storage modules (SM192, or SM4M) are swapped out at each weather stations at regular intervals. The full storage modules are then returned to the lab to download the data, and then erase and test the module (procedures below).There may be a way to do some of this with the CR10KD (downloading data at least) in the field but... no idea how.</p>"},{"location":"instruments/cr10dataloggers/#procedure-to-download-data-and-erasetest-modules-in-the-lab","title":"Procedure to download data, and erase/test modules in the lab","text":"<ul> <li>Connect the SC32A to a compatible datalogger (CR10, CR23x) and then to the storage module with an SC12 (3 ended) cable. Make sure the datalogger is turned on.</li> <li>Connect the other end of the SC23A to a computer's serial port (you will need a 9pin-250pin adapter - and possibly a USB-RS-232)</li> <li>Open LoggerNet on the computer and then choose the Storage Module (SMS) application from the menubar.</li> <li>There are tabs here for each type of storage module - choose the one that is appropriate (RBC stations will use SM192/716 or SM4M/16M).</li> <li>Select the COM port (COM1 probably) and select Via Datalogger, then select the datalogger model (if asked).</li> <li>Click <code>Connect</code>, and some information should appear in the StatusBox at the right.</li> <li>Download the module's stored data using the \"Data\" tab, putting the data in a new file.</li> <li>Erase and test the module using the \"Erase\" tab (should be pretty self explanatory).</li> <li>Be sure to mark the last time the module was erased/tested on a piece of tape attached to the module.`</li> </ul>"},{"location":"instruments/cr10dataloggers/#uploading-a-program-to-a-storage-module-then-to-a-datalogger-in-the-field","title":"Uploading a program to a storage module, then to a datalogger in the field","text":"<ul> <li>Connect a storage module as in steps 1-6 above</li> <li>Select Programs tab</li> <li>Use store button to place a program in one of the program locations (8 in an SM192).</li> <li>Disconnect module, and make a note of where the program is located</li> <li>In the field connect the storage module to the SC12 cable and then attach the CR10KD keypad to the other end of the cable.</li> <li>Use the program transfer mode (*D) to transfer programs. On the CR10D keypad press (treat A like enter?):</li> <li><code>*D</code> - Enter *D Mode</li> <li><code>7XA</code> -  Address Storage Module X (1-8 - in our case there is only one module, so use 1)</li> <li><code>1YA</code> - Save Program in Storage Module as Y (Y=1..8 - the location where the program is/was stored)</li> <li><code>2YA</code>  - Load Program Y from Storage Module</li> <li><code>3YA</code>  - Erase Program Y from Storage Module`</li> </ul>"},{"location":"instruments/cr10dataloggers/#setting-the-clock","title":"Setting the clock","text":"<ul> <li><code>*5</code> - Enter clock mode</li> <li><code>AXXXXA</code> - Advance to year, then enter year (XXXX), save with A.</li> <li><code>AYYYA</code> - Advance to day of year, enter doy (YYY), save with A.</li> <li><code>AHHMMA</code> - Advance to time, enter hours and minutes(HHMM), save with A.</li> <li><code>*0</code> - Back to status mode.</li> </ul>"},{"location":"instruments/cr10dataloggers/#preventing-data-loss","title":"Preventing data loss","text":""},{"location":"instruments/cr10dataloggers/#power-outages","title":"Power outages","text":"<p>CR10 dataloggers lose all stored data and programs when they power off because they do not have battery-backed memory (or power-free memory) like later dataloggers. This means that a new program will need to be loaded following a power outage. When powering up, the datalogger looks for an attached storage module, and will automatically load and run a program from storage module location 8 if it exists. So, when storage modules are swapped out, it pays to have the logger program loaded in location 8 of the replacement module.</p> <p>Load programs into storage module locations using the Loggernet SMS utility.</p>"},{"location":"instruments/ea-irms_sirfer/","title":"EA-IRMS analysis at SIRFER (U of U)","text":"<p>This protocol was adapted from Brad Erkkila's and Abby Howell-Dinger's descriptions of how to run samples on the EA-IRMS systems at the University of Utah's SIRFER facility. It mainly addresses C and N analysis (% and 13C/15N) of soil samples, but could be adapted for other uses.</p>"},{"location":"instruments/ea-irms_sirfer/#instruments","title":"Instruments","text":"<p>There are two IRMS units that have elemental analyzer (EA) front ends available, Madeye Moody, and Saltbush Bill. Madeye has a larger peak area range and is therefore more suitable for soils (which vary radically in C &amp; N concentrations). The following protocol is suitable for both units, but there are some slight adjustments depending on which machine is being used.</p> <ul> <li>Reference gases (which are selected through the Conflo interface) are plumbed differently</li> <li>Saltbush reference 1 = N~2~, reference 2 = CO~2~</li> <li>Madeye is reversed - reference 1 = CO~2~, reference 2 = N~2~</li> <li>Valve placement is a bit different</li> <li>The autoloader He pressure valve is directly under the autosampler on Madeye, and behind the EA instrument on Saltbush.`</li> </ul>"},{"location":"instruments/ea-irms_sirfer/#protocol","title":"Protocol","text":"<ul> <li>Close the isolation valve below the autosampler.</li> <li>Loosen the three screws holding the top of autosampler and open the door. If it won't open after the screws are moved away, briefly pressurize the autosampler chamber with He gas (then turn off).</li> <li>Make sure that well 50 is aligned over the hole into the combustion chamber, and that wheel advances such that samples will not be able to hang in the wheel.</li> <li>It may be necessary to advance the wheel a half turn manually (using manual advance box to right of sampler).</li> <li>Use the manual advance button, with the toggle at 50 or 100 samples, as needed to check this.</li> <li>Load the samples into the sample wheel in sequential order (1-50, or 1-100 depending on which wheel is being used).</li> <li>Close the sampler and replace and tighten the screws.</li> <li>Open the vacuum to the autosampler using the green valve behind it.</li> <li>Tighten the screws on the autosampler door again. The autosampler chamber will continue to be evacuated as the next steps are completed.</li> <li>Fill out the <code>elog</code> with the data for this EA-IRMS run (name, job #, etc). The <code>elog</code> is an excel file found on the desktop of the unit.</li> <li>Open the EA loader spreadsheet and enter sample ID and weight information.</li> <li>Open MS Excel &gt; recently used documents &gt; more, and open either the 50 or 100 sample EA_loader.xls file.</li> <li>Enter sample ID's in column A, starting with row 16 (the first 15 are conditioners).</li> <li>Enter sample weights in column B (?) using micrograms. These will be automatically converted to milligrams in the next column to the right.</li> <li>Now open the IsoDat Acquisition software, located in the Windows XP quick launch bar (lower left of desktop).</li> <li>In Acquisition, we must open a measurement instruction sequence.</li> <li>These are located in the Sequences tab on the lower left window of the interface.</li> <li>For CN analysis we choose <code>N2_CO2+He_1.seq</code>:?:</li> <li>This should open a datasheet window and some other stuff.</li> <li>If the instrument has recently been used with another front end it will need to be changed back to the EA Conflo interface.</li> <li>This is done using a cluster of valves below the left side of the mass spec.</li> <li>Close the pinvalve marked Otto (on Madeye :?:).</li> <li>Open the EA pinvalve.</li> <li>Make sure the green MS valve is open.</li> <li>Now copy and paste the sample ID's and weights from the EA_loader.xls file to the Acquisition datasheet. Column A (IDs) should go in Acquisition's <code>Identifier1</code> column, and Column D (weights) should be copied to the <code>Amount</code> column.</li> <li>Make sure that the column to the right of Identifier 1 correctly shows which samples are samples, which are references (with an RM), and which are other types of samples.</li> <li>Also make sure that the Methods column lists the correct method for all the samples (N2_CO2+He_1.seq).mmilodela</li> <li>Fill out the Comments column - this collects quality control and calibration data for every run.</li> <li>List the pressure (to the nearest 100psi) of all the reference gases<ul> <li>Primary He (Orange)</li> <li>Secondary He (Orange - regulator marked secondary)</li> <li>N~2~ (Blue)</li> <li>CO~2~ (Silver)</li> <li>O<code>~</code>2`~(Green - in middle of room).</li> </ul> </li> <li>Fill in mV value for a peak center on N~2~ (mass 28) and CO~2~ (mass 44).<ul> <li>Select N~2~ in the dropdown on the lower left of the interface</li> <li>Open the Conflo interface (middle left window of interface) and turn off all reference gases other than N~2~ `.</li> <li>Push the peak center button (green bell curve on upper far left).</li> <li>Do the same process for CO~2~ (first changing dropdown).</li> <li>There is also the possibility to do a jump calibration (ask Abby or Leslie Chesson).</li> </ul> </li> <li>Check the background values for N~2~<ul> <li>Switch dropdown menu to N~2~</li> <li>Turn off all reference gases in the Conflo interface</li> <li>Record mV for mass 28 and 29.</li> </ul> </li> <li>Time to jump<ul> <li>Open methods tab (lower left of interface)</li> <li>Go to the method being used N2_CO2....</li> <li>Go to time events</li> <li>Add together the CO~2~ switch time and the wait time (should be something like 290).</li> </ul> </li> <li>Record the number of samples on the combustion column (counter on front of EA).</li> <li>Record the furnace temperature</li> <li>Now close the vacuum valve.</li> <li>Pressurize the autosampler chamber with He gas, gradually letting the pressure rise to the mark on the chamber gauge.</li> <li>Open the isolation valve.</li> <li>In the IsoDat Acquisition interface, highlight the datasheet rows containing the samples to be run.</li> <li>Open the <code>Acquisition</code> menu and select <code>Start</code>.</li> <li>In the dialogue that opens,</li> <li>Add a folder name (usually the job number)</li> <li>Delete acquisition from the filename field.</li> <li>Export the file name to job #.</li> <li>Check the background (should show low N2 background #s) before hitting OK</li> <li>The job is now running - watch the first samples go through</li> <li>there should be a flat-topped reference gas set of peaks, then 2 peaks for N, a blip related to the IRMS jump, then 3  peaks for C, and another flat topped reference peak.</li> </ul>"},{"location":"instruments/ea-irms_sirfer/#data-reduction","title":"Data reduction","text":"<ul> <li>Download data from the EA-IRMS computer.</li> <li>Link to EA-IRMS results on the desktop.</li> <li>Datafile will be in the directory full of Excel files.</li> <li>Open the CHNOS template. This spreadsheet has:</li> <li>Two sheets for raw data (Sample gas 1= N, Sample gas 2 = C).</li> <li>Two data correction template sheets.</li> <li>Copy the first gas (N) and second gas (C) raw data and header row over to appropriate raw data sheets. Again, N is sample gas 1, and C is sample gas 2.</li> <li>Make sure that the headers match what is in the CHNOS template, then delete the extra header.</li> <li>Sort the rows by peak number, so that sample peaks are at the top of each sample gas sheet.</li> <li>Peak 1 = reference peak</li> <li>Peak 2 = N peak</li> <li>Peak 3 = C peak</li> <li>Peak 4 = reference peak</li> <li>Insert 5 blank rows between peaks 1 and 2 (Sample gas 1 sheet), and between peaks 4 and 3 (Sample gas 2 sheet).</li> <li>Select primary and secondary references from the dropdown menus in the upper left of the data correction template.</li> <li>Enter the start row and end row in the correction sheet for each gas. The correction sheet should then populate with the correct values from the sample gas sheets.</li> <li>Count the number of lines.</li> <li>Are they the same for each gas (were any peaks too small)?</li> <li>Does it match the number of samples?</li> <li>Enter the latest jump value in its cell - usually this is 290 for CN analysis.</li> <li>Enter the values for the secondary reference material (mean and sigma). All secondary reference results (right side of correction sheet) should then populate with the number of standard deviations. Consider discarding any over 2.</li> <li>For each of the sample gases:</li> <li>Plot the reference gas peaks</li> <li>Plot the background values</li> <li>Look for peak areas less than 15 (N) - these are likely too small.</li> <li>The chromatograms (peaks) can also be examined on the machine itself (the entire run is saved).</li> <li>Open the <code>Workspace</code> program.</li> <li>Open the directory named with the job number.</li> <li>All chromatograms are saved according to number, double click to open.</li> <li>There is a table under the chromatogram with a column for <code>Area All</code> with volt-seconds units.</li> <li>For N peaks, peak areas less than 15 volt-seconds are too small.</li> <li>For C peaks, large samples will result in squared-off peaks (sensor maxed out).</li> <li>Check whether any corrections are justified.</li> <li>Import the references into the filemaker database.</li> </ul>"},{"location":"instruments/li-190/","title":"Li-Cor LI-190 PPFD sensors","text":"<p>These sensors measure PAR, photosynthetically active radiation (wavelengths from 400-700 nm) as photosynthetic photon flux density (PPFD). PPFD is typically measured in \u03bcmol s^-1^ m^2^.</p>"},{"location":"instruments/li-190/#output-wiring-and-multipliers","title":"Output, wiring, and multipliers","text":"<p>These sensors output \u03bcA across two leads and a multiplier is used to relate the \u03bcA signal to \u03bcmol s^-1^ m^2^. Multipliers are typically in the range of -140 to -170 \u03bcmol s^-1^ m^-2^ per \u03bcA. The two leads are a bare shield (positive) and a center conductor (negative). This output can be converted to mV (which is read by Campbell dataloggers) using a resistor as long as the multipliers are also converted via Ohm's law. A shunt resistor (roughly 600 Ohms) is placed either between the datalogger terminals or across the leads using an adapter supplied by Li-Cor.</p>"},{"location":"instruments/li-190/#wiring-to-campbell-dataloggers","title":"Wiring to Campbell dataloggers","text":"<p>\\^ Diff H | Li-Cor Shield (Pos), resistor | \\^ Diff L | Li-Cor center conductor (Neg), resistor, ground jumper | \\^ gnd | ground jumper |</p> <p>Note: Li-Cor recommends connecting the wires in the reverse         way (Pos--&gt;Diff L, Neg--&gt;Diff H) to reduce interference,         but this gives only negative values on a Campbell CR23x, even if         the multiplier is entered as a negative value in the         datalogger program.</p>"},{"location":"instruments/li-190/#converting-the-multiplier-to-mv","title":"Converting the multiplier to mV","text":"<p>\u03bcA current output can be converted to mV using Ohm's law (Voltage = Current * Resistance). Multipliers (<code>mult</code>) are given as -\u03bcmol s^-1^ m^2^ per \u03bcA. A resistor has a voltage in Ohms. The steps to convert to \u03bcmol s^-1^ m^2^ per mV are:</p> <ul> <li>Convert <code>mult</code> to \u03bcA/1000\u03bcmol: 1000\u03bcmol/<code>mult</code></li> <li>Convert multiplier from \u03bcA to A: <code>mult</code> * 1A/10^6^\u03bcA</li> <li>Change amps to volts: mult` * R (R = resistance of shunt resistor in Ohms)</li> <li>Multiplier is now in V/1000\u03bcmol s^-1^m^2^`</li> <li>Change multiplier to mV: <code>mult</code> * 1000</li> <li>Reciprocal of this number(1000/<code>mult</code>) is the multiplier in \u03bcmol s^-1^m^-2^/mV//</li> <li>In short: $<code>mult</code>/0.001A * R$</li> </ul> <p>Note: Li-Cor and Campbell recommend a 604 Ohm resistor.         There may be problems especially with higher resistances.</p>"},{"location":"instruments/li-190/#edlog-instructions","title":"EDLOG instructions","text":"<p>These sensors are generally read using code like this:</p> <pre><code>Measure UPWARD looking Quantum sensor - Q27246\nBridged with a 604ohm resistor to convert uA to mV\n\n39: Volt (Diff) (P2)\n\n 1: 1        Reps\n 2: 41       10 mV, 60 Hz Reject, Fast Range\n 3: 5        DIFF Channel\n 4: 30       Loc [ PAR_Up    ]\n 5: 1.0      Multiplier\n 6: 0.0      Offset\n\nSet negative values to zero\n\n40: If (X&lt;=&gt;F) (P89)\n\n 1: 30       X Loc [ PAR_Up    ]\n 2: 4        &lt;\n 3: 0.0      F\n 4: 30       Then Do\n\n   41:  Z=F x 10^n (P30)\n    1: 0        F\n    2: 0        n, Exponent of 10\n    3: 30       Z Loc [ PAR_Up    ]\n\n42: End (P95)\n\nConvert mV to umoles m\\^-2 s\\^-1 using converted multiplier\n\n43: Z=X\\*F (P37)\n\n 1: 30       X Loc [ PAR_Up    ]\n 2: 243.112  F\n 3: 30       Z Loc [ PAR_Up    ]\n\n</code></pre>"},{"location":"instruments/li-190/#deployed-sensors","title":"Deployed sensors","text":"<p>\\^ Serial no. \\^ Location \\^ Measurement \\^ \u03bcA Multiplier \\^ Resistor \\^ mV Multiplier \\^ | Q27246 | Hidden Canyon - Met tower | Upward looking PPFD | -146.84 | 604\u03a9 | 243.112 | | Q33398 | Hidden Canyon - Met tower | Downward looking PPFD | -158.23 | 560\u03a9 | 282.553 |</p>"},{"location":"instruments/li-6400/","title":"Li-Cor 6400 with Soil Respiration Chamber","text":"<p>This instrument measures soil respiration fluxes at the soil surface. It consists of the Li-Cor 6400 Portable Photosynthesis system, coupled to the Li-Cor 6400-09 Soil CO~2~ Flux Chamber.</p> <p>This system is currently in use measuring soil respiration at Hidden Canyon and site-specific additions to this protocol will be posted on the linked page.</p>"},{"location":"instruments/li-6400/#measurement-protocol","title":"Measurement protocol","text":"<p>This protocol is adapted from the one compiled and edited by Claire Lunch and Suzanne Bethers in 2003 and 2004 for use at Corral Pocket.</p> <p>Notes in italics signify parameters or settings that will change depending on the site's flux rate or environmental conditions. See the notes at the end of the protocol.</p> <ol> <li>Connect tubing and electronics - see illustrations in 6400-09 manual.</li> <li>Turn on 6400</li> <li>Select <code>6400-09 Soil Chamber</code> configuration and answer <code>Y</code> to <code>Is chamber/IRGA connected?</code> </li> <li>Select <code>New Msmnts</code> (F4) from main menu</li> <li>Select menu level 1 and press <code>Open Log File</code> (F1)</li> <li>Enter name of soil respiration file in form <code>SITE_yymmdd</code>, and press enter when asked for comments.</li> <li>Select menu level 3 (press 3 on keyboard) and press <code>Aux OP Param</code> (F2) and set prompts to the following:</li> <li><code>Extra Draw Down (ppm)</code>= 0 (or more at higher fluxes)</li> <li><code>Flow During Draw Down</code> = 350 (or more at higher fluxes)</li> <li><code>Dead Time (secs)</code> = 15 (or more at higher fluxes)</li> <li><code>Min Measurement Time (secs)</code> = 30</li> <li>Select menu level 7, press <code>Log Only Final</code> (F4) and select <code>Everything</code></li> <li>Press <code>Esc</code> to return to main menu</li> <li>Allow instrument to warm up for about 20 minutes</li> <li>Zero the IRGA (Do this every time):</li> <li>Disconnect the \"Console Return\" hose from the \"Chamber Inlet\" hose (both are marked with black shrink wrap).</li> <li>Unscrew the plug from the \"To-Sample\" hose (at top of IRGA assembly, marked with black wrap) and plug the \"Chamber Inlet\" hose (disconnected from console in Step 1) with it.</li> <li>Connect the \"Console Return\" hose to the \"To-Sample\" tube (black wrap to black wrap)</li> <li>Turn scrub/bypass knobs (upper, larger knobs) on both chemicals to full scrub</li> <li>From the main menu on the 6400 press <code>Calib Menu</code> (F3)</li> <li>Choose <code>IRGA Zero (CO2S, H20S)</code> and say <code>Y</code> to the questions that follow</li> <li>Wait for CO2S and H2OS to become stable close to zero (or just below). If they do not drop to zero check the scrub chemicals and knobs, and all plumbing. </li> <li>When numbers are stable choose <code>Zero CO2&amp;H20</code> (F3)</li> <li>Press <code>Quit</code> (F5) to return to calibration menu</li> <li>Span (can be done in the lab periodically, with a small field cal tank, or with atmospheric air)</li> <li>Disconnect the \"Console Return\" tube from the \"To-Sample\" tube</li> <li>Connect the span gas tube (from toggle valve on cal tank) to the \"To-Sample\" tube</li> <li>In calibration menu, choose <code>IRGA Span</code> and say <code>Y</code> to the questions that follow</li> <li>Throw toggle to open the cal tank valve</li> <li>Wait for CO2S to become stable</li> <li>Close the toggle valve on the cal tank and reconnect the \"Console Return\" tube to the \"Chamber Inlet\" tube (while flow is on, the \"Console Return\" tubing inlet acts as a vent; when flow is off, gas can diffuse back into the chamber through that vent)</li> <li>Press <code>CO2S</code> (F2)</li> <li>Use arrow keys to adjust CO2S to the cal tank value (measured)</li> <li>Press <code>Done</code> (F5) to return to calibration menu</li> <li>Select <code>View, Store, Zeros &amp; Spans</code></li> <li>Press <code>Store</code> (F1)</li> <li>Press <code>Quit</code> (F5) to return to calibration menu</li> <li>Press <code>Esc</code> to return to main menu</li> <li>Re-plumb the system to measurement setup (see illustrations in manual) and remove the white cap from bottom of chamber</li> <li>Turn the knob on the CO~2~ scrub (soda-lime) one half-turn away from full bypass (It will probably be necessary to fiddle with this later -- watch the CO~2~ drop during scrubbing: it should fall steadily and overshoot <code>Target - Delta</code> slightly, but not my more than 5-10 ppm)</li> <li>Select <code>New Msmnts</code> (F4) from the main menu</li> <li>Insert the soil temperature probe to 5-15cm depth near soil collar (Currently this probe is broken, and it sits on the chamber and measures chamber/air temperature. A separate probe is used to measure the soil at 5 &amp; 15 cm depths)</li> <li>Lay the chamber on its side close to the ground. Face away from the chamber or hold your breath. Wait for CO2S (Monitor in <code>New Measurement</code> mode) to stabilize. This is <code>Target</code> (unlikely to change from plot to plot, but it is a good idea to check CO2S before each plot to make sure you haven't breathed in the chamber or hit a funny CO~2~ pocket)</li> <li>Measure the height of the soil collar rim above the interior soil level. Subtract 2cm, then multiply by -1. This is <code>Insertion depth</code>.</li> <li>Select menu level 7 and press <code>Start</code> (F3)</li> <li>You will be prompted for:</li> <li><code>Target</code>: found in step 18</li> <li><code>Delta</code>: 1 is default for low flux rates, but this is the most useful parameter to adjust for varying conditions. When fluxes are high <code>Delta</code> of 2 or 3 may be necessary. When live plants are present in the collars, <code>Delta</code> = 5-10. See general comments below.</li> <li><code>Plot #</code>: enter the number of the plot being measured</li> <li><code>Insertion Depth</code>: found in step 19</li> <li>The last prompt is <code>Append to current log file? y/n</code>. Place soil chamber gently on collar (don't press down) and press <code>Y</code></li> <li>Write the following in the field book as the measurement cycles take place.</li> <li>Time</li> <li>Plot #</li> <li>Insertion Depth</li> <li>Target</li> <li>Efflux for each of the 3 measurement cycles - the 6400 beeps during measurement and the instantaneous efflux is displayed. When the beeping stops and the machine pumps down for the next measurement, the final efflux value for the previous cycle is displayed. This number should be recorded (There will be 3 cycles per measurement).</li> <li>Soil temp at 5 &amp; 15 cm depth during the measurement</li> <li>Air temp inside the chamber (at the end of the final measurement cycle), displayed under <code>Tsch_C</code> \\</li> <li>Air temp outside the chamber (<code>Tsoil_C</code> while the Li-Cor probe is busted)</li> <li>Note anything unusual!!</li> <li>When the machine is done move to the next collar and repeat from step 17.</li> </ol>"},{"location":"instruments/li-6400/#measurement-notes","title":"Measurement notes","text":"<ul> <li>Steps 5-8 must be performed every time the 6400 is turned off and back on!</li> <li>Battery voltage is displayed on the main menu and also under display <code>g</code> in <code>New</code> <code>Measurements</code> mode. It is a good idea to check this once in a while, but a good pair of batteries generally lasts 3-4 hours. Think about changing them when voltage drops below 12v.</li> <li>Keep the console out of direct sunlight, especially if it is hot. It must be shaded with something or else the screen will eventually go blank (LCD problem)</li> </ul>"},{"location":"instruments/li-6400/#site-variability-notes","title":"Site variability notes","text":"<p>These directions were written for use at Corral Pocket, which has extremely low CO~2~ fluxes. At higher productivity sites several things will likely need changing:</p> <ul> <li>Turn up the soda-lime scrub knob</li> <li>Increase the flow during draw-down and/or add an extra draw-down</li> <li>Increase the dead time</li> <li>Increase <code>Delta</code> </li> </ul> <p>Increasing <code>Delta</code> ensures that [CO~2~] does not shoot through the measurement range too fast. The other changes accomodate the need for a overshoot of <code>Target-Delta</code>. There is an initial spike in [CO~2~] following the scrub, which is physical rather than biological and should not be measured. By extending the time and CO~2~ range before measurement begins, you avoid including that spike.</p> <p>At low fluxes, the most time-consuming part of the measurement can be waiting around while CO~2~ rises after the scrub. Keeping the scrub/bypass know on the soda-lime as close to bypass as possible is helpful, as is a careful choice of <code>Delta</code> value. <code>Delta</code> should be large enough so that rising from <code>Target-Delta</code> to <code>Target+Delta</code> takes some time (the machine should beep a dozen times or so), but small enough so that [CO~2~] actually reaches <code>Target+Delta</code> (or close to it) before the machine stops the measurement automatically. If [CO~2~] does not even reach <code>Target</code>, <code>Delta</code> is too large.</p> <p>If the scrub seems to overshoot <code>Target-Delta</code> by a large margin, even with the scrub turned low, try a different <code>Target</code>. For mysterious reasons sometimes changing it by 1 or 2 ppm improves the situation. If fluxes are painfully low and nothing speeds up the process, drop the number of cycles from 3 to 2 (in <code>New Measurements</code> menu level 7)</p>"},{"location":"instruments/li-6400/#downloading-soil-respiration-data","title":"Downloading soil respiration data","text":"<ul> <li>Connect the RS-232 cable to the right side of the 6400 console and to the serial port of the computer.</li> <li>Turn on the 6400 (chamber, soil probe, etc do not need to be attached).</li> <li>Select <code>Soil Chamber</code> configuration and say <code>N</code> to the <code>Is chamber/IRGA connected?</code> question. It will tell you how to connect the IRGAs; press <code>Enter</code></li> <li>Select the <code>Utility Menu</code> (F5) and chose <code>File Exchange Mode</code>. Screen should read <code>waiting to establish connection</code></li> <li>On the computer...FIXME</li> </ul>"},{"location":"instruments/li-6400/#maintenance","title":"Maintenance","text":"<ul> <li>Keep dust off of everything if possible, especially the tubing and fans. Don't set the chamber down on the ground unless the cap is on.</li> <li>Don't let it get wet. If measurements must be made in rain, cover everything - console and chamber - with plastic bags. Remember that a rainstorm is not a good time to make accurate respiration measurements.</li> <li>When the Drierite becomes mostly pink it is time to change both canisters of chemicals. The canisters can be unscrewed from the console using the small knobs below the scrub/bypass knobs. Do not empty or fill the canisters through the top  - unscrew the bottom cap. Use gloves, don't inhale the dust, put the old stuff in waste containers, etc.</li> </ul>"},{"location":"instruments/li-6400/#maintenance-log","title":"Maintenance Log","text":"<ul> <li>The Li-Cor temperature probe is currently broken. Substitute the omega probe for soil temp measurements.</li> <li>Replaced chemical traps (Drierite and Soda-lime) and bought 2 new batteries in July 2010.</li> <li>Fixed a leak around the top of the soil respiration collar in July 2012.</li> </ul>"},{"location":"instruments/sdi12/","title":"Using SDI12 sensors","text":""},{"location":"instruments/sdi12/#general","title":"General","text":"<p>SDI12 is a protocol for addressing and interacting with sensors in a network configuration. SDI12 devices have a unique address that </p>"},{"location":"instruments/sdi12/#wiring-for-campbell-dataloggers","title":"Wiring for Campbell dataloggers","text":"<p>SDI12 devices generally have at least 3 wires - power, ground, and serial communication. For a Campbell datalogger device the power will come from a 12V port (12V or SW12V), ground is G, and the serial wire connects to one of the COM ports (C1-C8). there may also be ground and shield wires for the serial circuit that should be connected to a G terminal.</p>"},{"location":"instruments/sdi12/#changing-sdi12-addresses","title":"Changing SDI12 addresses","text":"<p>SDI12 devices on a circuit (connected to same serial port) need to have unique addresses. Potential addresses are \"0-9\",\"a-z\", and \"A-Z\" (62 total). If the datalogger has multiple serial ports available these addresses may be reused in SDI12 commands through these additional ports.</p> <p>SDI12 commands can be sent through a terminal program using the SDI12 protocol. The serial port location of the sensor must be specified. The basic SDI12 command that can be sent once connected to the SDI12 port through a terminal are:</p> <pre><code>?!   # Query the current SDI12 address of the connected sensor\ncAx! # Assign device to new address - \"c\" is current and \"x\" is new address\n</code></pre> <ul> <li>For addressing sensors with a Campbell datalogger see the pdf linked from here</li> <li>Info on this should also be in the sensor manual (esp Campbell sensors)</li> </ul>"},{"location":"instruments/sdi12/#datalogger-programming","title":"Datalogger programming","text":"<p>The <code>aM!</code> and <code>aMx!</code> SDI12 commands, where <code>a</code> is the SDI12 address and <code>x</code> is a command modifier, are used to poll the sensor. Command <code>aM!</code> returns three values: VWC, EC, and T. Note that some sensors use internal logical tests to remove erroneous values (outside operational limits or accuracy specs) and return errorvalues such as 99999. Depending on the sensor, these logical tests may be ignored by sending the correct <code>Mx!</code> command.</p> <p>In CR Basic, the command to read an SDI12 sensor is <code>SDI12Recorder()</code>. The</p>"},{"location":"instruments/sentek_enviroscan/","title":"Sentek EnviroSCAN profiles","text":"<p>These soil sensor profiles are inserted into a buried PVC tube to measure soil water content and salinity. The sensor profile assembly consist of a plastic probe, sensors placed at 10cm intervals along the probe, and an SDI-12 interface (circuitboard) at the top of the assembly. Sensors are connected to the SDI interface with a ribbon cable running along the probe. Installation and communication with old dataloggers are both a bit tricky.</p>"},{"location":"instruments/sentek_enviroscan/#installation","title":"Installation","text":""},{"location":"instruments/sentek_enviroscan/#standard-auger-method","title":"Standard auger method","text":"<p>This installation is for uniform, fine textured soils. Details here. In rocky mountain soils it is better to opt for the slurry method below (in my opinion).</p>"},{"location":"instruments/sentek_enviroscan/#slurry-method","title":"Slurry method","text":"<p>This installation is suitable for soils in which augering a uniform hole for the access tube is difficult. This includes rocky, non-uniform soils such as those found in subalpine forests. A hole slightly larger than the diameter of the access tube is augered to depth with a standard bucket auger. Rocks must be broken up and removed with the auger, so if this is not possible with the larger rocks, the hole will have to be relocated. Once a suitable hole is augered, it is filled ~1/3 full with a slurry mixture of grey cement, kaolinite clay powder, and water. A sealed PVC access tube is then inserted into the slurry and pushed down to the desired depth, and the slurry is allowed to set around the tube. The sensor probe is then inserted into the access tube and wired once the slurry is dry.</p> <p>In theory, the slurry around the access tube equilibrates with the soil water content of surrounding soil and allows good measurements of relative soil water content. This method does not allow accurate measurements of absolute soil moisture unless extensive calibrations are done using the slurry and field soil (this sounds like it would be very difficult in practice).</p>"},{"location":"instruments/sentek_enviroscan/#datalogger-wiring","title":"Datalogger Wiring","text":"<p>Our sensors have the old revision of the SDI interface board (v1.2). At the top of the board are 5 input/output pins labeled from 1 to 5. Most pins have broken off and they are now soldered to wires. The terminals are (in order):</p> <ul> <li>+12Vin</li> <li>not used</li> <li>not used</li> <li>Ground</li> <li>SDI-12 data</li> </ul> <p>The +12V and Ground wires must be connected to an appropriate power source (12V/SW12V and G on a Campbell 23x for example) and the SDI-12 data wire must connect to an SDI-12 capable serial port.</p>"},{"location":"instruments/sentek_enviroscan/#datalogger-control","title":"Datalogger control","text":"<p>Measurement commands are issued with SDI-12 from a serial port. For example, on a Campbell edlog datalogger, the SDI-12 Recorder instruction is issued using a control port and an SDI address. If there are multiple SDI devices on the same line, they may need separate SDI addresses to work. The sensor then takes measurements at each sensor along the probe and sends back a scaled frequency measurement and a water content that is derived from this scaled frequency (WC alone can also be sent).</p>"},{"location":"instruments/sentek_enviroscan/#edlog-program","title":"Edlog program","text":"<p>media/code/sentek_test_1.csi is a working program for measuring a Sentek probe with 4 sensors using a Campbell cr23x datalogger.</p>"},{"location":"instruments/vaisala_gmp252/","title":"Vaisala GMP 252 CO~2~ probe","text":""},{"location":"instruments/vaisala_gmp252/#general","title":"General","text":""},{"location":"instruments/vaisala_gmp252/#installation","title":"Installation","text":"<p>Fits inside a 1 inch Schedule 40 PVC pipe, but in order to fint a membrane the thinner-walled 1 inch class 200 will be preferable.</p> <p>Membrane materials:</p> <pre><code>* Tyvek 1443R - light (1.25oz/yd), thin (5.3mil), strong, water resistant, pretty easy to work with.\n* Gore-Tex socks - source?\n</code></pre>"},{"location":"instruments/vaisala_gmp252/#datalogger-programming","title":"Datalogger programming","text":"<p>Sensors can output data in several configurations, both analog (voltage and current) and digital (Modbus).</p>"},{"location":"instruments/vaisala_gmp252/#single-ended-voltage-measurement","title":"Single ended voltage measurement","text":"<p>With probes configured for analog output at 0-5V, a single ended or differential voltage measurement can be used to measure the probe. In Campbell dataloggers the relevant commands are SeVolt</p>"},{"location":"instruments/vaisala_gmp252/#temperature-pressure-and-other-compensations","title":"Temperature, pressure, and other compensations","text":"<p>Temperature compensation can be performed by </p>"},{"location":"math/anova/","title":"Analysis of Variance (ANOVA)","text":"<p>General resources</p> <p>A tutorial geared towards psychology research.</p>"},{"location":"math/anova/#one-way","title":"One Way","text":""},{"location":"math/anova/#two-way","title":"Two Way","text":""},{"location":"math/anova/#repeated-measures","title":"Repeated Measures","text":"<p>In R, repeated measures ANOVA needs to be fit using aov, so that a multi-stratum error term can be a part of the model (lm does not support this). lme (from the nlme library'') can also be used for this purpose, specifying the error terms in the random= argument.</p> <ul> <li>A compendium of resources</li> <li>http://statistics.ats.ucla.edu/stat/r/seminars/Repeated_Measures/repeated_measures.htm</li> </ul>"},{"location":"math/anova/#post-hoc-analysis","title":"Post-hoc analysis","text":"<p>If multiple treatments need to be compared after the experiment, a couple types of post-hoc analyses can be used to test whether the means of the treatments are significantly different. These are the Tukey range test (also known as Tukey-Kramer, or Tukey HSD) and Scheffe's method.</p>"},{"location":"math/correlation/","title":"Correlation","text":"<p>Data analysis often calls for testing whether two or more sets of numbers are related in some way (see Correlation_and_dependence). Here are some methods to test for or describe a relationship (usually a linear one) between random variables, or two sets of data.</p> <p>Resources</p> <ul> <li>John Cook's take</li> </ul>"},{"location":"math/correlation/#parametric","title":"Parametric","text":""},{"location":"math/correlation/#pearsons-product-moment-correlation-test","title":"Pearson's product-moment correlation test","text":"<p>This is a common test for a linear relationship between two variables which yields a correlation coefficient between 1 and -1. This test should give the same significance (p-value) as a simple linear regression on the same data.</p> <ul> <li>Use <code>cor.test</code> in R.</li> <li>Use <code>scipy.stats.pearsonr</code> or <code>pandas.DataFrame.corr(method='pearson')</code> in python</li> </ul>"},{"location":"math/correlation/#simple-linear-regression","title":"Simple linear regression","text":"<p>Linear regression is related to correlation and a sample correlation can be calculated as the square root of the R^2^ (Coefficient_of_determination), with the sign of the slope of the regression line (the coefficient of x).</p> <ul> <li>Use <code>lm</code> in R</li> <li>Use <code>regress</code> in MATLAB.</li> <li>Numpy has <code>polyfit</code> and Scipy has <code>linregress</code></li> <li>See other notes here.</li> </ul>"},{"location":"math/correlation/#non-parametric","title":"Non-parametric","text":"<p>If the relationship is non-linear or variance is not normally distributed, these may be useful. Both of the tests below can be used with <code>cor.test</code> in R. Python pandas also has these tests in <code>pandas.DataFrame.corr(method='spearman OR kendall')</code> in python</p>"},{"location":"math/correlation/#spearmans-rank-test","title":"Spearman's rank test","text":"<p>Ranks x and y datapoints and then does a correlation test between the ranked data. May demonstrate a correlation when a Pearson or linear model test do not.</p>"},{"location":"math/correlation/#kendalls-tau","title":"Kendall's Tau","text":"<p>Looks at pairs of data points and tests how frequently the relationship between the pair goes in one direction or the other.</p> <p>This paper uses Kendall's Tau for analyzing climate trends.</p>"},{"location":"math/correlation/#corrections-for-significance","title":"Corrections for significance","text":"<p>It can be hard to get a good significance value if you are doing multiple comparisons. Lots of people recommend against this, but the Bonferroni correction can be applied. Also, it may be useful to construct 95% confidence intervals around a correlation, and use that instead (does it include 0?).</p> <p>Further reading on this</p> <ul> <li>Gotelli and Ellison, Chapter 10</li> <li>SE question</li> </ul>"},{"location":"math/decay_turnover/","title":"Decay, turnover, and residence time of pools of stuff","text":"<p>Mathmatical models describing the decay or turnover of stuff. I use these to describe the docomposition of organic matter, the size of organic matter pools as a function of their inputs and outputs, but the same concepts and mathematical models are applicable to the decay of radionuclides, hydrologic turnover, and other phenomena.</p>"},{"location":"math/decay_turnover/#decay-functions-no-inputs","title":"Decay functions (no inputs)","text":"<p>The general function for decay of leaf litter (for example) is: </p> <p>$$L_t = L_0e\\^{-kt} $$ or, $$ ln \\frac{L_t}{L_0} = -kt $$</p> <p>where $L_0$ is the mass at time 0, $L_t$ is the mass at time $t$, and $k$ is the decomposition constant. The mean residence time, or time required for $L_0$ to decompose under steady state conditions equals $\\frac{1}{k}$.</p>"},{"location":"math/decay_turnover/#dual-pool-decay","title":"Dual pool decay","text":""},{"location":"math/decay_turnover/#turnover-functions-inputs-and-outputs","title":"Turnover functions (inputs and outputs)","text":"<p>When a pool of stuff has both inputs and outputs, the change in the pool with time is defined as</p> <p>$$ \\frac{\\partial S}{\\partial t} = I - kS$$</p> <p>where $I$ is the input to the pool, $k$ is the decomposition rate, and $S$ is the size of the pool.</p>"},{"location":"math/decay_turnover/#resources","title":"Resources","text":"<ul> <li>http://www.plantsciences.ucdavis.edu/agroecology/staff/documents/encycl.pdf</li> </ul>"},{"location":"math/integration/","title":"Integration","text":"<p>Need to calculate the area under a curve or data series? Here are some ways to do it.</p>"},{"location":"math/integration/#integrating-a-function","title":"Integrating a function","text":""},{"location":"math/integration/#integrating-using-samples","title":"Integrating using samples","text":"<ul> <li> <p>The scipy.integratepackage offers some ways to do this:`</p> <p>trapz         -- Use trapezoidal rule to compute integral from samples. cumtrapz      -- Use trapezoidal rule to cumulatively compute integral. simps         -- Use Simpson's rule to compute integral from samples. romb          -- Use Romberg Integration to compute integral from (<code>2**k + 1</code>) evenly-spaced samples.</p> </li> <li> <p>MATLAB also has similar functions.</p> </li> </ul>"},{"location":"math/linear_regression/","title":"Linear Regression","text":"<p>Bivariate (simple) and multiple regression models.</p> <p>In MATLAB, the backslash operator can be used, or functions such as polyfit and regress. See here</p>"},{"location":"math/linear_regression/#bivariate-regression-simple-linear-regression","title":"Bivariate regression (simple linear regression)","text":""},{"location":"math/linear_regression/#multiple-regression","title":"Multiple regression","text":"<ul> <li>This paper has an interesting approach (and references) for using multiple regression in climate trend analysis.</li> </ul>"},{"location":"math/linear_regression/#collinearity","title":"Collinearity","text":"<p>One of the worst pitfalls of multiple regression is in using the technique for multivariate datasets in which some or all of the independent variables are correlated. Collinearity is a VERY common phenomenon in environmental data. In this case, regression coefficients estimated in the model are generally not usable in describing the effect of the independent variables, even though the model may fit the data well. Interpretation of the model coefficients and hypothesis testing becomes difficult or impossible in this situation. Some resources on detecting and dealing with this situation:</p> <ul> <li>Detecting collinearity</li> <li>Relationships between independent variables can be assessed with correlation testsor simple linear regression.</li> <li>Dealing with collinearity</li> <li>Remove predictor variables</li> <li>Use principal components (or other ordination axes) as indpendent variables in the model (they are orthogonal).<ul> <li>Principal components regression can estimate coefficients for the original independent variables.</li> </ul> </li> <li>Partial least squares regression</li> <li>Ridge regression.</li> </ul>"},{"location":"math/linear_regression/#variations","title":"Variations","text":"<ul> <li>Time-series multiple regression (see this page) - Values of the dependent and/or independent variables at a previous timestep are incorporated into the model.`</li> </ul>"},{"location":"math/multilevel_models/","title":"Multilevel models","text":"<p>These are models for grouped data in which datapoints may be correlated, and there is a need to treat variance differently among the groups. They are variously called multilevel, mixed, or heirarchical/ models.</p> <p>Suitable for longitudinal data in which one is interested in estimating coefficients for groups or individuals within the population, rather than for the population at large.</p> <p>There is a chapter (10) in Ben Bolker's book about them.</p> <p>A few resources:</p> <ul> <li>Pinheiro and Bates, Mixed-Effects Models in S and S-PLUS. 2000, Springer</li> <li>Gelman and Hill, Data Analysis Using Regression and Multilevel/Hierarchichal Models. (website)</li> <li>West, Welsch, and Galecki, Linear Mixed Models: A Practical Guide Using Statistical Software. (website)</li> <li>Mixed effects modelsappendix to John Fox's book \"An R and S-PLUS Companion to Applied Regression\".</li> <li>http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models#16415\\</li> <li>http://stats.stackexchange.com/questions/17331/what-is-the-difference-between-generalized-estimating-equations-and-glmm?lq=1</li> </ul>"},{"location":"math/normalitytests/","title":"Testing for Normality","text":"<p>Many inferential statistical procedures require that data be normally distributed. Here are a few ways to check if this is the case.</p> <p>Note that the conventional wisdom that these tests must be applied before using normal theory statistical procedures is debated (see here and here). Unless the sample size is very small, it is probably best to proceed with the statistical procedures when the data are approximately normal. Assessing approximate normality might best be accomplished using the graphical methods below.</p>"},{"location":"math/normalitytests/#graphical-methods","title":"Graphical methods","text":"<p>Q-Q plots</p> <p>p-p plots</p> <p>Histograms or density plots</p>"},{"location":"math/normalitytests/#formal-tests","title":"Formal tests","text":"<p>These test AGAINST the null hypothesis that the data tested are normal. Low p values reject the null hypothesis, meaning that the data cannot be assumed to be normal. High p values fail to reject the null, and the data may (or may not) be normally distributed.</p>"},{"location":"math/normalitytests/#shapiro-wilks","title":"Shapiro-Wilks","text":""},{"location":"math/normalitytests/#anderson-darling","title":"Anderson-Darling","text":""},{"location":"math/normalitytests/#kolmogorov-smirnov","title":"Kolmogorov-Smirnov","text":""},{"location":"math/normalitytests/#resources","title":"Resources","text":"<ul> <li>Tutorial using R</li> </ul>"},{"location":"math/pca/","title":"Principal components analysis","text":"<ul> <li>Excellent for high-dimensional (multivariate) datasets that exhibit some collinearity. </li> <li>Most useful for hypothesis generation, not hypothesis testing.</li> <li>No division of dataset into dependent and independent variables.</li> <li>Reduces the dimensionality of complex data.</li> <li>There is no significance value or test of a null hypotheses given by the technique.</li> <li>Not necessarily an end in itself, but best followed with further inferential tests, such as ANOVA or regression on the first and second principal components.</li> </ul>"},{"location":"math/pca/#procedure","title":"Procedure","text":""},{"location":"math/pca/#1-choose-variables-to-include","title":"1. Choose variables to include","text":"<ul> <li>In descriptive studies it is often best to include all numerical variables measured.</li> <li>Probably best to leave out the dependent variables you are interested in.</li> <li>Check each variable for normality (see here)?</li> <li>Log transform each variable?</li> <li>Sometimes rare observations can be excluded.</li> </ul>"},{"location":"math/pca/#2-pca-calculations","title":"2. PCA calculations","text":"<p>The basic steps in this process are:</p> <ul> <li>Center and scale all variables (Often the PCA software will do this automatically).</li> <li>Calculate a correlation matrix for the chosen variables.</li> <li>Calculate the eigenvalues (% variance explained) and eigenvectors (loadings) of the correlation matrix.</li> <li>Calculate singular values from the square root of the eigenvalues (StdDev).</li> <li>Calculate axis scores for each observation on each axis.</li> </ul>"},{"location":"math/pca/#3-examine-variance-explained-by-the-principal-components","title":"3. Examine variance explained by the principal components","text":"<p>A set of principal components, as many as there are explanatory variables, are then produced. The first two or three principal components explain most of the variance in the data, and are therefore the most informative. There are a few common tests of which components to retain. The first is to reject any whose percent variance explained is less than $100/N$, where $N$ is the number of variables. Another method, called the broken stick model, calculates expected values for percentage variance given the number of variables. Axes with less variance than expected are excluded. Another, more simplistic method is subjective visual selection of components using a scree plot:</p> <p>Sometimes, a threshold at which percent variance explained drops is visible on the screeplot as an elbow among the bars. Axes below the elbow are rejected.</p>"},{"location":"math/pca/#4-examine-eigenvector-loadings-on-selected-components","title":"4. Examine eigenvector loadings on selected components","text":"<p>Once the number of principal components is selected, examine the eigenvector loadings. Loadings for each component indicate each variable's correlation with the component, so a high positive or negative loading indicates that the component describes some aspect of that variable's range.</p>"},{"location":"math/pca/#5-plot-ordination-diagrams","title":"5. Plot ordination diagrams","text":"<p>It is probably wise to plot scattergraphs of the first and second principal component scores of all the rows (observations) in the data matrix. If other principal components are to be used, then also plot these scores in combination with the first two. This way, observations that cluster together can be seen.</p>"},{"location":"math/pca/#6-biplots","title":"6. Biplots","text":"<p>Biplots are a nice way to summarize an ordination like PCA. They show the tendencies in the data, but they are not an inferential test in any way. They consist of two components:</p> <ul> <li>A scattergraph of ordination scores for observations (rows) in the matrix, like the ordination diagram above.</li> <li>A set of vectors representing the eigenvector loadings for each variable. These are arrows drawn from the origin to the designated coordinate.</li> </ul>"},{"location":"math/pca/#inferential-statistics-following-pca","title":"Inferential statistics following PCA","text":"<p>Principal components scores can be used like measured variables, i.e., they can be analyzed with statistics such as regression or ANOVA. There are 2 criterion to use before doing this. First, the axes must be normally distributed, which is easily checked with standard normality tests. If an axis is not normally distributed, different inferential statistical tests, such as non-parametric methods like Spearmans correlation or Kruskal Wallace ANOVA can be used (see here). Also, Monte-Carlo tests can be used, comparing the results of a correlation test, with the same test run on a large number of randomized versions of the data. The second criteria is that there should be no a priori connection between the dependent and independent (the axes) variables in your test. Therefore, when setting up the PCA, be sure to leave out the classifying/dependent variables that you will be testing against the ordination axes.</p>"},{"location":"math/pca/#some-resources","title":"Some resources","text":"<ul> <li>Short tutorial</li> <li>An in-depth tutorialusing R.</li> <li>An in-depth tutorial with MATLAB code.</li> <li>Another in-depth tutorialwith MATLAB code.</li> <li>Much of this page is summarized from Multivariate Statistics for the Environmental Sciences by Peter J. A. Shaw.</li> </ul>"},{"location":"math/timeseries/","title":"Time series analysis","text":"<p>Time series analysis is applied to data coming from a dynamical system, ie, where a variable changes over time. The goal is to understand the dynamical system (with a model) using the noisy data you have.</p> <p>Resources</p> <ul> <li>Shumway and Stouffer's book on time series analysis.</li> </ul>"},{"location":"math/timeseries/#moving-window-statistics","title":"Moving window statistics","text":"<p>These are calculated by selecting a window size for the statistic, then calculating the statistic for each point using that window. Three things to remember when coding these:</p> <ul> <li>First, the calculated statistic will likely be out of phase with the original timeseries by half the moving window size. Therefore, the statistic should be calculated with an odd number of points, and can then be shifted back into phase with the original data. </li> <li>Second, the calculated statistic will have a smaller window size at the edges of the timeseries, and is therefore less reliable in these areas. This may not be a significant problem if the dataset is large compared to the moving window.</li> <li>Third, missing data (NaN's) in a location will be propagated to the entire moving window. The simpler way to deal with this is to interpolate over the missing elements first. The other way is to exclude the missing elements, and thus use a variable number of points in calculating the statistic.`</li> </ul>"},{"location":"math/timeseries/#mean","title":"Mean","text":"<ul> <li>Use <code>filter</code> in MATLAB if there is no missing data.</li> <li>moving_average MATLAB</li> </ul>"},{"location":"math/timeseries/#median","title":"Median","text":"<ul> <li>Use <code>medfilt1</code> in MATLAB if there is not missing data.`</li> </ul>"},{"location":"math/timeseries/#variance","title":"Variance","text":""},{"location":"math/timeseries/#standard-deviation","title":"Standard Deviation","text":"<ul> <li>A MATLAB implementation</li> </ul>"},{"location":"math/timeseries/#filteringsmoothing","title":"Filtering/smoothing","text":""},{"location":"math/timeseries/#links","title":"Links:","text":"<ul> <li>Hampel filter implementation in Matlab(on Matlab Central).</li> <li>Robert Pearson's blogposts on data cleaning, particularly the Hampel filter.</li> <li>Another article by Ron Pearsonon different non-linear filtering methods.</li> </ul>"},{"location":"math/timeseries/#mean-of-multiple-timeseries-aggregate","title":"Mean of multiple timeseries (aggregate)","text":"<p>This is useful when similar timeseries need to be averaged, such as one-year timeseries data for multiple sites, or generating an average timeseries from multiple years.</p> <ul> <li>In MATLAB, <code>accumarray()</code> seems to work for this.</li> <li>In R, there are a few ways: <code>aggregate()</code>, and maybe some functions in the <code>doBy</code> package.</li> </ul>"},{"location":"math/timeseries/#autocorrelation","title":"Autocorrelation","text":"<p>Is the value at some time (t) in the series statistically dependent on the value at another time (s)? Put another way, is there a time lag effect, or a memory effect, or does today depend on yesterday?</p> <ul> <li>the Autocorrelation function ('acf') can be applied in R.</li> <li>ARMA and ARIMA models are also useful.</li> </ul>"},{"location":"math/timeseries/#spectral-analysis","title":"Spectral analysis","text":""},{"location":"math/timeseries/#decomposition","title":"Decomposition","text":"<p>Method for deconstructing a time series into notional components:</p> <ul> <li>A trend component</li> <li>Seasonal components - reflect seasonality</li> <li>Cyclical components - repeated, but non-periodic fluctuations</li> <li>Irregular components - random or stochastic influences - often a residual of the timeseries.</li> </ul>"},{"location":"math/timeseries/#prediction-and-forecasting","title":"Prediction and forecasting","text":""},{"location":"math/timeseries/#multivariate-models","title":"Multivariate models","text":"<ul> <li>TSA package for R</li> <li>A bunch of resources in thisand this SE question</li> </ul>"},{"location":"math/timeseries/#time-series-regression","title":"Time series regression","text":"<p>This is a way of construction a multiple regression model in which values of the dependent and/or independent variables at a previous timestep become independent variables in the model.</p> <ul> <li>http://www.sciencedirect.com/science/article/pii/S0038071712002970</li> </ul>"},{"location":"math/timeseries/#r-packages","title":"R packages","text":"<ul> <li>forecast</li> <li>TSA package</li> </ul>"},{"location":"math/toolboxes/","title":"Math and statistics toolboxes","text":"<p>A variety of computation tools for doing math and statistics in data analysis.</p> <p>See also: General programming info on some of the tools mentioned below</p>"},{"location":"math/toolboxes/#general-numerical-tools","title":"General numerical tools","text":"<ul> <li>NumPy- Implements n-dimensional arrays and related numerical methods. Most python math/stats toolboxes are built on this module.</li> </ul>"},{"location":"math/toolboxes/#statistics","title":"Statistics","text":"<ul> <li>SciPy</li> <li>gretl- Gnu regression, Econometrics and Time-series library.</li> <li>pandas- the Python Data Analysis Library.</li> <li>Has DataFrame objects - equivalent to the data.frame object in R</li> <li>Has tools for many statistics (covariance, correlation, regression)</li> <li>Timeseries analysis</li> <li>Missing data functions</li> <li>Some stats may be moved to StatsModels (below)</li> <li>StatsModels- Python module with classes and functions for many statistical models, tests, and data exploration.</li> </ul>"},{"location":"math/toolboxes/#symbolic-math","title":"Symbolic Math","text":"<ul> <li>Maxima- a computer algebra system (similar to Mathematica/Maple)</li> <li>SymPy- Symbolic math/algebra with Python.</li> </ul>"},{"location":"niwot_girdling/activitylog_1/","title":"Niwot Girdling Activity log","text":"<p>This log mainly covers ...</p> <p>See also: Data QC, Soil analysis methods</p> <p>3/7/2014 * Approaching a workable method for soil extracts. More testing next week.</p> <p>1/6-7/2014</p> <ul> <li>Did QC of final dogbone samples from Nicole - they seem ok (see 2013 output from irga_gasbench_process.m).</li> <li>2011 bulk soil samples (from flask sampling) were sent to Sirfer</li> </ul> <p>6/25/2013</p> <ul> <li>Subsamples (~3ml) of all extracts were sent to Kiowa lab today for TOC/TN analysis. Coordinating with Holly Hughes.</li> </ul> <p>4/25/2013</p> <ul> <li>Have begun grinding flask soils from July 1, 2011</li> <li>Working on method for %C and 13C of soil extractions (fumigated and non-fumigated). See here</li> </ul> <p>4/11/2013</p> <p>Now have data for soil and flask data.</p> <ul> <li>Need to plot soils with flask data.</li> <li>Also try plotting values of respiration/bulk d13C components (from Sean Schaeffer)?</li> </ul> <p>2/12/2013</p> <p>Have finished making a bad data removal routine for the soil CO2 data. See the data QC page.</p> <p>2/11/2013</p> <p>I am making local copies of my scripts, and giving them my own naming system. I will place them under version control with Mercurial.</p> <p>2/7/2013</p> <ul> <li>We now have all the soil CO2 data back from Brad and it all plots fine in MATLAB.</li> <li>I'm making a data QC pageto keep track of how we clean this dataset.</li> </ul> <p>1/31/2013</p> <ul> <li>Got a new script going - Niwot_2011_dogbone_workup_130130.m. This makes the 2 plots from the poster for either 2011 or 2012. Will rename.</li> <li>There is still bad data being plotted though, and I think this will need to be addressed in the Niwot_gasbench_ver3GM.m file with some input from one of Dave's workbooks.</li> <li>Note that in the compiled.GB.data.130130peak1.xls file, the 13C number for sirfer #12-11224 comes from peak 2, because peak 1 was too low. Can't seem to make that note without screwing up xlsread in matlab</li> </ul> <p>1/28/2013</p> <ul> <li>Ran another set of vials - FEF 9/21/2013.</li> <li>That is the last set.</li> </ul> <p>1/25/2013</p> <ul> <li>Ran another set of vials - NWT 9/24/2013.</li> <li>Replaced all septa</li> </ul> <p>1/24/2013</p> <ul> <li>Ran another set of vials last night and took them to Brad - FEF 9/4/2012.</li> <li>Met with Dave and for next week I need to- </li> <li>Finish the exetainers</li> <li>Run the data for 2012 through niwot dogbone workup script and reproduce last year's poster plots for 2012.</li> <li>Remember to use time since disturbance as the x axis rather than year of disturbance.</li> </ul> <p>1/20/2013</p> <ul> <li>Ran another set of vials - FEF 8/8/2012.</li> <li>Replaced all septa</li> </ul> <p>1/17/2013</p> <ul> <li>Ran another set of vials - NWT 8/27/2012.</li> <li>Met with Dave. We have 4 more sets of vials to run at SIRFER. When these are done the next priorities are</li> <li>Work up the data for 2011 and 2012 (dogbone_workup file)</li> <li>Run one sampling date of bulk organic samples (24 organic and 24 mineral soil samples) at sirfer for %C and 13C.</li> <li>Work out a method for running the extracts - %C, %N, and 13C</li> </ul> <p>1/10/2013</p> <ul> <li>Ran another set of vials - FEF 6/14/2012.</li> <li>Replaced all septa</li> </ul> <p>1/4/2013</p> <ul> <li>Ran another set of vials - NWT 7/2/2012 and took this and NWT 6/18 to Brad.</li> <li>Data is looking mostly fine at this point.</li> </ul> <p>12/20-28/2012</p> <ul> <li>Ran 2 sets of vials:</li> <li>FEF 6/26/2012 - all went well</li> <li>NWT 6/18/2012 - Had to replace an injection needle and there were problems with standards following this. 4 peaks were removed from the compiled CO2 file. </li> <li>Replaced all septa.</li> </ul> <p>12/17/2012</p> <ul> <li>TO DO:</li> <li>Create 2 new datafiles - One with mean peaks for 2012, one with only peak one</li> <li>Also, get rid of 2011 \"reanalyzed\" column. Its a legacy of 2011 and we can use that all the time now.</li> <li>Plot 2012 data (13Cvs area) over QC'd data from 2011.</li> <li>Found some duplicate vials for the 120724 FEF sampling date. There 2 vials each for 110-0, 110-161, and 110-488. Not sure why, but I suspect that one set is from a different plot and they were mixed up. Not sure which other plot appears to be missing yet.</li> <li>It is looking like all the wierdly enriched samples came from NWT on 7-17-2012 - this may not be a sirfer problem.</li> </ul> <p>12/10/2012</p> <p>Got some data from Brad (4 sets of vials). Currently working it over.</p> <ul> <li>Some vials are being re-run more than once this year, and that is creating some problems with the script I am using. Can separate these out using the run number.</li> <li>At the sampling on 7-24-2012 (FEF) plot 110 was sampled twice. Either this, or it is mismarked. Mismarked is a possibility because plot 104 is missing in this set. Not sure what to do here. Will look at the vials, but I think it will be hard to figure out what happened here. For now I have marked them as run 2 to keep them separate.</li> <li>Have made some changes to the compiled.CO2 and compiled.GB data files. Added a run # field to CO2 and standard deviation and peak amplitude to GB data files. These are reflected in the new version of the scripts that read those files.</li> </ul> <p>11/12/2012</p> <p>Met with Brad and Dave to discuss a way forward with the OA(surface) dogbone samples that have high 13C standard deviations.</p> <ul> <li>According to Brad, the main option we have is to run all the CO2 in an exetainer at once (by cryotrapping and sending it all to mass-spec). This would give us a larger peak, which should be more accurate. However, we can only do one measurement per sample (so no StDev).</li> <li>For now, we will have Brad process 4 more dates (2 from NWT and 2 from FEF) and then do some analysis of those to see how the OA samples look</li> <li>Also need to compare data from dogbones to TDL data from 2011 and 2012. Dave sent data in a .mat file.</li> <li>Plotting changes:</li> <li>Change order of histograms to reflect depth (top plot = shallow dogbones) in dogbone workup script.</li> <li>Do some plotting of ^13^C StdDev and mass spec peak area.</li> </ul> <p>10/29/2012</p> <p>Ran another injection series with the IRGA (FEF 120823).</p> <ul> <li>Changed all septa.</li> <li>This time I turned up delivery pressure and cleaned the syringe. RMSE looks about the same, and the bad RMSE is probably due to just one or two injections that are much higher than the rest. </li> <li>Maybe I will try a new syringe next time?</li> <li>Also ran the Niwot test samples that Nicole sent (6 samples, taken during radiocarbon sampling).</li> </ul> <p>Also, received one set of data from Brad. It looks like the surface samples produce small peaks, and therefore high standard deviations for 13C. I asked Brad if it would be better to run larger samples for these.</p> <p>10/15/2012</p> <p>Looked at data from the first two gasbench sample sets, and all IRGA dates (up to 10/9). Met with Dave about this.</p> <ul> <li>Gasbench data look ok, but there have been some x-axis shifts in the calibration. Peak area given by the mass spec has changed, and its relationship to 13C appears to have changed. Will meet with Brad about this.</li> <li>IRGA data has a bit higher RMSE than last year (~10). It appears that the A tank (~390ppm) is reading a bit inconsistently when injected into the IRGA. Two things to try:</li> <li>Clean the syringe plunger</li> <li>Turn up the delivery pressure on the cal tank regulators. </li> </ul> <p>10/9/2012</p> <p>Ran a bunch of FEF samples on the IRGA today. Some of the A cal injections were below the 100ppm threshold for the processing program to count them as peaks. Could this be because the pressure is low in the A tank? Or are the samples becoming a bit more moist and it is water vapor causing this problem. The IRGA seems to be wandering a bit more overall, so perhaps I need to check for leaks or something.</p>"},{"location":"niwot_girdling/data_qc/","title":"Niwot/Fraser girdling study data QC","text":"<p>See also: Activity log</p>"},{"location":"niwot_girdling/data_qc/#soil-co2-data","title":"Soil CO2 data","text":"<p>We use two methods of finding and removing bad data in our soil CO~2~ dataset. The first is event-based, where datapoints generated on bad sampling days, or bad instrument runs are flagged and removed. Usually this removes a large block of samples from a particular date. These are explained in the first three subheadings below. The second method is to remove datapoints that fall far outside of the distribution of a statistic describing our data (outliers). In this case, we use a regression line and remove data that falls too far away from this predicted line. Finding and removing problematic data and outliers, is done in the <code>irga_gasbench_process.m</code> script.</p>"},{"location":"niwot_girdling/data_qc/#problematic-sampling-dates","title":"Problematic sampling dates","text":"<p>Sometimes sampling dates have lots of outliers. There may be a known reason for this, or not. These are removed by flagging the IRGA date in which they were run. \\^ Bad sample dates \\^ Description \\^ Reason \\^ Action \\^ IRGA date \\^ | 120717 (NWT) | Enriched d13C | Rain storm? | Removed | 121008 | | | | | | |</p>"},{"location":"niwot_girdling/data_qc/#problematic-irga-data","title":"Problematic IRGA data","text":"<p>IRGA run quality is assessed using the RMS error of the calibration tank measurements. Lower values are better. \\^ Bad IRGA dates \\^ Description \\^ Reason \\^ Action \\^ | 110928 | ? | Problem with Licor integration | Removed | | 111004 | ? | Problem with Licor integration | Removed | | 111102 | High RMS error | Bad syringe | none | | 121008 | Enriched d13C | Bad sampling date (see above) | Removed | | | | | |</p>"},{"location":"niwot_girdling/data_qc/#problematic-gasbench-data","title":"Problematic gasbench data","text":"<p>Gasbench/IRMS run quality is assessed using its distance from a reference line. In 2011, this was the regression line for gasbench date 111209 (see Niwot_gasbench_verX.m comments around line 262). We retained this line for 2012, but it may make sense to make a new one. \\^ Bad Gasbench dates \\^ Description \\^ Reason \\^ Action \\^ | 111025 | Off the ref. line | | Removed | | 111130 | Off the ref. line | | Removed | | 120831 | Follows last years ref line | New sample loop installed after this date | Still there (for now) | | 121119 | NWT samples have low peak areas | These failed and were re-run by Brad | Removed (NWT only) |</p>"},{"location":"niwot_girdling/data_qc/#outlier-detectionremoval","title":"Outlier detection/removal","text":"<p>In 2011, outliers were removed by distance from a regression line of IRMS area on IRGA CO~2~. It looks like this regression line was created before removing all the bad sampling/instrument dates (above). This is documented in the old Niwot_gasbench_verX.m scripts, and in Dave Bowling's lab notebook #5, pp 58-59. For 2012 this was refined by adding two other methods of bad-data removal.</p> <p>Currently, two regression relationships, IRMS area vs. IRGA CO~2~ and \u03b413C vs. 1/IRGA CO~2~, are generated for each site in 2011 and 2012. A threshold distance is set for each line and datapoints beyond this threshold are removed. This threshold is the same in both years and for both sites. This technique should remove anomalies for CO~2~ (leaky vials, bad injections, IRMS area problems) and \u03b413C (diffusional wierdness and IRMS issues) data.</p> <p>![media/niwot_girdling/nwt_forestd13co2.png?180|Niwot aboveground forest air d13C vs CO2 concentration] There are also deep (&gt;10cm depth) measurements that have near-atmospheric \u03b413C values (&gt; -12permil). This is most likely a mistake because any atmospheric air (-8 to -12permil - see fig at right) that is advected into the soil mixes with much more depleted CO~2~. These values are also removed (value &gt; -12 permil AND measurement deeper than OA horizon). This was formerly done using depth/concentration thresholds in the soilCO2_tdist.m file.</p>"},{"location":"niwot_girdling/data_qc/#final-qcd-data","title":"Final QC'd data","text":"<p>Number of samples removed, 2011</p> <p>\\^ Problem \\^ IRGA dates \\^ GB dates \\^ IRGA/IRMS CO2 \\^ d13C/invCO2 \\^ Enr. deep d13CO2 \\^ Total 2011 \\^ | NWT | 70 | 10 | 9 | 1 | 5 | 95 | | FEF | 49 | 56 | 15 | 4 | 0 | 124 | 219 samples removed total</p> <p>Number of samples removed, 2012</p> <p>\\^ Problem \\^ IRGA dates \\^ GB dates \\^ IRGA/IRMS CO2 \\^ d13C/invCO2 \\^ Enr. deep d13CO2 \\^ Total 2012 \\^ | NWT | 70 | 20 | 4 | 4 | 2 | 100 | | FEF | 0 | 0 | 0 | 4 | 7 | 11 | 111 samples removed total</p> <p>![media/niwot_girdling/girdling_baddata2011.png?400|Bad data removed 2011]![media/niwot_girdling/girdling_baddata2012.png?400 |Bad data removed 2012 - note 2 sample lines from gasbench]</p>"},{"location":"niwot_girdling/labeldecomp_overview/","title":"Labeldecomp overview","text":"<p>THIS EXPERIMENT IS CURRENTLY ON HOLD</p>"},{"location":"niwot_girdling/labeldecomp_overview/#decomposition-of-labeled-litter","title":"Decomposition of labeled litter","text":"<p>The objective of this experiment is to measure the response of below-snow organisms and soil organic matter decomposition to differences in winter snowpacks. Snowpacks insulate soils from winter temperature extremes and supply liquid water during snowmelt and these effects facilitate the activity of microbial communities below the snow. These communities reach high biomass levels by the onset of snowmelt and play an active role in organic matter decomposition and biogeochemical cycling. This experiment measures rates of ^13^CO~2~ efflux and decomposition of labeled needle litter under low, and normal (control) snowpack treatments. We expect that winter snowpack size is a key driver of interannual variability in soil temperature and moisture, and that this variability has consequences for biological C (and N) cycling in soils. See Schmidt and Lipson, 2004((Schmidt SK, Lipson DA (2004) Microbial growth under the snow: implications for nutrient and alleochemical availability in temperate soils. Plant Soil 259:1\u20137. )), Schmidt et al, 2008((Schmidt SK, Wilson KL, Gebauer MM, Meyer AF, King AJ (2008a) Phylogeny and ecophysiology of opportunistic \u2018\u2018snow molds\u2019\u2019 from a sub-alpine forest ecosystem. Microb Ecol . )), Schmidt et al, 2007((Schmidt SK, Costello EK, Nemergut DR, Cleveland CC, Reed SC, Weintraub MN et al (2007) Biogeochemical consequences of rapid microbial turnover and seasonal succession in soil. Ecology 88:1379\u20131385. )), Monson et al, 2006((Monson, R. K., D. A. Lipson, S. P. Burns, A. A. Turnipseed, A. C. Delany, M. W. Williams, and S. K. Schmidt (2006), Winter forest soil respiration controlled by climate and microbial community composition, Nature, 439, 711 \u2013 714))"},{"location":"niwot_girdling/labeldecomp_overview/#hypotheses","title":"Hypotheses","text":"<ul> <li>Winter decomposition and associated soil ^13^CO~2~ efflux are reduced in low snowpack years (simulated by snow removal), and this effect is correlated with lower soil temperatures and a shorter snow-covered period.</li> <li>Control snowpack plots will have longer periods of below-snow decomposition, and shorter periods of growing-season soil drought, leading to greater cumulative decomposition and soil ^13^CO~2~ during the experiment.</li> <li>Labeled carbon compounds will be more rapidly added to stable fractions of the soil organic matter in control snowpack treatments.</li> </ul>"},{"location":"niwot_girdling/labeldecomp_overview/#experimental-design","title":"Experimental design","text":"<p>To test these hypotheses we will make continuous measurements of respiration from ^13^C labeled organic matter added to soils, and measure the enrichment of SOM constituents at intervals after this addition. These measurements will occur under snow removal treatments and an undisturbed snow control. There will also be an undisturbed control with no added needles. Efflux of ^13^CO~2~ from labeled needle litter additions indicates active periods of label decomposition. ^13^C enrichment of soil organic matter fractions can be used to measure the transformation of fresh litter into other soil organic matter fractions.</p> <p>If microcosms are used in the experiment, decomposition may be measured in a more quantitative way.</p>"},{"location":"niwot_girdling/labeldecomp_overview/#labeled-litter-addition","title":"Labeled litter addition","text":"<p>^13^C and ^15^N labeled needles from //Pinus ponderosa// will be obtained from Jeff Bird. These have a \u03b4^13^C of 2487 per mil and a ^15^N enrichment of 5.5 atom %. See the Bird and Torn, 2006 paper for details on this material((Bird, J.A., Torn, M.S. Fine roots vs. needles: A comparison of 13C and 15N dynamics in a ponderosa pine forest soil (2006) Biogeochemistry, 79 (3), pp. 361-382. doi: 10.1007/s10533-005-5632-y)). 10 grams of labeled litter will be placed on the forest floor over a 20cm^2^ area and secured with plastic netting. Gas sampling inlets will be placed directly over this netting in the center of the label.</p> <p>Labeled root litter is also available and can be placed under a separate set of inlets. 10g of root litter would be placed at 5-10cm depth at the interface of the organic and mineral horizons. Inlets would either be placed directly above this root litter and then be covered with organic horizon soil, or at the surface as in the needle litter arrangement above. Deployment of litter would require some reduction in the replication of inlets.</p> <p>Label and gas sampling inlets, plus any other measurement activities will be replicated (n = 3-5) under these treatments:</p> <ul> <li>Snow removal: Snow shoveled to a constant 20cm depth for the entire winter.</li> <li>Control: Undisturbed snowpack</li> <li>No-Label control: Undisturbed snowpack, no label addition</li> </ul> <p>The experimental design without and with roots is summarized in these tables:</p> <p>Design with needles only</p> <p>\\^ \\^ Snowpack treatment \\^\\^\\^ \\^ No. of inlets | Removal | Control | Control (no label) | \\^ Needle litter | 5 | 5 | 5 | //15 sample inlets total//</p> <p>Design with needles and roots</p> <p>\\^ \\^ Snowpack treatment \\^\\^\\^ \\^ No. of inlets | Removal | Control | Control (no label) | \\^ Needle litter | 3 | 3 | 3 | \\^ Root litter | 3 | 3 | 3 | //18 sample inlets total//</p>"},{"location":"niwot_girdling/labeldecomp_overview/#alternative-microcosm-litter-addition-method","title":"Alternative microcosm litter addition method","text":"<p>Microcosms similar to those used in Bird and Torn, 2006 will be installed in each treatment plot (n=9) with inlets placed at the top of one microcosm per treatment. The advantage of this design is that smaller amounts of litter might be used (~1.25 per microcosm in Bird 06), so the potential for replication is higher, and the addition of 13C to soil organic matter pools can be measured in a more quantitative way. These microcosms will be installed for equilibration in July or August of 2010. Labeled litter will be mixed in to the soil at the top of the microcosm in mid-late October. Three microcosms will be collected at three discrete time intervals for measurement of 13C enrichment of SOM pools. These time intervals might be following snowmelt (May 2011), the next fall (Oct, 2011) and the second snowmelt (May 2012) after installation.</p> <p>**Microcosm design including roots **</p> <p>\\^ \\^ Snowpack treatment \\^\\^\\^ \\^ No. of microcosms(# inlets) | Removal | Control | Control (no label) | \\^ Needle litter | 9(3) | 9(3) | 9(3) | \\^ Root litter | 9(3) | 9(3) | 9(3) | //18 sample inlets total//</p> <p>Three microcosms to be collected during the first spring (~ May 2011), first fall (Oct 2011), and second spring (May 2012) after installation.</p> <p>Efflux of ^13^CO~2~ from inlets will be measured for two full         winters (2010/11, 2011/12) if possible.</p>"},{"location":"niwot_girdling/labeldecomp_overview/#measurement-of-labeled-carbon-dioxide-efflux","title":"Measurement of labeled carbon dioxide efflux","text":"<p>Inlets are placed directly above the labeled litter and CO~2~ concentration and \u03b4^13^CO~2~ will be measured every 3 hours using a Campbell Tunable Diode Laser that has been operating at Niwot Ridge for several years. There are 20 inlets to this device currently available. CO~2~ flux rates can be measured with measurements of the concentration gradient (below snow sample vs an above snow sample) and Fick's law. These measurements will require frequent sampling of snowpack density, and flux measurements may only be feasable for the control snow treatment.</p> <p>Replicated measurements of soil moisture and temperature will be continuously taken in each treatment. Soil temperature will be measured using either thermistors or iButtons. Soil moisture will be measured using sensors such as Decagon EC-5s or Campbell CS-616s. In addition, there is a soil moisture and temperature profile operating 10 m away.</p>"},{"location":"niwot_girdling/labeldecomp_overview/#soil-organic-matter-sampling","title":"Soil organic matter sampling","text":"<p>At the end of this experiment, enrichment of soil organic matter fractions will be assessed by coring beneath the label and measuring the carbon isotope ratio (\u03b4^13^C) of selected SOM fractions. Coring will occur immediately after snowmelt in 2012. SOM fractions to be analyzed include dissolved organic carbon from both O and A fractions, microbial carbon (by fumigation extraction) from O and A horizons, bulk organic layer samples, and light (particulate) and heavy (mineral associated) organic matter from the A horizon. These samples will be measured for carbon isotope ratios using an EA-IRMS system at the SIRFER lab (University of Utah).</p> <p>If microcosms are used, similar measurements and methods will be employed with each set of microcosms that are returned to the lab.</p> <p>See the measurements page for more detail.</p>"},{"location":"niwot_girdling/labeldecomp_overview/#issues-to-address","title":"Issues to address","text":"<ul> <li>What are the best ways to measure the addition of the labels (13C and 15N) to the soil organic matter, including the microbial community, over time?</li> <li>The activity of many organisms is being observed in this experiment. Can we separate them into phylogenetic groups and identify what communities are actively decomposing/respiring during different times of the year? What measurements would be useful to do this?</li> <li>If we are identifying actual organisms, enzymes, etc, what are they telling us about C cycling in terms of what is being decomposed, when, and how quickly?</li> <li>Using microcosms, would it be possible to have a true mass balance of the label? Will the mass of 13C respired, and added to SOM pools at the harvest of the microcosms all add up to what was originally added as labeled litter?</li> <li>There are problems with calculating flux rates from soil beneath a disturbed snowpack.</li> <li>What parts of the SOM are we going to measure for changes in enrichment? What do these SOM fractions represent?</li> <li>What is the overall significance? Climate variability (in the form of snow depth) leads to biologically mediated changes in C processing and storage? Snow removal may decrease respiration, but does this lead to increases in stored soil C, or could a reduced rate of processing litter into longer-lived C compounds lead to less soil C storage?</li> </ul>"},{"location":"niwot_girdling/soilanalysis/","title":"Soil analysis","text":""},{"location":"niwot_girdling/soilanalysis/#soils-from-flask-resp-measurements","title":"Soils from flask resp measurements","text":"<p>These were run for %C and \u03b4^13^C at SIRFER.</p> <ul> <li>Soil prepmethods.</li> </ul>"},{"location":"niwot_girdling/soilanalysis/#soil-extracts","title":"Soil extracts","text":"<p>Soil samples were collected at all forest plots. Soils were homogenized, a subsample was fumigated with chloroform, and fumigated and unfumigated samples were then extracted with a salt solution. The extracts must be analyzed for total organic carbon (TOC), total nitrogen (TN), and \u03b4^13^C. There is currently no method for \u03b4^13^C analysis of soil extracts at SIRFER.</p>"},{"location":"niwot_girdling/soilanalysis/#preliminary-info","title":"Preliminary info","text":"<ul> <li>5g soil extracted with 25ml of 0.5M K~2~SO~4~</li> <li>Nicole Trahan's data from similar Niwot Ridge soil extracts show between 200 and 2500 \u00b5g C/g of dry soil.</li> <li>If ours are similar, we should expect between 40 and 500 \u00b5gC/ml of extract.</li> </ul>"},{"location":"niwot_girdling/soilanalysis/#toctn-analysis","title":"TOC/TN Analysis","text":"<p>We are sending subsamples of our extracts (3-4ml) to the Kiowa lab in Colorado. These will be run on a Shimadzu TOC instrument using a combustion with catalyst method.</p>"},{"location":"niwot_girdling/soilanalysis/#13c-analysis","title":"\u03b413C Analysis","text":"<p>This analysis will be done at the SIRFER lab. There is currently no protocol for \u03b4^13^C measurements of soil extracts at SIRFER. Originally, we proposed to run the freeze dried extracts on an EA-IRMS unit there. Unfortunately, these extracts contain too much salt for the combustion system. To run 0.4mg of C through the system, a sample containg around 70mg of salt (or more) would need to to be loaded into the instrument. The C in the extracts must be run without this salt (some possible methods for this are in the references below). We will use a form of liquid oxidation to produce CO~2~ from the C in the extracts and then run the CO~2~ to the mass spec from the Gasbench. Here is a proposed outline:</p> <ul> <li>Prepare sugar standard extracts (for testing and as reference material during analysis)</li> <li>See sugar standards section</li> <li>Place a small amount of extract in an Exetainer.</li> <li>Scrub out all CO~2~ in the headspace of the vial (vacuum?).</li> <li>Add an oxidant to convert C in extracts to CO~2~. There are multiple possible oxidants.</li> <li>Load exetainers into the Gasbench and run through the MS from there.</li> <li>It may be necessary to purify the CO~2~ before it enters the MS using some type of cryotrapping.</li> </ul> <p>See the full procedure</p>"},{"location":"niwot_girdling/soilanalysis/#references","title":"References","text":"<ul> <li>Vance et al, An extraction method for measuring soil microbial biomass C. SBB, 1987</li> <li>Murrage et al,Modification of the original chloroform fumigation extraction technique to allow measurement of \u03b413C of soil microbial biomass carbon. SBB, 2007</li> <li>Potthoff et al, The determination of d13C in soil microbial biomass using fumigation-extraction</li> <li>Werth and Kuzyakov, 13C fractionation at the root-microorganisms-soil interface: A review and outlook for partitioning studies SBB, 2010</li> </ul>"},{"location":"nmeg/fluxall_qc_pseudocode/","title":"Pseudocode for new fluxall qc processing","text":"<p>These are new scripts that will make up the new fluxall qc processing pipeline. This is run by site/year. Should enable showing corrections and gapfilling in diagnostic plots.</p>"},{"location":"nmeg/fluxall_qc_pseudocode/#general-workflow","title":"General workflow","text":"<ol> <li> <p>load fluxall file for site/year and create fluxall_raw</p> </li> <li> <p>correct_and_calibrate</p> <ol> <li>input fluxall table (fluxall_raw)</li> <li>fix_datalogger_timestamps</li> <li>Apply radiation corrections/calibrations</li> <li>Precip fixes</li> <li>Correct flux data.<ul> <li>Burba</li> </ul> </li> <li>Output corrected fluxall table (fluxall_qc)</li> </ol> </li> <li> <p>remove_bad_data</p> <ol> <li>input  corrected fluxall table (fluxall_qc)</li> <li>fix_specific_problem_periods</li> <li>flag flux data<ul> <li>ustar, wind, etc</li> </ul> </li> <li>exceptions</li> <li>Output rbd table (fluxall_qc_rbd)</li> </ol> </li> <li> <p>gapfill_fluxall_qc</p> <ol> <li>input corrected, rbd table (fluxall_qc_rbd)</li> <li>fill fluxes from 30min</li> <li>fill flux from local (regression fit from another NMEG site)</li> <li>fill met gaps from nearby site</li> <li>Output gapfilled fluxall table (fluxall_qc_rbd_gf)</li> </ol> </li> <li> <p>write output files</p> <ol> <li>site_year_fluxall_qc.txt output from fluxall_qc_rbd table</li> <li>site_year_fluxall_qc_gf.txt output from fluxall_qc_rbd_gf table.</li> <li>Possibly create and output a standardized 'for_gapfilling' file</li> </ol> </li> </ol>"},{"location":"nmeg/fluxall_qc_pseudocode/#2-types-of-plots","title":"2 types of plots","text":"<p>These can be toggled with a flag in master scripts</p> <ul> <li>QC/RBD plots - show all changes to data as they happen - data removed, corrections, etc.</li> <li>Summary plots - summaraize the data in a nice way, may indicate corrections to make of bad data to remove.</li> </ul>"},{"location":"nmeg/fluxproc_log/","title":"NMEG flux processing log","text":""},{"location":"nmeg/fluxproc_log/#ameriflux-data-submissions","title":"AmeriFlux data submissions","text":"<p>2007-2014 are submitted and in review, and we think these data may make it into the FLUXNET2015 data release.</p>"},{"location":"nmeg/fluxproc_log/#the-bad-irga-0922","title":"The bad IRGA - #0922","text":"<p>Bai Yang originally brought to my attention the fact that there were higher H2O concentrations at PJ control in 2014 than in earlier years ( by about 5-10%. Upon investigation I noticed that the timing of these higher than normal values roughly coincided with the period that a specific IRGA (ser# 0922) was installed at the site (5/2/2014 to 10/31/2014). This IRGA was subsequently installed at</p> <ul> <li>PJ_girdle from 2014/11/7 to 2015/1/23</li> <li>PPine from 2015/2/17 to 2015/8/18, checked agc at this time and it was ok</li> </ul> <p>Later investigation by Steven indicated that this IRGA was not holding its calibration well, so we sent the IRGA back to LiCor. I don't think the data need to be corrected, however. There appears to be a similar pattern in atmospheric H2O concentration observed at PJ_girdle during the 2013-2014 growing seasons (low in 2013, higher by a similar amount in 2014). So, though the data may be slightly more noise in 2014 due to the calibration problem, there doesn't appear to be a level shift that needs to be corrected for this IRGA.</p>"},{"location":"nmeg/fluxproc_log/#2007-and-2008-qc","title":"2007 and 2008 QC","text":"<p>GLand and SLand done </p>"},{"location":"nmeg/fluxproc_log/#drought-and-info-flow-papers","title":"Drought and Info-flow papers","text":""},{"location":"nmeg/fluxproc_log/#drought","title":"Drought","text":"<ul> <li>Sites are arranged along an elevation/water availability gradient. It follows that drought severity and ecosystem response is higher at low elevation.<ul> <li>Rank years according to precip defecit, vpd anomaly, and other metrics along the gradient.</li> <li>Which are good years and which are bad relative to our measurements and according to long term mean?</li> <li>Need to add some metric of soil water availability</li> </ul> </li> <li>Quantify ecosystem response - is there a gradient of drought sensitivity among our sites?<ul> <li>Calculate NEE, GPP, and RECO anomaly in each year. Is this related to precip defecit, vpd anom, soil water availability?</li> <li>Dan's sensitivity curves (flux vs temp/vpd/etc - binned for all years and by season/year).</li> </ul> </li> <li>What does THE GRADIENT tell us about southwestern drought and vulnerability to drought?</li> </ul>"},{"location":"nmeg/fluxproc_log/#info-flow","title":"Info-flow","text":"<ul> <li>Hypothesis - Info flow between water (swc, vpd, etc) is higher at low elevation sites.<ul> <li>below a certain availability threshold water has little info flow to fluxes</li> <li>Above the water availability threshold - Tair, radiation, nutrient avail, etc contains more information.</li> </ul> </li> </ul>"},{"location":"nmeg/fluxproc_log/#pj_girdle-2009","title":"PJ_girdle 2009","text":"<ul> <li> <p>There is a correction to 2009 PJ_girdle fluxes that was developed in 2010 and has been used ever since. However, it was commented out in the code I ran to reprocess fluxes recently and this is the reason that my fluxes looked so different for this year. This correction gets applied in the UNM_flux_031010.m script. I have added the appropriate conditional statments to make it run whenever this script is run for PJ_girdle prior to Sept 1 2009.</p> </li> <li> <p>That said - I think the fluxes look a little bogus with this correction. There appears to be an abnormally high amount of CO2 uptake early in 2009. Just a feeling, but maybe the data prior to about April 20 does not need any correction.</p> </li> </ul>"},{"location":"nmeg/fluxproc_log/#partitioning-type-analysis","title":"Partitioning type analysis","text":"<p>See the IPy notebook. Several sites have strange issues with partitioned fluxes</p>"},{"location":"nmeg/fluxproc_log/#what-happens-when-datalogger-clocks-are-reset","title":"What happens when datalogger clocks are reset?","text":"<p>Periodically we have to reset our datalogger clocks. We are curious about what happens to the data logged when this occurs.</p>"},{"location":"nmeg/fluxproc_log/#30-min-data","title":"30 min data","text":"<ul> <li>Data is collected as normal, and the observations stored in the internal table in the datalogger either are missing timestamps or have duplicate timestamps.</li> <li>Consequently, during the half hour period during which the clock is changed a larger or smaller number of observations is used to derive each 30 min value in the output table.</li> <li>The sign of the change in observations  depends on whether the datalogger time was moved forward (fewer observations) or back (more observations).</li> <li>This can be seen in the <code>n_Tot</code> column in the TOA5 file. A normal half hour period with no clock change has 18,000 observations.</li> </ul>"},{"location":"nmeg/fluxproc_log/#10-hz-data","title":"10 hz data","text":"<ul> <li>Again, data is collected as normal in the datalogger table, which may mean not all timestamps are present in the observations, or some timestamps are duplicated.</li> <li>All observations are present in the 10hz output table (TOB1 file).</li> <li>When 10hz data are processed into 30 min fluxes, the <code>iok</code> variable indicates how many 10hz observations were used to calculate the flux. This value fluctuates up or down during the 30 min clock reset period.</li> </ul>"},{"location":"nmeg/fluxproc_log/#2015-04-20","title":"2015-04-20","text":"<ol> <li>Moved all 8100 data from Sandia and consolidated files from PJ and PJG folders into one - <code>PJX_all8100data</code>. A few of the testing folders on the ftp might still have different files (did not overwrite).</li> </ol>"},{"location":"nmeg/fluxproc_log/#2015-04-14","title":"2015-04-14","text":"<p>In the last couple days I have:</p> <ol> <li>Created new issue trackers for all sites and shared them with the lab.</li> <li>Created an always-up-to-date tutorial for people using processed data.</li> <li> <p>Figured out how to create new daily files from the ameriflux files I have made</p> <ul> <li> <p>This involves creating an \"aggregator\" object for the site, then writing a file from it:</p> <pre><code>agg = UNM_Ameriflux_daily_aggregator(UNM_sites.JSav);\nagg.write_daily_file();\n</code></pre> </li> <li> <p>The aggregator output is dependent on the files it finds  in $FLUXROOT$/Ameriflux_files</p> </li> </ul> </li> </ol>"},{"location":"nmeg/fluxproc_log/#2015-04-10","title":"2015-04-10","text":"<p>This has been put in the issue tracker spreadsheets</p> <p>Looking at results from MPI gapfiller/partitioner.</p> <ul> <li>JSav 2014 is terrible - fluxes are bad and there are SWin spikes.</li> <li>GLand 2009 looks bad - Lots of missing fluxes and LE filling looks bad  - maybe there is a bad met stream?</li> <li>MCon 2009, 2010, and 2012 SWin looks bad - see note below about met gapfilling.</li> <li>PPine SWin also looks bad early and late in year - appears related to met gapfilling.  I think most of this gapfilling is unnecessary... related to mysterious end of year radiation removal.</li> <li>PPine flux fingerprints, especially LE,  are a little wide after gapfilling also - maybe related to radiation?</li> <li>Of course 2013 gapfilling at MCon and PPine sucks...</li> </ul>"},{"location":"nmeg/fluxproc_log/#2015-04-09","title":"2015-04-09","text":"<p>This has been put in the issue tracker spreadsheets</p> <p>Gapfilled Met data for GLand, SLand, New_GLand, PJ, PJ_girdle, and JSav 2009:2014</p> <ul> <li>Need to look at PPine gapfilling - precip units are wierd, we must have some RH, and SWin filling is a little lower than ours.<ul> <li>There also appears to be a timeshift/tilt issue in 2009</li> </ul> </li> <li>PJ sites were filled with bad PPFD values from JSav in 2014.</li> <li>MCon 2014 precip gauge data reads 0 for the 1st half of year, so it is not gapfilled. Need to change this.</li> <li>First 100 days at SLand in 2014 probably need to be set to NaN and filled in (no precip).</li> <li>Radiation filling at MCon is bad, it looks ok in a timeseries but the sensor is tilted/shifted - see early 2009 data. Also, not sure why it is filling SWin at the end of the year... raw data looks ok there.</li> </ul>"},{"location":"nmeg/fluxproc_log/#ameriflux-qc","title":"Ameriflux QC","text":"<p>This has been put in the issue tracker spreadsheets</p>"},{"location":"nmeg/fluxproc_log/#pj","title":"PJ","text":"<ul> <li>2009: Big precip events days 250-255.</li> <li>2011: Fc noisy in Dec. due to IRGA issues</li> <li>2012: Fc noisy in Jan. due to IRGA issues</li> <li>2013: SWin gap in middle of summer.</li> <li>2014: PAR drops significantly around day 168. Gapfilling from JSav this year is bad.</li> </ul>"},{"location":"nmeg/fluxproc_log/#pj_girdle","title":"PJ_girdle","text":"<ul> <li>2009: Precip data missing at start of year, but it appears to be NaN (not zero) so it should be gapfilled. RH missing early in year.</li> <li>2010: Something strange with PPFD in mid summer (2 sensors maybe?).</li> <li>2011: Good</li> <li>2012: Large gap days 300-339</li> <li>2013: SWin a little high early and late in year</li> <li>2014: Big PPFD drops between days 50 and 75. IRGA change and big [CO2] drops between days 112-130 - it looks to me like this  negatively impacts Fc (false uptake?). Code removes this now. SWin a little high early and late in year. Gapfilling from JSav this year is bad.</li> </ul>"},{"location":"nmeg/fluxproc_log/#mcon","title":"MCon","text":"<ul> <li>2009: No SWin until May, then it seems a little high. No precip data.</li> <li>2010: No PPFD until August, and SWin looks high. Precip starts around day 280. RH looks bad in first half of year. There is something funny with Fc (or maybe just less noisy than usual) and perhaps datalogger timing in Nov-Dec (Day 300 on look strange).</li> <li>2011: SWin a little high. Maybe a slight timing issue in Jan (related to end of year 2010?). Noisy Fc data at end of year - check for IRGA change.</li> <li>2012: Datalogger times were a total mess this year. Looks somewhat better, but the CNR1 was tilted. SWin a little high. Huge precip event around day 75. Fc is very noisy at the beginning and end of the year and there are some high IRGA CO2 values here (cold weather problems?).</li> <li>2013: CNR1 tilted and small datalogger time reset in Jan-Feb. Sensor was probably leveled in late March? SWin still high. Fire wiped out most of year. No precip after fire.</li> <li>2014: Clock shift. Precip missing until day 184. SWin is too high. No RH.</li> </ul>"},{"location":"nmeg/fluxproc_log/#ppine","title":"PPine","text":"<p>Pay attention to sketchy normalization steps with PPine respiration - setting CO2 limits results in unexpected behavior.</p> <ul> <li>2009: SWin is strange up to mid-May. PPFD a little low until it drops to 0 on day 318. No precip data. Period of bad Fc (removed) days 151-183, but bad LE and HS are not removed at this time.</li> <li>2010: PPFD and SWin drop to 0 on day 318 again. Precip data starts day 236.</li> <li>2011: PPFD and SWin drop to 0 on day 318 again (WTF???) must be some kind of cutoff coded somewhere? No RH data.</li> <li>2012: No RH data. Same PPFD/SWin day 318 problem.</li> <li>2013: Clock reset for small period in late Nov. No good PPFD or SWin. No RH data.</li> <li>2014: Clock reset in middle of summer. No RH.</li> </ul>"},{"location":"nmeg/fluxproc_log/#new_gland","title":"New_GLand","text":"<ul> <li>2010: Clock reset June 27. No PPFD until day 85. Scattered low [CO2] at various periods - seems to be coincident with rain - removing some higher resp pulses.</li> <li>2011: Power outages in July? SWin is too low. Precip seems a little low - winter/spring storm amounts are lower than at GLand and New_Gland is missing an event on day 100 that is present at GLand .</li> <li>2012: PPFD is too high in Nov &amp; Dec. SWin is too low.</li> <li>2013: SWin too low. Precip has one unbelievably large event (25mm). May have missed events at days ~135 and 160 that happened at GLand. Funny PPFD calibration starting day 250.</li> <li>2014: SWin still low to approx. day 18. Missing precip to Apr 2 - make sure it is filled - CHECKED - it is.</li> </ul>"},{"location":"nmeg/fluxproc_log/#sland","title":"SLand","text":"<ul> <li>2009: SWin is too high. PAR is messed up. Fc looks pretty good and there arent many coarse filters applied.</li> <li>2010: Generally pretty good.</li> <li>2011: PPFD disappears around day 160. Precip may have a problem - there are fewer precip events than GLand and old code used to remove data.</li> <li>2012: Pretty good.</li> <li>2013: Possibly some funny SWin at the end of the year - possibly related to a header change (non-Tcor to Tcor?)</li> <li>2014: Jan Rg data looks too high (see not above). PAR sensor change in May and large PPFD spike days 257-267. Missing early year Precip followed by huge event on day 92.</li> </ul>"},{"location":"nmeg/fluxproc_log/#gland","title":"GLand","text":"<p>Shifted everything 1.5 hours</p> <ul> <li>2009: PPFD data needs major attention. Gaps in flux data days 26-52 and 295-332. We could consider filling these with 30 min data (if available) or other site data (suspect this was done in the past).</li> <li>2010: PPFD is crazy high, but then gets normalized. Big gaps in flux data days 100-119 and 295-327 -- Fill? IRGA calibration looks terrible this year. Exceptions were made for low CO2 calibration problems around days  85-99 and 152-168. Not sure if this was the right decision.</li> <li>2011: PPFD disappears around day 160? Gaps in 30min and flux data days 165-207 could maybe fill a little flux data in with 30min data.</li> <li>2012: Gap days 197-215. Crazy PPFD normalization again. Clock shift in Dec.</li> <li>2013: PPFD too big.</li> <li>2014: PPFD shifts down at around day 150 - new sensor? Bad IRGA cal period in middle of monsoon (Days 323-328) - probably should remove.</li> </ul>"},{"location":"nmeg/fluxproc_log/#jsav","title":"JSav","text":"<ul> <li>2009: There was a clock reset in the code, but looks like it was wrong. Days 1:65 and 140:165 are missing flux data. Might be filled with 30 min data. PPFD has some funky negative values early in year.</li> <li>2010: Not too many problems. Half-hour shift. Missing data between days 210 &amp; 235.</li> <li>2011: Days 45-52 have crazy [CO2] values.</li> <li>2012: Looks pretty good except for gap in flux data at end of year (fill w/ 30min?). Huge precip value around day 190.</li> <li>2013: Missing flux days 1-16. Funky missing flux data days 80-105. Possibly another clock shift around day 178. Huge precip event around day 214</li> <li>2014: Have to remove giant PPFD spike early in year. Funny high nighttime Rg early in year. RH is too high (out of cal?).</li> </ul>"},{"location":"nmeg/fluxproc_log/#2015-03-31","title":"2015-03-31","text":""},{"location":"nmeg/fluxproc_log/#problem-with-pj-data-card","title":"Problem with PJ data card?","text":"<p>There is a funky gap between about 1/10 and 02/04 in the card data. Can't tell why yet but it seems to fill in data that is missing in the FLUXALL file/TOA5 files. Data look ok so far.</p>"},{"location":"nmeg/fluxproc_log/#problem-at-gland-and-new_gland","title":"Problem at GLand and New_GLand","text":"<p>Between 2014-01-17 and 2014-03-04 the datalogger program was incorrect and did not increment the CS616 measurement channel properly. This means the data files for this period are incorrect because every other sensor's column contains the data from the prior sensor in the mux measurement loop (for sensors on the same CR5000 SE channel). The program loaded on 2014-03-04 fixes this problem. This should probably be fixed in the header resolution files somehow...?</p> <p>Something similar happened at New_GLand between 2014-06-13 and 2014-07-11 - Fixed (card was read incorrectly as GLand).</p>"},{"location":"nmeg/fluxproc_log/#par-at-gland-and-sland","title":"PAR at GLand and SLand","text":"<p>During 2009 (and possibly other years) GLand and SLand had standard PAR sensors (LiCor?) and Par-Lite sensors (not sure what this is). These sensors have a fundamentally different response. Par lite data is preferred for whatever reason, so there is a script (<code>combine_PARavg_PARlite.m</code>) to use Par-Lite data when available and linearly correct the other PAR observation to this.</p>"},{"location":"nmeg/fluxproc_log/#filling-data-for-2009-2014","title":"Filling data for 2009-2014","text":""},{"location":"nmeg/fluxproc_log/#pjc","title":"PJC","text":"<ul> <li>2009-2011 - no large gaps.</li> <li>2012 = TOB1 data missing on Dec 3 &amp; 4, not in wireless data.</li> <li>2013 and 2014 are pretty good </li> </ul>"},{"location":"nmeg/fluxproc_log/#pj_girdle_1","title":"PJ_girdle","text":"<ul> <li>2009: Data start February 23. Lots of gaps after this. Wireless downloads and cards do not have the data.</li> <li>2010 and 2011 are pretty good</li> <li>2012: Large gap Oct 29 to Dec 3 - not on wireless or in old card data</li> <li>2013 has a 6-day gap in mid August and a 2 day gap in late Nov (TOA5 and TOB1 data missing) not on wireless or in cards.</li> <li>2014 is pretty good </li> </ul>"},{"location":"nmeg/fluxproc_log/#mcon_1","title":"MCon","text":"<p>Earliest wireless is from Sept 2012</p> <ul> <li>2009 - 18 days of TOA5 and TOB1 data are missing at the start of the year. Can't get them off card and no wireless at this time.</li> <li>2010 - Card data from late-Jan to mid-March and mid-June to mid-July is corrupt and there are big gaps in TOA5 and TOB1 data here both times. This data may exist somewhere, because the old fluxall seems to have some of the february data, but I'm not sure where....</li> <li>2011 - is pretty good.</li> <li>2012: There are large gaps in TOB1 and TOA5 data between April 15 and May 11, and between June 17 and July 20 that are not fillable with card data.</li> <li>2013: Site down from May 3 to Nov 11 due to fire.</li> <li>2014 - Jun 12, 09:30 to  Jun 20, 11:00 - not sure what happened here - original TOA5 data looks like these dates were overwritten with earlier data. There were some instrument changes and new programs that may have contributed.</li> </ul>"},{"location":"nmeg/fluxproc_log/#gland_1","title":"GLand","text":"<ul> <li>2009: A fair amount of TOB1 data is missing - wireless downloads are unreliable this far back. A small amount of card data was repaired and filled in a couple days. In old fluxall files some of this may be filled in with either the 30 minute data or the <code>UNM_gapfill_from_local_data.m</code> script.</li> <li>2010 is pretty good.</li> <li>Large gap in June/July 2011 - was able to fill in some TOA5 data, but card and wireless ts_data files for this period are corrupt (I guess).</li> <li>No TOA5 data or TOB1 data to fill gaps in July/Aug 2012</li> <li>2013 and 2014 are good</li> </ul>"},{"location":"nmeg/fluxproc_log/#new_gland_1","title":"New_GLand","text":"<ul> <li>2010: Large gaps in TOA5 and TOB1 data Feb 19 to March 12 . Not fillable from card or wireless.</li> <li>2011: May 12 to June 22 TOB1 data is missing. Was able to rescue most of this from a corrupted card file, remaining fluxes could be filled in from TOA5 data.</li> <li>2012: Missing TOB1 data Aug 2-8 and TOA5 gap on the 7-8th. Filled Aug 2-7 with wireless data. TOA5 missing data seems to show the batteries or power failing in early morning hours starting in April/May.</li> <li>2013 &amp; 2014 are pretty good - 1-2 days TOB1 and TOA5 (contiguous) data missing</li> </ul>"},{"location":"nmeg/fluxproc_log/#sland_1","title":"SLand","text":"<ul> <li>2009 - All but 10 days of TOA5 data filled in</li> <li>2010-2012 are pretty much filled in</li> <li>2013 -13 day gap in August that I can't find wireless TOA5 or TOB1 data for.</li> <li>6 day gap in July 2014 data that I can't find wireless TOA5 or TOB1 data for.</li> </ul>"},{"location":"nmeg/fluxproc_log/#jsav_1","title":"JSav","text":"<ul> <li>There was no wireless at JSav prior to 2011.</li> <li>2009 - There is a gap in TOB1 and TOA5 data at the start of the year (Jan 1 to Feb 2nd) that I can't find or fix data for. Some of the fluxes after Jan 20 can be filled in with existing 30min data. There is also a big gap in TOB1 data from 5/21 to 6/14/2009 that could be filled in with 30 min data (TOB card data is corrupt).</li> <li>2010 - There is quite a bit of TOA5 and TOB1 data in August that seems to be unrecoverable.</li> <li>2011 - pretty good</li> <li>2012 - gap in fluxes at end of year that could be filled with 30 min data?</li> <li>2013 there are multi-day gaps in TOB1 data in March and July that cannot be found on wireless.</li> <li>2014 - pretty good except for two 1-day TOA5 gaps in September (not on wireless)</li> </ul>"},{"location":"nmeg/fluxproc_log/#ppine_1","title":"PPine","text":"<ul> <li>Don't think there is any wireless data for PPine before 2012 or maybe 2013.</li> <li>2009 and 2010 TOB1 data are good. There is missing TOA5 data in March/April 2010, but it appears to be related to battery/power problems at the site ( ? - short outages ).</li> <li>Early 2011 gaps (Jan/February) in TOB1 and TOA5 data seem to be due to faulty/corrupt cards (tried to repair but failed). Some fluxes might be fillable from TOA5 data.</li> <li>There was a corrupt data card in Aug 2012 that was sent to Campbell and data was returned with the wrong timestamps. This needs to be put into the fluxall files manually, but I'm not sure how to do this yet. See documentation for this card in the <code>PPine\\ts_data\\20_Aug_2012_PPine_tsdata</code> directory and the <code>fix_20Aug2012_PPine_card_data.m</code> script.  A couple days of TOA5 data were lost around the same time.</li> <li>2013 - Large gap associated with the fire (not fillable) and ~30 days of missing TOA5 and TOB1 data in late Nov through Dec that cannot be found in wireless or card data.</li> <li>2014 - Quite a bit of TOA5 data missing, but never for very long periods (except for 1 day in Dec that was not on wireless).</li> </ul>"},{"location":"nmeg/fluxproc_log/#3-12-2015","title":"3-12-2015","text":""},{"location":"nmeg/fluxproc_log/#3-11-2015","title":"3-11-2015","text":"<p>Making new FLUXALL and AF files.</p> <ul> <li>Most of PJ AF files look good - but the Rg filling still needs to be fixed. Waiting for PJ_girdle to be finished first.</li> <li>Filling in some missing data for 2009 and 2012 at PJ_girdle.<ul> <li>Could not find any missing 2009 data - wireless files were unusable.</li> <li>Was able to fill in some data in Aug 2012 with wireless, but not the large gap from 10/29/2012-12/04/2012</li> <li>Made new fluxall files for PJ_girdle 2009-2013.</li> </ul> </li> <li>Working on GLand filling. Lots of gaps in 2009, 2011, and 2012.</li> </ul>"},{"location":"nmeg/fluxproc_log/#3-5-2015","title":"3-5-2015","text":"<p>Making new FLUXALL and AF files.</p> <ul> <li>All PJ - relaxed CO2 limits to: co2_max_by_month = [ 2.5, 2.5, 2.5, 3.5, 4, 5, 6, repmat( 6, 1, 5 ) ]</li> <li>One of the HMP sensors delivered bad Tair and RH data from 2009 to late 2012 (or later). Data from this sensor, which appears to be the canopy level sensor (not top of tower)  is labeled AirTC_2 and RH_2. This is a potential issue for other variables output from the datalogger as well, as e_sat, e, h2o_hmp, and rho_d are calculated with these to variables by the datalogger. This AirT and RH data is now excluded from the qc and gf files (but notes remain  below).</li> <li>PJ 2009 &amp; 2010: OK, except for rH</li> <li>PJ 2011<ul> <li>rH</li> <li>There are some low CO2 filters applied at the end of the year that seem a little strange. CO2 (ppm measured by IRGA) went pretty low at this time, probably due to some type of calibration problem. There are also cal problems in 2012.</li> <li>Missing data periods: day 33, 59.5-61.5, 336-338, 352-357</li> </ul> </li> <li>PJ 2012<ul> <li>rH has the same problem, but changes to a new problem out 2/3 through year</li> <li>Change in datalogger time on afternoon of day 342.</li> <li>Created exception to low_co2 filter for days 168-174.</li> <li>Missing data periods: day 19-23.5 (not fixable), 70.5-71.5, 220-237.5 (found on wireless), 312.5-317.5 (not fixable), 338-340 (not fixable).</li> </ul> </li> <li>PJ 2013<ul> <li>rH looks like garbage the entire year.</li> <li>There is a messed up Rg sensor from mid June to mid July. Logs indicate that there was a broken lead that led to this that was repaired around July 17.</li> <li>Filling the Rg data was problematic. We fill from PJG by default, which has a similar timeshift, so initially the timing of the filled in Rg data was offset. MAKE SURE TO CORRECT ALL \"for_gapfilling\" FILES BEFORE MET GAPFILLING!!!!.</li> </ul> </li> </ul>"},{"location":"nmeg/fluxproc_log/#2-27-2015","title":"2-27-2015","text":""},{"location":"nmeg/fluxproc_log/#timing-issues","title":"Timing issues","text":"<p>Shifts and such are working now - one remaining issue - plotting data for a timestamp means you are plotting the data for the prior half hour. This means data in plots is always plotted 15 minutes later than the period it was collected. I'm not sure how this affects our time shifts yet.</p>"},{"location":"nmeg/fluxproc_log/#10hz-data-processing","title":"10hz data processing","text":"<p><code>UNM_process_10hz_main.m</code> is the main script that does this, This is called from <code>card_data_processor.m</code> if there is no 10hz data, or if reprocessing is specified.</p> <ol> <li><code>UNM_process_10hz_main</code> breaks the processing period into chunks ( 30 day is the default ) and gets directory of TOB1 files.</li> <li><code>UNM_process_10hz_main</code> then loops through each chunk and passes the start and end time of each chunk, some configuration items (lag, sonic orientation), and the directory to <code>process_TOB1_chunk.m</code>.</li> <li><code>process_TOB1_chunk.m</code> opens each file using <code>read_TOB1_file</code>, vertically concatenates the data, makes a matlab datenum for each observation, bins observations into 30 minute periods, and sends each 30 minute chunk of data to <code>UNM_30min_TS_averager</code>.</li> <li>It appears that <code>process_TOB1_chunk.m</code> correctly shifts the timestamp on these chunks such that each 30 minute average is for the the observations in the half hour PRIOR to its timestamp.</li> <li><code>UNM_30min_TS_averager</code> calls a number of other functions to correct high frequency data and calculate the 30 min fluxes from 10hz data ( <code>UNM_dry_air_conversions</code>, <code>UNM_csat3</code>, <code>UNM_flux_031010</code> and others).</li> <li>These 30 minute averages are then vertically concatenated by <code>process_TOB1_chunk.m</code>, the 30 day chunks are returned to <code>UNM_process_10hz_main</code>, vertically concatenated, given a timestamp, given <code>jday</code>, and then missing data periods are filled in (with timestamp and NaNs).</li> </ol>"},{"location":"nmeg/fluxproc_log/#2-25-2015","title":"2-25-2015","text":"<ul> <li>Something is wrong with the way <code>jday</code> gets calculated in the fluxall files. It starts out ok, but then gets rounded.<ul> <li>Figured this out - it was an issue with the precision of exporting FLUXALL files in <code>export_dataset_tim.m</code> I set this to 8 (should be a minimum from now on).</li> </ul> </li> <li>I have noticed that in most instances the timestamp variables are getting moved by <code>UNM_fix_datalogger_timestamps.m</code>, so the 30min data moves, but its timestamp variable is moved along with it.</li> <li>10hz data is moved relative to the 30min data, but its timestamps move with it also.</li> <li>However, AF files ARE actually shifted relative to FLUXALL data - this was confusing.</li> <li>After examination this doesn't seem to be too bad of a problem. In the <code>RBD</code> script at least, the original data timestamp is what is used to create the output files, so effectively, the data do move relative to the timestamp, even though it does not appear this way in the <code>UNM_fix_datalogger_timestamps.m</code> script. There is probably a better and more consistent way to do this though.</li> </ul>"},{"location":"nmeg/fluxproc_log/#2-22-2015","title":"2-22-2015","text":"<p>Implemented script to calculate solar noon, theoretical sunrise, and sunset for a site to use in our analysis (<code>noaa_solar_calculations.m</code>). This is based on the NOAA calculator here.</p>"},{"location":"nmeg/fluxproc_log/#2-20-2015","title":"2-20-2015","text":""},{"location":"nmeg/fluxproc_log/#investigating-time-shifts","title":"Investigating time shifts","text":"<p>I have mainly used PJ2010 as a case study for this, and have only used 1 date for checking (July 21, 2010 at 3pm). From the figures the AF team sent in August, it looks like our clocks are at least an hour fast (at solar noon, roughly 12:10 actual time our clocks say it is 1:30) in 2008-2010.</p> <ul> <li>There doesn't appear to be any problem putting 30min data into a fluxall file. Timestamps and data seem to match up between the TOA5 files and the fluxall files made in 2013.</li> <li>Made new 10hz data (TOB1_filled.txt files) and FLUXALL files for PJ 2009 and 2010 so I could compare timestamps in the FLUXALL files and make sure they were lining up with the raw data files (TOA5 and TOB1).</li> <li>Data in new FLUXALL files (mine) line up with the TOA5 files and TOB1 files!</li> <li>Some covariances and ustar also appear to line up pretty well between the 10hz and 30min sides of the files (ie. <code>wt_covariance</code> vs. <code>cov_Ts_Uz</code> and <code>ustar</code> vs <code>u_star</code>), but they are not exact.</li> <li>Fc values don't match particularly well between the 30min and 10hz sides of the FLUXALL file.</li> <li>The AF files on cdiac  (made in spring/summer 2013) contain the shifts defined in the current version of RBD.m and the qc file made around the same time. This means that 10hz data is shifted 30 minutes earlier, and 30min data is shifted 1 hour earlier relative to the fluxall made at the same time. The more current version of the fluxall file that I made has the same shifts.</li> </ul>"},{"location":"nmeg/fluxproc_log/#the-way-it-was-done-by-timrandy","title":"The way it was done by Tim/Randy","text":"<ol> <li><code>get_solar_elevation.m</code> is used to calculate a theoretical solar angle for each timestamp in a dataset.</li> <li>A function called <code>match_solar_elevation.m</code> then calculates the time differences (offset) between observed sunrise (first increase in radiation), and theoretical (solar elevation above 0).</li> <li>Running <code>UNM_site_plot_doy_time_offsets.m</code> for a particular day of the year will show the results of <code>match_solar_elevation.m</code>.</li> <li>These offsets are plotted in the function <code>UNM_site_plot_fullyear_time_offsets.m</code>.</li> <li>Somehow these plotted time offsets are used to make a determination about how much and when to shift the data.</li> <li>Actual shifting of the data (by site and year) is configured and executed in <code>UNM_fix_datalogger_timestamps.m</code> when the <code>RemoveBadData</code> scripts are run.</li> <li><code>UNM_fix_datalogger_timestamps.m</code> executes each individual shift with a call to<code>shift_data.m</code>. I suspect that the timestamp columns are being moved along with the data in most cases because the 'columns_to_shift' variables are set incorrectly. This should be fixed.</li> </ol>"},{"location":"nmeg/fluxproc_log/#2-17-2015","title":"2-17-2015","text":"<p>Currently updating ancillary meteorology (for gapfilling) data. Note that:</p> <ul> <li>PRISM data for the 2nd half of 2014 is still provisional (I have not downloaded it yet)</li> <li>There is no DayMet data for 2014 (yet?)</li> <li>I have found a way to download good data from the DRI Jemez site (documented in the README file)</li> <li>2014 VCP SNOTEL sites and VCP preserve sites are current.</li> <li>Still trying to track down a good way to get Sev data</li> </ul>"},{"location":"nmeg/fluxproc_log/#2-11-2015","title":"2-11-2015","text":"<p>Some current issues:</p> <ul> <li>Soil sensor/header histories are now complete, including new header resolution files. Soil sensor histories are in a separate note here. The code also incorporates separately logged data (second datalogger) into the fluxall files automatically now for all sites but MCon and PPine. There is still some work to do on resolving the DRI soil sensor data for  these sites. Still have not made new AmeriFlux \"soil\" files.</li> <li>Am working on improvements to the precipitation filling scripts. Once these are complete, data for 2014 from the various sources needs to be tracked down.</li> </ul>"},{"location":"nmeg/fluxproc_log/#1-16-2015","title":"1-16-2015","text":"<p>Have been piecing together the soil sensor/header histories for Sev sites and JSav for the last week. This has resulted in a new series of header resolution files and some changes to how they are generated. Once all sites are done, there are a few tasks left:</p> <ul> <li>Assemble good datasets of separately logged soil data from sites with extra dataloggers (MCon, PJ sites, PPine, and Jsav in 2012-13)</li> <li>Incorporate separately logged soil data into fluxall making code.</li> <li>Remove calls to other header renaming code (currently this is the <code>UNM_assign_soil_data_labels</code> files) as fluxall files are made.</li> <li>Figure out how to make new AF soil files using these new fluxall files.</li> </ul>"},{"location":"nmeg/fluxproc_log/#1-8-2015","title":"1-8-2015","text":"<p>Now that we are processing cards that span 2014-2015 there is a problem in which the 10 hz data processsing code does not allow this. So These cards will have to be split up to complete the 2014 fluxall files and star the 2015 fluxall files. This should be done in card_data_processor.m.</p>"},{"location":"nmeg/fluxproc_log/#1-5-2015","title":"1-5-2015","text":"<p>What is the deal with soil sensor data at a site? See the [[Site soil sensor histories]] note</p>"},{"location":"nmeg/fluxproc_log/#12-18-2014","title":"12-18-2014","text":"<p>Prior to going to AGU I made new AF files with filled precip. However, there is an issue where some files end with a row of zeroes that throws of reading and analyzing the data - should fix this.</p> <p>There is still a problem with headers PJG 2009 Radiation and CNR1 temp are messed up!</p>"},{"location":"nmeg/fluxproc_log/#12-11-2014","title":"12-11-2014","text":"<p>Have been working on improving the met gapfilling procedures and adding data sources for this (mainly for precip). See commits () and ().</p> <p>Note that all qc and for_gapfilling files will need to be remade.</p>"},{"location":"nmeg/fluxproc_log/#12-8-2014","title":"12-8-2014","text":"<p>Precip values in qc files are not corrected for the incorrect multiplier values.... this should be fixed.</p>"},{"location":"nmeg/fluxproc_log/#12-5-2014","title":"12-5-2014","text":"<p>To access DayMet data for our sites, there is a multiple pixel extractor tool that can be downloaded from the site. Downoad and unzip this tool, then edit the <code>latlon.txt</code> file within the new folder to include our site coordinates and the desired output filenames. Run <code>daymet_multiple_extraction.sh</code> (or the .jar file) and this will download temp, precip, and other data (1980 to 12/31 of prior year) for each point in the <code>latlon.txt</code> file and create a new csv file. </p>"},{"location":"nmeg/fluxproc_log/#12-4-2014","title":"12-4-2014","text":"<p>To access PRISM datasets requires reading the band interleaved (.BIL) files they provide. This can be done in by using GDAL from python, but some things need to be installed on the system first. On windows it is a little tricky and you need to be sure the GDAL you install (which comes from here) is compatible with your Python installation. </p> <ul> <li>Decent instructions can be found here.</li> </ul> <p>Once GDAL is installed, the python bindings need to work. I have installed the Anaconda Python distribution on the lab computer, with Python 3.4. GDAL is only compatible up to 3.3, so we need to also create a Python 3.3 environment. Instructions for this can be found here.</p> <ul> <li>Use <code>activate py33</code> from the windows shell to switch to this environment</li> </ul> <p>There are two python scripts and a list of coordinates (<code>site_coords.txt</code>) needed to generate daily precip datasets from PRISM. In ipython run <code>getPrismPrecip.py</code>. This calls the <code>prismParser</code> class to open the Bil files and extracts the PRISM grid cell value for each site listed in the <code>site_coords.txt</code> file.</p>"},{"location":"nmeg/fluxproc_log/#12-2-2014","title":"12-2-2014","text":"<ol> <li>Added a linear fit to make Rlong_incoming data at PJG for 2009 to 2013 match the same data at PJC. There was an offset between the sensors that  seemed to be in error. I generated the regression coefficients for each year using R, then applied the linear fit in <code>UNM_RBD_apply_radiation_calibration_factors.m</code>. See changeset 731 (05b89ee30c8b).</li> </ol>"},{"location":"nmeg/fluxproc_log/#11-25-2014","title":"11-25-2014","text":"<ol> <li> <p>Made new fluxall files for 2007-2008 at most sites (with records that far back). These have not had 10hz data filled in with 30min data because the <code>30_min_spooler</code> does not currently work for them.</p> </li> <li> <p>There is no precip data for MCon or PPine in 2007 &amp; 2008. It appears in the old fluxall files, but I have no idea where this originally came from. I tried pasting in precip from old files, but this didn't work well.</p> </li> <li> <p>There are no radiation components (including CNR1 temp) for PJC in 2007.</p> </li> <li> <p>PJG 2013 Rlong in and out are still missing in the new ameriflux files!</p> </li> <li> <p>PJ 2007 looks terrible in the partitioned data returned from eddyproc - should probably investigate this and exclude from Biederman analysis.</p> </li> <li> <p>Ultimately I was unable to make new <code>fgf</code> and <code>fgf_filled</code> files for MCon and PPine 2007 &amp; 2008, or for PJ 2007 due to the issues above. Instead of try to solve these problems now, I sent older <code>fgf_filled</code> files to Jena and will use these (at least for the 4 sites Joel needs).</p> </li> <li> <p>Also sent an older <code>fgf_filled</code> file to partitioner for MCon 2010 because the fluxall file I made is missing precip values and has large unfilled gaps (compared to old files).</p> </li> </ol>"},{"location":"nmeg/fluxproc_log/#11-24-2014","title":"11-24-2014","text":"<ol> <li>Have now created new AF files for all sites from 2009-2013<ul> <li>PJ 2011 will need to be revised to use the updated fluxall from Dan</li> </ul> </li> <li>Moving towards making new fluxall files for 2007-08<ul> <li>Copied TOB1 data over from archive disk (MyBook, L:).</li> <li>Have also moved old fluxall.xls files over from jemez to compare with.</li> </ul> </li> <li><code>UNM_30min_spooler</code> and the <code>UNM_30min_flux_processor</code> scripts that it calls need major work. Some of the old versions of them seem pretty buggy. </li> </ol>"},{"location":"nmeg/fluxproc_log/#11-20-2014","title":"11-20-2014","text":"<ol> <li> <p>I think the Massman and WPL corrections need to be examined in detail. When processing 10hz data files program calls are: <code>process_TOB1_chunk.m</code> -&gt; <code>UNM_30min_TS_averager.m</code> -&gt; <code>UNM_flux_031010</code> -&gt; <code>UNM_WPLMassman.m</code> (which calculates WPL and calls -&gt; UNM_massman.m to get the Massman correction). However, the WPL correction is also calculated in <code>UNM_flux_031010</code>, so it doesn't look like the <code>UNM_WPLMassman.m</code> calculation is used. I'm not sure which is correct. In any case, if these need to be changed all 10hz data will need to be recalculated into 30min fluxes. Which will take ... forever.</p> <pre><code>Another solution is just to remove the cold weather fluxes when they look bad at MCon. Essentially this is what the CO2 max-min filter (which I am still using) is doing, but I will have to improve on this solution.\n</code></pre> </li> <li> <p>Marcy is curious if PJG and PJC have the same energy balance. Need to compare R_long in and R_short in for both sites and see if they correlate well. If not, can we correct this (a regression maybe?).</p> </li> <li> <p>The radiation corrections in <code>UNM_RBD_apply_radiation_calibration_factors.m</code> are highly suspect. These need to be reviewed.</p> </li> </ol>"},{"location":"nmeg/fluxproc_log/#11-16-2014","title":"11-16-2014","text":"<ol> <li>Could not fill any missing 2013 or 2012 data for MCon using the wireless data that Renee provided (all are real gaps-no irga/sonic data).</li> <li>Filled part of 11/15-11/19 and 11/25-11/28 gaps with wireless data and will generate flux data with 30min side (ts_data file is corrupt).</li> <li>Figured out what fluxes from which files make it into Ameriflux files. This is determined in <code>UNM_Ameriflux_prepare_output_data.m</code>. Lasslop partitioning is used.</li> </ol>"},{"location":"nmeg/fluxproc_log/#unfilled-fluxes-are","title":"Unfilled fluxes are:","text":"<ul> <li><code>NEE_obs</code> comes from <code>fc_raw_massman_wpl</code> in the qc file for the site/year.</li> <li><code>LE_obs</code> comes from <code>HL_wpl_massman</code> in the qc file</li> <li><code>H_obs</code> comes from <code>HSdry_massman</code> in the qc file</li> </ul>"},{"location":"nmeg/fluxproc_log/#gapfilled-fluxes-are","title":"Gapfilled fluxes are:","text":"<ul> <li><code>NEE_f</code> comes from <code>NEE_f</code> in DataSetafterFluxpart.txt.</li> <li><code>LE_f</code> comes from <code>LE_f</code> in DataSetafterFluxpart.txt.</li> <li><code>H_f</code> comes from <code>H_f</code> in DataSetafterFluxpart.txt.</li> </ul> <p>The script ensures that observed values are used where available.</p>"},{"location":"nmeg/fluxproc_log/#partitioned-fluxes-are","title":"Partitioned fluxes are:","text":"<ul> <li><code>RE_f</code> comes from <code>Reco_HBLR</code> in DataSetafterFluxpartGL2010.txt (Lasslop partitioned).</li> <li><code>GPP_f</code>comes from <code>GPP_HBLR</code> in DataSetafterFluxpartGL2010.txt (Lasslop partitioned).</li> </ul>"},{"location":"nmeg/fluxproc_log/#recalculations-to-nee-gpp-and-re-to-ensure-carbon-balance","title":"Recalculations to NEE, GPP and RE to ensure carbon balance","text":"<ul> <li><code>NEE_2</code> = <code>NEE_f</code></li> <li><code>GPP_2</code> = <code>RE_f</code>  - <code>NEE_2</code>, when GPP is negative, set to zero and add difference to RE.</li> <li><code>RE_2</code> = <code>RE_f</code> plus the difference when NEE exceeds RE.</li> </ul> <p>These values (<code>NEE_2</code>, <code>GPP_2</code>, and <code>RE_2</code>) are then sent to the output both preserving original gaps, and in gapfilled form.</p>"},{"location":"nmeg/fluxproc_log/#11-12-2014","title":"11-12-2014","text":"<p>Verified that the burba correction we have in RBD.m is correct, according to the spreadsheet that came from George Burba. I put MCon 2010 data into both and came out with roughly the same values.</p> <p>I am a little concerned that units under particular headers differ between some years/sites. For example MCon 2010 <code>H2O_mean</code> header is in mmol/mol, while this changes to g/m^3 in later years. Rho measurements appear to be in different units, and moist and dry values may be confused in some fluxall files (based on what I can tell from the datalogger program).</p>"},{"location":"nmeg/fluxproc_log/#11-10-2014","title":"11-10-2014","text":"<ol> <li>Moved all old <code>fluxall.txt</code> files over from jemez (except 2014) from the 8 sites. For now I am ignoring Excel fluxall files but have included other ancillary datasheets (not sure what some are).</li> <li>Moved all old <code>processed_data</code> files over from jemez. Within each sites <code>processed_data</code> directory, the old files are in the <code>fgf_archive</code>, <code>fgf_filled_archive</code>, <code>qc_archive</code>, and <code>fluxpart_archive</code> folders.<ul> <li>for the most part I have checked and then left files in subdirs of <code>processed_data</code> on jemez. Usually these are subsets of data, archived data (perhaps prior to processing changes), or irregular looking files. May be worth looking at these at some point.</li> </ul> </li> <li>In process of moving all data in each sites <code>toa5</code> and <code>ts_data</code> directories from Jemez to Sandia.<ul> <li>GLand, JSav, MCon, New_GLand, PJ, PPine, PJ_girdle, SLand</li> <li>All subdirectories in Jemez <code>toa5</code> and <code>ts_data</code> folders were moved</li> </ul> </li> <li>Moved old ancillary met data over from jemez.</li> <li>Moved old licor 8100 data files and MCon precip data files over from jemez</li> </ol>"},{"location":"nmeg/fluxproc_log/#11-07-2014","title":"11-07-2014","text":"<p>Met with Marcy/Dan/Laura:</p> <p>TO DO in prep for AGU:</p> <ol> <li>Get calculated u* tresholds calculated for all years at all sites (or as many as possible) using the MPI eddyproc tool.</li> <li>Make plots of the binned u/NEE from each site/year (output from RBD.m) 3 Compare the MPI and our u thresholds</li> <li>Check that longwave radiation looks ok in current and older AF files</li> <li>Compare Reichstein and Lasslop partitioning of Reco at MCon and decide which to use.</li> <li>Regenerate older AF files with new changes to RBD (inc. Burba, u*, etc) - do this for as many sites/years as possible.</li> </ol>"},{"location":"nmeg/fluxproc_log/#11-06-2014","title":"11-06-2014","text":"<p>Working on Burba correction and other issues at MCon. </p> <ul> <li>implemented the Burba correction but it doesn't change winter uptake fluxes radically (they are still there mostly).</li> <li>Also investigated timestamp corrections to see if they were to blame, but no indication of this yet</li> </ul>"},{"location":"nmeg/fluxproc_log/#11-05-2014","title":"11-05-2014","text":"<p>Met with Marcy to discuss some issues with MCon data. Part of this is motivated by Joel Biederman's findings that this site seems to have abnormally low Reco (relative to GPP). A few things need to be examined.</p> <ul> <li>We have somewhat unusual looking amounts of uptake (negative NEE) in the depths of winter, even when temps are low. This may indicate that our Burba cold temp correction is faulty.<ul> <li>I checked this, and think the Burba correction isn't being applied to the flux data. It seems to be calculated correctly (I think), but it is never applied to the <code>Fc</code> values that are exported for further processing.</li> </ul> </li> <li>The partitioned fluxes at MCon look sort of funny. Reco is pretty low in winter/spring (and similar to PPine), but there is not much of an increase when the weather warms up. This could be a partitioner problem (we should compare both the Lasslop and Reichstein methods). </li> <li>Other possible explanations for the low respiration include N deposition, which can suppress decomposition and stimulate GPP, low C stocks due to harvest (though Marcy seems to think there is tons of soil C there), or we may be losing some of the respiration to downslope flow from the ridge.</li> <li>There is a pulse of respiration around May during many years at MCon. Maybe this is snowmelt related?</li> </ul> <p>TO DO:</p> <ol> <li>Fix the Burba correction in RBD.m and see that it is applied to the outgoing data.</li> <li>Make plots of Fc with and without the Burba correction.</li> <li>Make winter plots of uncorrected and Burba corrected flux vs temperature.</li> <li>Plot summer Reco form both Lasslop and Reichstein partitioning methods.</li> <li>Look at the timing of snowmelt (maybe using albedo) at MCon to see if the positive NEE pulse is related to this.</li> <li>Marcy wants to know what parameters the eddyproc site uses - not sure which parameters or how to find them out. Maybe look in the info log?</li> <li>Prod Marcy to ask around about N deposition and soil C data at MCon.</li> </ol>"},{"location":"nmeg/fluxproc_log/#11-04-2014","title":"11-04-2014","text":"<p>Solving GLand/New_GLand problems from prior months - all card data files have been moved to the right folders and are ready to be reprocessed. New(ish) cards have also had files copied to <code>Raw_data_from_cards</code> directories, and to a directory on the NAS.</p> <ul> <li>There is a missing <code>1473.ts_data.dat</code> file (for GLand) from 9/4/2014. This file exists on the card but it is corrupt and cannot be copied or repaired by CardConvert. May need to be found on wireless.</li> </ul>"},{"location":"nmeg/fluxproc_log/#11-03-2014","title":"11-03-2014","text":"<p>Revisited ustar filtering (run <code>UNM_RemoveBadData(UNM_sites.{SITE}, {YEAR}, \"iteration\", 1, \"draw_plots\" 3)</code> at SLand, PJ, PJG, and JSav.</p> <ul> <li>Changed JSav: 0.8 --&gt; 0.9</li> <li>I sent 2013 data from the lowest 6 sites to the MPI partitioner without u filtering applied and asked for eddyproc to do u filtering.<ul> <li>In general, eddyproc's selected u* thresholds are much more conservative (higher) than ours.  They have been added as comments in RBD.m</li> </ul> </li> <li>Did not change any limits for now. These seem like pretty subjective calls to use the plots RBD.m makes.</li> </ul>"},{"location":"nmeg/fluxproc_log/#10-31-2014","title":"10-31-2014","text":"<p>The 1 day per 30 gaps in fluxall TOB data is being introduced in the flux processing code for some reason</p>"},{"location":"nmeg/fluxproc_log/#10-29-2014","title":"10-29-2014","text":"<ul> <li> <p>There are now 2 irgas at SLand. Archive the <code>ts_data2.dat</code>and <code>flux.dat</code> files in the <code>Raw_data_from_cards</code> folder, then cancel conversion to TOA5 and TOB1. At some point we'll need to process these, and there will be an extra irga at GLand.</p> </li> <li> <p>Other sites missing data in 2013:</p> <ul> <li>PJG and SLand have missing 10hz data one day a month (not sure why) - this was all filled in with 30min data</li> <li>PJ 11/22 - 26 - nothing on wireless, looks like irga was down - not filled</li> <li>GLand 10/18 - 22 - filled in with wireless data.</li> <li>New_GLand 11/22-24 and 11/24-25 were missing from the card data and wireless data - not filled</li> <li>PPine has big gaps in Nov. and Dec., but there is no wireless data to fill with.<ul> <li>30min data that is available (rare) during PPing gaps looks pretty suspect to me so I didn't fill from 30 min periods.</li> </ul> </li> <li>Similar problem at MCon - there are gaps (outside of fire) but the 30min data available to fill them looks funny to me.</li> </ul> </li> <li> <p>All the 30min fill in periods for JSav, SLand, GLand, and PJ_girdle were added to <code>UNM_30_min_spooler.m</code>.</p> </li> </ul>"},{"location":"nmeg/fluxproc_log/#10-28-2014","title":"10-28-2014","text":""},{"location":"nmeg/fluxproc_log/#gap-filling","title":"Gap filling","text":"<ul> <li>Done patching JSav 2013 from wireless and 30 min data<ul> <li>Filled  3/20-4/17 and 4/24-5/16 gaps with wireless data, but the 3/20-4/17 part is very sparse (intermittent irga problems?)</li> <li>Also filled some gaps in October with wireless data</li> <li>Filled 3/27- 4/1, 7/20-7/24 and 11/27 from 30min data (<code>UNM_30min_flux_processor_Tim</code>)</li> <li>Couldn't find wireless or 30 min data to fill missing periods from Jan 1 - 16 (irga down) and July 25-29 (irga and sonic down?)</li> </ul> </li> <li>Done patching SLand 2013<ul> <li>Patched 1/1 - 1/16, 2/13-2/15, 3/15-3/19 with wireless data.</li> <li>Filled 1/1, 3/2, 8/28, 9/28, 10/28 with 30min data.</li> <li>Couldn't find  wireless or 30min data to fill Aug 14-28.</li> </ul> </li> <li>Done patching PJ_girdle<ul> <li>Patched 4/24-4/30 with wireless data - still seems to be empty, to the irga was probably down</li> <li>Couldn't find wireless or 30min data to patch Aug 14-21 or Nov 22-25</li> </ul> </li> </ul>"},{"location":"nmeg/fluxproc_log/#10-24-2014","title":"10-24-2014","text":"<p>Today I am filling missing 2013 data from wireless downloads. First step was to make a new TOA5 file for JSav 2013 (<code>TOA5_JSav_2013_04_24_1230.dat</code>) using a wireless download and vim. The data were missing due to a messed up card (I think) but were found on the wireless.</p>"},{"location":"nmeg/fluxproc_log/#steps-to-create-new-toa5-files-from-wireless-data","title":"Steps to create new TOA5 files from wireless data.","text":"<p>WARNING - Poorly formatted TOA5 files cause big problems. Be careful to follow all these steps. Also, this will overwrite old fluxall files. Make a backup!</p> <ol> <li>Copy the desired wireless files (something like <code>NMUFN_{SITE}_CR5000_flux.dat.backup</code> file to the <code>FLUXROOT\\{SITE}\\wireless_data</code> directory.</li> <li>Open the file in Excel and delete the 'Records' column (second column) from the second line to the end of the file.</li> <li>Excel screws up date formatting, so select the dates and format to custom -&gt; 'yyyy-mm-dd hh:mm:ss' 4 Save as a csv file with a filename that matches the startdate of the 30min period being patched (<code>TOA5_JSav_2013_4_24_1230.dat</code>).</li> <li>Since Excel has screwed up the formatting, copy the correct headers (lines 2-4) over from the wireless backup data file, then remove the second headers in lines 2-4 (indicating \"Record\"). I normally do this in a diff program.</li> <li>Open in vim (or another text editor) and delete all lines with timestamps outside the range needed to fill the gap. 7 Put quotes around the date string. In vim do this with:<ul> <li><code>%s/^2013-/\"2013-/g</code></li> <li><code>%s/:00,/:00\",/g</code></li> <li>These should make changes on all lines</li> </ul> </li> <li>Excel has also changed \"-INF\" values to #NAME? so these need to be changed back with <code>%s/#NAME?/\"-INF\"/g</code></li> <li>Put the new TOA5 files in  <code>\\wireless_data\\toa5_patch</code>.</li> </ol> <p>From here - copy the new files into the appropriate folder (<code>\\toa5</code>), make a new cdp object and do <code>cdp.update_fluxall</code> (making sure it loads and processes correct TOA5 files).</p>"},{"location":"nmeg/fluxproc_log/#steps-to-create-new-tob1-files-from-wireless-data","title":"Steps to create new TOB1 files from wireless data.","text":"<p>WARNING - this will overwrite old fluxall files. Make a backup!</p> <ol> <li>Copy the desired wireless files (something like <code>NMUFN_{SITE}_CR5000_ts_data.dat.backup</code> file to the <code>FLUXROOT\\{SITE}\\wireless_data</code> directory.</li> <li>Change the extension of files you wish to convert to TOB1 format to <code>{FILENAME}.ts_data.dat</code>  (remove <code>backup</code> and change 2nd to last '_' to '.'</li> <li>Run <code>[tsdata_convert_success, ts_data_fnames] = tsdata_2_TOB1(UNM_sites.JSav, fullfile(get_site_directory(UNM_sites.JSav), 'wireless_data'), 'wireless', true)</code></li> <li>This should go through the archived wireless file and make new TOB1 files and put them in <code>\\wireless_data\\ts_data_patch</code>.</li> </ol> <p>From here - copy the new files into the appropriate folder, make a new cdp object and do <code>cdp.update_fluxall</code> (making sure it loads and processes correct TOB1 files).</p>"},{"location":"nmeg/fluxproc_log/#10-22-2014","title":"10-22-2014","text":"<p>There are a few precipitation wiring and logging issues that have taken place at some sites over the years, and these need to be corrected during the flux processing. Currently these are only fixed on the way to making Ameriflux files (so fluxall and qc files are wrong) by <code>fix_incorrect_precip_factors.m</code>.  </p> <p>The problems described below are fixed as of today (see Changeset 699 (dbf0805b78ad))</p> <ol> <li> <p>PJC</p> <ul> <li>The metric precip gauge was installed 1/15/2008</li> <li>On 5/12/2010 the precip multiplier changed (vol per tip in mm or in) from .254 to 0.1 in <code>PINON_JUNIPER_051210.CR5</code></li> <li>The rain gauge has metric output (0.1 mm/tip), so this corrected incorrect logging of precip (was .254 since install)</li> <li>This program was uploaded to the CR5000 on 5/31/2010 according to the logs</li> <li><code>fix_incorrect_precip_factors</code> corrected the earlier data for the Ameriflux files (but not other files), but the dates of the correction were wrong (started too early - May 12th) and extended too long (Randy extended to 2013).</li> <li>2010 Ameriflux file may need to be remade.</li> </ul> </li> <li> <p>JSav</p> <ul> <li>A metric rain gauge (.1mm/tip) was installed at the site on 1/15/2008 (though this was either verified or reinstalled on 1/11/2012).</li> <li>The datalogger programs recorded each tip as .254 mm until January 10, 2014 (program <code>JSav_010714.CR5</code> sent to datalogger).</li> <li>This was corrected in <code>fix_incorrect_precip_factors</code> for 2010-2013, but early 2014 precip still needed a correction</li> </ul> </li> <li> <p>SLand</p> <ul> <li>In 2008 a new TE525USW (inches - 0.01in/tip) gauge was installed.</li> <li>The rain gauge at the site broke sometime in summer/fall of 2011.</li> <li>It is not clear if it was fixed or was replaced with a US inches calibrated gauge (.254 mm/tip), around Nov/Dec of that year and there is a note that it was still not working on 11/22/2011</li> <li>It was also changed to pulseport 1 on 11/22/2011</li> <li>Jonathan notes the rain gauge was working again (after some troubleshooting) on 4/2/2014</li> <li>Datalogger programs seem ok - conversion factor is 0.24</li> <li>Doesn't really seem like anything needs fixing - it will just have to be filled in with another site.</li> </ul> </li> <li> <p>GLand</p> <ul> <li>In 2007ish a new TE525mm (metric - 0.1mm/tip) gauge was installed.</li> <li>This was replaced on 8/21/2008 with a new gauge (same model?).</li> <li>Precip multiplier was .254 until program <code>UPLAND_GRASS_011614.CR3</code></li> <li>New program (corrected multiplier to 0.1) was sent on 1/17/2014</li> </ul> </li> <li> <p>New_GLand</p> <ul> <li>On January 17, 2014 a new program was uploaded (<code>newgrass_011614.CR5</code>) which changed the precip measurement from pulse port 1 to port 2. The gauge, however, was still wired to pulse port 1, so no precipitation (all zeroes) was collected thereafter.</li> <li>At the same time, the multiplier changed from .254 to 0.1 (wrong).</li> <li>The gauge was rewired to pulse port 2 on 4/2/2014 and started working again, but the multiplier was still 0.1 (incorrect)</li> <li>Correct multiplier (0.254) restored on 9/24/2014 when newgrass_092414.cr5 was loaded.</li> <li>Note that we are MISSING PRECIP data up to 4/2/2014! These should be filled from another site, which will happen because earlier these data are now set to NaN by 'RemoveBadData.m`.</li> </ul> </li> </ol>"},{"location":"nmeg/fluxproc_log/#10-15-2014","title":"10-15-2014","text":"<p>There was a problem with the filling of data from nearby sites at PPine (see Rg filling for PPine 2013 when <code>UNM_fill_met_gaps_from_nearby_site.m</code> is run)</p> <ul> <li>The filled data was MUCH lower in magnitude than the original data, creating a mismatch</li> <li>This appears to be related to the linear fit that is applied to some data - see the <code>fill_variable</code> function in the script.</li> <li>FIXED: It was the linear fit, but it failed because there were zeroes in the Fluxall data (removed with RBD.m now see  Changeset 694 (a0741548c5de))</li> </ul>"},{"location":"nmeg/fluxproc_log/#08-29-2014","title":"08-29-2014","text":"<ul> <li>Fixed matching timestamp problem in the combined 23x fluxall files I am making for laura. It has to do with the start and end date of the soil and fluxall files and whether they match.<ul> <li>see the comments in preprocess_PJ_soildata_DK1.m</li> <li>There is still some manual fiddling required to make the files build that depends on what the fluxall files look like for that year</li> </ul> </li> <li>Tweaked the JSav header resolutions file - now it does not work in the script - and will need more fixing?</li> </ul>"},{"location":"nmeg/fluxproc_log/#08-27-2014","title":"08-27-2014","text":"<ul> <li>Made <code>UNM_Ameriflux_Filemaker.m</code> work<ul> <li>it needs all yearly Ameriflux datafiles to make the concatenated daily file, these are put in FluxOut by default</li> </ul> </li> <li>There is a problem with the outputted file from PJG - no Rlong data. It is being removed by the <code>UNM_Ameriflux_prepare_output_data_TWH</code> script because it is out of range. This may not be a problem for other sites.</li> <li>Longwave radiation is negative when it comes in on cards - why?</li> </ul>"},{"location":"nmeg/fluxproc_notes/","title":"NMEG flux processing workflow (MATLAB version)","text":"<p>These are the basic steps/scripts for generating 30-minute averages and calculating fluxes from our 10hz data at each site.</p> <ol> <li> <p><code>UNM_process_10hz_main.m</code> is the primary script for processing 10hz data in TOB1 files.</p> <ul> <li>Generates list of daily TOB1 files (in 30 day chunks by default) for processing by <code>process_TOB1_chunk.m</code></li> <li>Concatenates the resulting data.</li> <li>Default sonic rotation is 3d (3-axis or double rotation)</li> </ul> </li> <li> <p><code>process_TOB1_chunk.m</code></p> <ul> <li>Read TOB1 files in each 30 day chunk, break into 30min blocks</li> <li>Send each 30min block to <code>UNM_30min_TS_averager.m</code> and then concatenate resulting data.</li> </ul> </li> <li> <p><code>UNM_30min_TS_averager.m</code></p> <ul> <li>Calculates average values for 30 minute blocks of 10hz data, including calculated fluxes. In the process of doing this it calls:<ul> <li><code>UNM_dry_air_conversions.m</code> which calculates Tdry, despikes pressure data, and does other conversions.</li> <li><code>UNM_csat3.m</code> which despikes u, v, w wind data, adjusts THETA using the site-specific sonic orientation, and calculates 30min averages of wind data.</li> <li><code>UNM_coordrot.m</code> which provides the coordinate rotation for flux calculations. Default is 3d, which is not ideal, but there is a method for planar rotation here too.</li> <li><code>UNM_flux_031010</code> which calculates the covariance matrices for wind and scalar data, performs raw flux calculations and corrections (see sub-bullets). This code can loop using different lag times, but the default is no lag. It returns a 30 minute flux observation. The corrections called are:<ul> <li><code>UNM_WPLMassman.m</code> which calls <code>UNM_Massman.m</code>. These two scripts perform the high frequency loss correction (Massman method), and the Webb, Pearman, Leuning density corrections (in that order).</li> <li>Linear detrending of variables has in the past occurred in the <code>UNM_Massman.m</code> code, but it appears that this is no longer the case.</li> <li>The WPL correction may actually be done at the very end of this script (needs to be checked) and that becomes the 'FC_corr/raw_massman_ourwpl.m' flux.</li> </ul> </li> </ul> </li> <li>The <code>UNM_30min_TS_averager.m</code> then puts these together and returns met and flux averages for the 30 minute period.</li> </ul> </li> </ol> <p>The steps above create a dataset that is merged into the SITE_fluxall.dat file. There are additional steps that occur in the <code>RemoveBadData.m</code> script. These are:</p> <ol> <li>Burba correction (T below 0 C)</li> <li>Lots of hi/low filtering, flux removal based on precip, behind tower wind, nighttime negative fluxes, low ustar values, outlier removal,</li> </ol>"},{"location":"nmeg/fluxproc_priorities/","title":"NMEG FluxProc Priorities","text":"<ol> <li> <p>Gapfill and partition the PJC and PJG datafiles</p> <ul> <li>May require regenerating the \"for_gapfilling\" files with VPD (in hectapascals?) and a qc column.</li> </ul> </li> <li> <p>What to do with separate Met and Li8100 data?</p> <ul> <li>Archive raw card data for now.</li> </ul> </li> <li> <p>Fluxflag scripts?</p> </li> <li> <p>Temperature corrected longwave radiation does not make it to ameriflux files - seems that the uncorrected values are going into the files instead</p> </li> <li> <p>Fluxall files are inconsistent in the dates that they contain. Some start at year/1/1 00:00 (last half hour of previous year), some start a half hour later. Some end a half hour early. This cause problems when combining the data with other dataloggers. Need to address this and create some timestamp conversion tools.</p> </li> <li> <p>An unburned grassland (New_GLand) card may have been processed as GLand on 8/5/2014 (see incorrect datalogger id in the 7z file)</p> </li> <li> <p>Where does the Air temperature in the for_gapfilling file come from - it is not from the qc file (see pj and pj girdle Dec 8, 2011 at 5pm)</p> <ul> <li>It is actually derived from Tdry (dry bulb temperature in Kelvin) and converted to C.</li> <li>but where does Tdry come from - it comes in with the flux data.</li> <li>This is sonic air temperature (Ts), and it is corrected to be \"dry temperature\" by the \"dry_air_conversion\" file.</li> </ul> </li> <li> <p>RBD file assigns the last header matching a list to air_temp_hmp and rH. At sites with multiple sensors this is often wrong - look at air temp at PJC.</p> </li> <li> <p>There is no hmp RH data from PJ girdle for 2011 and 2012 (not in FLUXALL file), but something does get put in the qc and for gapfilling files - what is this? </p> <ul> <li>It is calculated from other hmp variables found in FLUXALL FILE</li> </ul> </li> <li> <p>Rounding errors for PAR in Ameriflux Daily files  - see PJ Daily AF files (PAR rounded to thousands place)</p> </li> <li> <p>There are 2 UNM_Ameriflux_filemaker  pipelines (regular and \"_TWH\"). The TWH version uses the output from the MPI online flux partitioning tool (old version). The new one uses the REddyProc R scripts (Accessed with UNM_run_gapfiller.m).</p> </li> <li> <p>PJ Girdle 2014 fluxall file is missing a bunch of data - mostly the 10hz data</p> </li> <li> <p>JSav 2013 Fluxall has lots of NAN columns in the TOB1 data - need to finish making Amflux files</p> <ul> <li>apparently these exist in the TOB1 data also - maybe they can be filled with wireless data?</li> </ul> </li> <li> <p>Soil data headers, including soilT, SWC and shf data are renamed from the incoming raw data  by the <code>UNM_Assign_soil_data_labels.m</code> script.</p> <ul> <li>This interferes with the header resolution scripts, and it may make more sense to remove this and replace it with the header resolution system<ul> <li>Soil header changes would be put into the header resolution file (or maybe just for 2013 on?)</li> </ul> </li> <li>Also, there is a separate file that deals with JSav 2009 - which seems like unnecessary code overlap</li> </ul> </li> <li> <p>Variable names in old fluxall files (starting in around 2010) are different.</p> <ul> <li>I have changed 'u_mean' to 'u_mean_unrot' in 2010 fluxall files from some sites.</li> <li>I have changed 'windDirection(theta)' to 'wind_direction' in some 2010 fluxall files</li> </ul> </li> <li> <p>Something is wrong with PAR at JSav in 2014. It is all getting normalized to zero.</p> </li> <li> <p>PPine 2013 Ameriflux file</p> <ul> <li>Fix UNM_fill_met_gaps_from_nearby_site.m so that it uses the correct data for PPine.</li> <li>This should eliminate gaps in precip and shitty VPD gapfilling.</li> </ul> </li> <li> <p>Add criteria and data removal for atmospheric stability parameters  and advection at sites</p> <ul> <li>For stability and advection calculations see chapter in Lee, Massman, and Law book.</li> <li>This will be done in RemoveBadData for now.</li> </ul> </li> <li> <p>JSav 2013 Ameriflux problems.</p> <ul> <li>Fluxes Reco, GPP, FC seem very low (magnitude and variability) between days 60-170.</li> <li>Is there an incorrect hard cap on R, GPP, or FC for the springtime period in 2013? </li> <li>Is there a diurnal StdDev check that isn't functioning?</li> </ul> </li> <li> <p>Make ameriflux file spit out Tsoil so we can test whether this makes for better flux filling/partitioning?</p> </li> <li> <p>In the ameriflux files, there is no way to tell what values were locally gapfilled. This can be a problem if values calculated (in RBD, say) from missing met data end up as NaN, but the missing met data is later filled in with local or gapfiller data.</p> </li> <li> <p>New precip problem at PJ Girdled from 1/1/2009 to 6/2/2009 (or a little earlier)</p> <ul> <li>Data (no of tips) copied from PJC, then converted with the PJG multiplier (so precip is doubled)</li> </ul> </li> </ol>"},{"location":"nmeg/removebaddata_primer/","title":"RemoveBadData.m primer","text":"<p>This is a brief tour of the <code>UNM_RemoveBadData.m</code> script that filters all our flux data. I am making this so I can figure out how to improve this script.</p>"},{"location":"nmeg/removebaddata_primer/#script-settings-l67-106","title":"Script settings (L67-106)","text":"<p>Reads in arguments, prepares input and output filenames, switches on or off various analyses and outputs.</p> <p>Note that this is where iteration number is set. Its default is 6, which means all iterations are run</p>"},{"location":"nmeg/removebaddata_primer/#siteyear-parameter-setting-l107-275","title":"Site/Year parameter setting (L107-275)","text":"<p>For each site set the ustar threshold, standard deviation filter,  hi/low values for met and micromat variables. Some sites have changing parameters for different years.</p>"},{"location":"nmeg/removebaddata_primer/#parse-fluxall-shift-timestamps-assign-variables-l276-538","title":"Parse fluxall, shift timestamps, assign variables (L276-538)","text":"<ol> <li>Parse the fluxall file (<code>UNM_parse_fluxall_txt_file.m</code>) and pull out size, etc.</li> <li>Shift the timestamp data in the file using <code>UNM_fix_datalogger_timestamps.m</code></li> <li>Assign a bunch of met and micromet variables from the fluxall data.</li> <li>There are also a few unit conversions in this section.</li> </ol>"},{"location":"nmeg/removebaddata_primer/#soil-data-assignments-l539-665","title":"Soil data assignments (L539-665)","text":"<p>This section mainly assigns the proper soil header to the a particular variable and applies conversions and calibration factors.</p>"},{"location":"nmeg/removebaddata_primer/#radiation-corrections-l666-704","title":"Radiation corrections (L666-704)","text":"<ol> <li>Calibrate radiation measurements with <code>UNM_RBD_apply_radiation_calibration_factors.m</code></li> <li><code>UNM_RBD_calculate_net_radiation</code></li> <li><code>normalize_PAR_wrapper</code></li> </ol>"},{"location":"nmeg/removebaddata_primer/#burba-correction-l705-773","title":"Burba correction (L705-773)","text":"<p>This code applies the Burba 2008 correction for sensible heat conducted from a LiCor 7500 during flux measurements.</p>"},{"location":"nmeg/removebaddata_primer/#iteration-1-coarse-co2-flux-filters-and-ustar-l775-900","title":"Iteration 1 - Coarse CO2 flux filters and ustar (L775-900)","text":"<p>Sets up qc flags and removes bad data periods for:</p> <ol> <li>Precipitation</li> <li>Wind direction</li> <li>Night time negative fluxes</li> <li>Night time cold air drainage (PPine only)</li> <li>Bizarre periods at SLand and Grassland in 2007 and 2009</li> </ol> <p>Also makes a plot that can be used to decide the ustar threshold. This should then be entered in the site/year parameter block above (Check this!).</p>"},{"location":"nmeg/removebaddata_primer/#iteration-2-ustar-threshold-filter-l902-918","title":"Iteration 2 - Ustar threshold filter (L902-918)","text":"<p>Remove periods below ustar threshold above.</p>"},{"location":"nmeg/removebaddata_primer/#iteration-3-min-and-max-co2-flux-values-l920-973","title":"Iteration 3 - Min and Max CO2 flux values (L920-973)","text":"<ol> <li>Remove a variety of problems at particular sites and years with <code>remove_specific_problem_periods</code> function.</li> <li>Select some daily max and min CO2 values using <code>get_daily_maxmin</code> and the monthly CO2 thresholds specified above.</li> <li>Allow exceptions to this filter with <code>specify_siteyear_filter_exceptions</code> function.</li> <li>Remove and tally the max/min CO2 periods.</li> </ol>"},{"location":"nmeg/removebaddata_primer/#iteration-4-high-and-low-co2-concentration-filter-l975-1015","title":"Iteration 4 - High and low CO2 concentration filter (L975-1015)","text":"<ol> <li>Find high CO2 values ( &gt;450ppm )</li> <li>Specify some exceptions to this filter with <code>specify_siteyear_co2_conc_filter_exceptions</code></li> <li>Remove some site-specific low CO2 periods</li> </ol>"},{"location":"nmeg/removebaddata_primer/#iteration-5-apply-running-stddev-filter-l1017-1183","title":"Iteration 5 - Apply running StdDev filter (L1017-1183)","text":"<p>Appears to be a 1 day StdDev period</p>"},{"location":"nmeg/removebaddata_primer/#plot-the-entire-nee-series-with-filter-results-l1185-1126","title":"Plot the entire NEE series with filter results (L1185-1126)","text":"<p>Plot with `plot_NEE_with_QC_results'</p>"},{"location":"nmeg/removebaddata_primer/#filter-sensible-heat-l1127-1272","title":"Filter sensible heat (L1127-1272)","text":""},{"location":"nmeg/removebaddata_primer/#filter-other-variables-l1273-1381","title":"Filter other variables (L1273-1381)","text":"<ul> <li>Latent heat gets corrected then filtered</li> <li>Then filter temp (sonic), rH, h2o mean, pressure, soil heat fluxes.</li> <li>There are also some site/year specific data removals at the end of this section.</li> </ul>"},{"location":"nmeg/removebaddata_primer/#print-number-of-removals-l1382-1399","title":"Print number of removals (L1382-1399)","text":"<p>To screen, but should probably go into a log also.</p>"},{"location":"nmeg/removebaddata_primer/#prepare-and-write-for_gap_filling-file-l1400-1511","title":"Prepare and write <code>for_gap_filling</code> file (L1400-1511)","text":"<p>There is some funky data removal at the end of this section also.</p>"},{"location":"nmeg/removebaddata_primer/#prepare-and-write-_qc-file-l1512-1629","title":"Prepare and write <code>_qc</code> file (L1512-1629)","text":""},{"location":"nmeg/removebaddata_primer/#mysterious-outfile-preparation-l1631-1747","title":"Mysterious outfile preparation (L1631-1747)","text":"<p>Seems to tally removed data, but cannot tell if it actually is output at some point.</p>"},{"location":"nmeg/removebaddata_primer/#functions-l1749-","title":"Functions (L1749 -&gt;)","text":"<ol> <li><code>normalize_PPine_NEE</code></li> <li><code>get_daily_maxmin</code></li> <li><code>remove_specific_problem_periods</code> (this is huge L1799-2059)</li> <li><code>specify_siteyear_filter_exceptions</code> (also large L2060-2422)</li> <li><code>co2_conc_filter_exceptions</code> L2423-2478</li> <li><code>format_SHF_labels</code> L2481:end</li> </ol>"},{"location":"nmeg/soilsensor_history/","title":"Site soil sensor histories","text":"<p>Note that I am using the period data for CS616 output (in uSeconds) at sites where this is available.</p>"},{"location":"nmeg/soilsensor_history/#gland","title":"GLand","text":""},{"location":"nmeg/soilsensor_history/#2007-06-05-first-toa5","title":"2007-06-05 (First TOA5)","text":"<ul> <li>23 SWC (<code>cs616_wcr(x)</code> and 23 Tsoil (<code>soil_water_T(x)</code>) headers.</li> <li><code>soil_water_T(x)</code> fields seem to be some kind of artifact - the values are scaled right, but they seem to be logging wrong (usually they are all the same).</li> <li>There is a <code>Tsoil_Avg</code> column in TOA5 data that appears to represent surface soil temperature.</li> <li>There may be some confusion between SWC_O3_37.5 and SWC_O3_52.5 sensors - check.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2008-08-21","title":"2008-08-21","text":"<ul> <li>1 CS616 fixed</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-02-20","title":"2009-02-20","text":"<ul> <li>Program change</li> <li>Stopped reading the 1 extra CS616</li> <li><code>cs616</code> headers changed to <code>(x,y)</code> format</li> <li>Added HFP and TCAV measurements</li> <li>Still no Tsoil measurements except <code>SoilT_Avg</code></li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-07-15","title":"2009-07-15","text":"<ul> <li>20 thermocouples were installed  at the site</li> <li>Headers are <code>mux25t_signal_Avg(x)</code></li> <li>The depths are approximately the same as the CS616s, except that there are no sensors at G3 37.5 and 52.5</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-08","title":"2009-08","text":"<ul> <li>Fire at the site. CS616 and TCs went to NAN</li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-2-24","title":"2010-2-24","text":"<ul> <li>Thermocouple multiplexer turned on and TC measurements appear to be working again shortly after this.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-05-12","title":"2010-05-12","text":"<ul> <li>20 Thermocouples and 18 new echo EC5 sensors installed in 4 pits.</li> <li>Thermocouples buried at all 5 depths in each pit, but there are no 52.5cm echo probes in the 2 Open pits.</li> <li>Note that the TC headers did not change from <code>mux25t_signal_Avg(x)</code>, which is problematic.</li> <li>There are also 4 \"dummy\" columns, which don't seem to be working.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2011-3-10","title":"2011-3-10","text":"<ul> <li>Echo probes rewired so that Grass1 and Open 1 have all 5 depths and Open2 and Grass2 have only 2.5 to 37.5cm depths.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2011-3-23","title":"2011-3-23","text":"<ul> <li>20 CS616s were installed in the 4 pits alongside the Echo probes and TCs, but they are not read until June.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2011-06-09-to-2011-07-27","title":"2011-06-09 to 2011-07-27","text":"<ul> <li>New programs to read the CS616 probes, and there was some downtime during this changeover.</li> <li>Echo probes in Open pits were not read and headers disappeared after this change.</li> <li>CS616s were briefly read into <code>mux_soil_wcr(1-20)</code> headers (3 days), then into <code>swc_open1_depthincm_Avg</code>.</li> <li>Both of these headers persisted after July, but only the <code>swc_...</code> have data.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2012-08-09","title":"2012-08-09","text":"<ul> <li>Program change that reduced soil measurement cycling from 5min to 30min frequency.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2014-01-16","title":"2014-01-16","text":"<ul> <li>New program with new headers: <code>SWC_G1_2p5_Avg</code>, <code>SWC_echo_G1_2p5_Avg</code>, <code>SoilT_G1_2p5_Avg</code></li> </ul>"},{"location":"nmeg/soilsensor_history/#2014-01-17-and-2014-02-17-2-toa5-files","title":"2014-01-17 and 2014-02-17 (2 TOA5 files)","text":"<ul> <li>Tsoil headers are messed up. There are no O3 and G3 pits and the headers are mixed up with the current ones, so these need to be renamed</li> </ul>"},{"location":"nmeg/soilsensor_history/#jsav","title":"JSav","text":""},{"location":"nmeg/soilsensor_history/#2007-05-04-first-toa5","title":"2007-05-04 (First TOA5)","text":"<ul> <li>No SWC or SoilT data.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2007-08-31","title":"2007-08-31","text":"<ul> <li>New program - SWC and Tsoil headers present<ul> <li>20 Tsoil sensors logging under <code>soilT_Avg(x)</code> (but there are 28 headers), VWC logging under <code>Period(X)</code> and <code>VWC(X)</code>, but both are NANs</li> </ul> </li> <li>There was a gap in the data between 8/6 and 8/31</li> </ul>"},{"location":"nmeg/soilsensor_history/#2007-09-24","title":"2007-09-24","text":"<ul> <li>Now the SWC sensors are working (logging to <code>VWC(X)</code>).</li> </ul>"},{"location":"nmeg/soilsensor_history/#2008-02-25","title":"2008-02-25","text":"<ul> <li>SWC headers changed to <code>cs616_wcr(x)</code>, but no data logged yet.</li> <li>There are also <code>soil_water_T(1)</code> headers, but these also have no data, and may just be an artifact.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2008-03-31","title":"2008-03-31","text":"<ul> <li>Data is now logging into <code>cs616_wcr(x)</code> and <code>soil_water_T(x)</code> fields - data look similar but are scaled differently (not sure what the <code>_T</code> field is)</li> <li>Actually I think, in this case the<code>_T</code> field is calculated VWC, while the <code>cs616_wcr(x)</code> field is the period.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2008-06-10","title":"2008-06-10","text":"<ul> <li>SHF plates installed</li> </ul>"},{"location":"nmeg/soilsensor_history/#2008-12-18-to-2009-02-02","title":"2008-12-18 to 2009-02-02","text":"<ul> <li>Soil sensors down for a while here</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-07-08","title":"2009-07-08","text":"<ul> <li>There has been a changeover in soil data and probe locations</li> <li>Tsoil headers changed to <code>SoilT_J1_2.5_Avg</code></li> <li>Unfortunately the period headers <code>cs616_wcr(x)</code>, stayed the same, but now refer to different locations. We'll have to change the old ones to <code>_old</code> in the code</li> <li>The calculated VWC headers changed to:<code>SWC_J1_2.5</code></li> </ul>"},{"location":"nmeg/soilsensor_history/#summer-2011","title":"Summer 2011","text":"<ul> <li>Intermittent problems with SWC sensors (recording NAN).</li> </ul>"},{"location":"nmeg/soilsensor_history/#2011-08-11","title":"2011-08-11","text":"<ul> <li>MUX replaced and seems to have fixed problem.</li> </ul>"},{"location":"nmeg/soilsensor_history/#spring-2012","title":"Spring 2012","text":"<ul> <li>More intermittent CS616 problems</li> </ul>"},{"location":"nmeg/soilsensor_history/#2012-04-27","title":"2012-04-27","text":"<ul> <li>CS616s removed from CR5000 and placed on a CR1000 in the same box.<ul> <li><code>swc_j1_2.5_avg</code> headers are calculated VWC,  <code>PA_uS_Avg(1-18)</code> are the cs616 periods</li> <li>These data come in on separate TOA5 files that are in the <code>soil</code> directory.</li> </ul> </li> <li>Headers disappear from CR5k TOA5 files at this point</li> </ul>"},{"location":"nmeg/soilsensor_history/#2014-01-10","title":"2014-01-10","text":"<ul> <li>Header changes for CR5k<ul> <li><code>sap_Avg(x)</code> to <code>sap_J1a_Avg</code></li> <li><code>open_north_Avg</code> to <code>shf_open_north_Avg</code></li> <li>Added soil CO2 measurements</li> </ul> </li> </ul>"},{"location":"nmeg/soilsensor_history/#2014-01-17","title":"2014-01-17","text":"<ul> <li>Header changes affecting the CR1000 soil moisture datalogger<ul> <li>cs616 headers changed to <code>SWC_O1_2p5</code> format</li> </ul> </li> </ul>"},{"location":"nmeg/soilsensor_history/#2014-02-25-to-2014-02-27","title":"2014-02-25 to 2014-02-27","text":"<ul> <li>Reinstalled CS616s in new pits (see new headers in format <code>SWC_J1_2p5_Avg</code>, <code>SoilT_J1_2.5_Avg</code>)</li> <li>All CS616s are on the CR5000 again</li> <li>Soil heat flux plates reinstalled in 4 (?) pits (3 pits in data headers).</li> <li>TCAVs added in similar pits as well.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2014-03-28","title":"2014-03-28","text":"<ul> <li>Dug up and reinstalled CS616 and Tsoil at 5, 10, 30 cm. Headers changed on 4/15 (possibly earlier)</li> </ul>"},{"location":"nmeg/soilsensor_history/#sland","title":"SLand","text":""},{"location":"nmeg/soilsensor_history/#2007-05-30-first-toa5","title":"2007-05-30 (first TOA5)","text":"<ul> <li>21 <code>soil_water_T(1)</code> and 21 <code>cs616_wcr(x)</code> headers, but they don't seem to be functioning correctly.</li> <li>Not sure what depth/location that 21st SWC and Tsoil reading is from</li> </ul>"},{"location":"nmeg/soilsensor_history/#2007-07-05","title":"2007-07-05","text":"<ul> <li>cs616's are functioning, but sensors 12 and 13 are all NANs</li> <li><code>soil_water_T</code> sensors are not giving reasonable data and all sensors are identical.</li> <li>There is a <code>Tsoil_avg</code>column that looks correct.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-02-20_1","title":"2009-02-20","text":"<ul> <li>New program: 22 SWC sensor columns with <code>soil_water_T_Avg(1)</code> and <code>cs616_wcr_Avg(x)</code> headers</li> <li>Now there are 6 heat flux plates</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-03-31","title":"2009-03-31","text":"<ul> <li>At this point some rewiring was done on the AM16/32 Mux where the CS 616s are connected</li> <li>It seems that there were a couple problems:<ul> <li>There are only 20 probes, but 22 are being output by the datalogger program</li> <li>Pit O2 sensors are wired out of order at the MUX, so they have been logged in the wrong headers</li> <li>Not sure what to do about this yet</li> </ul> </li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-06-24","title":"2009-06-24","text":"<ul> <li>20 thermocouples wired in to AM25t mux - reading into <code>mux25t_signal_Avg(1)</code> headers</li> <li>Old thermocouple headers and data still present, but still garbage.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2012-08-09_1","title":"2012-08-09","text":"<p>Program changes:  * Tsoil now read into <code>soilt_S2_52.5_Avg</code> headers. * CS616s now read every 30 minutes.</p>"},{"location":"nmeg/soilsensor_history/#new-gland","title":"New GLand","text":""},{"location":"nmeg/soilsensor_history/#2009-08-06-first-toa5-file","title":"2009-08-06 (first TOA5 file)","text":"<ul> <li>There are 20 CS616s and 20 Tsoil sensors with headers: <code>cs616_wcr(20)</code>,  <code>SWC_g1_2.5</code> (calculated VWC - not used), and <code>SoilT_g1_2.5_Avg</code></li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-02-24","title":"2010-02-24","text":"<ul> <li>Header changes for SWC to <code>cs616_wcr(1,1)</code>, <code>SWC_</code> (calculated values) headers disappeared.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-03-12","title":"2010-03-12","text":"<ul> <li>10 of the CS616s stopped being logged - Only Grass1 and Open1 probes are now being measured</li> <li>Order of measurements changed to G1 O1 G2 O2 for Tsoil</li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-03-26","title":"2010-03-26","text":"<ul> <li>New CS616s were added and wired in the G1 O1 G2 O2 order, but it doesn't look like they are being logged yet.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-05-05","title":"2010-05-05","text":"<ul> <li>Wiring continues to change, but it doesn't seem to affect the TOA5 files.</li> <li>Supposedly TC order is G1, O2, O1 G2</li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-05-12_1","title":"2010-05-12","text":"<ul> <li>Further rewiring: Supposedly CS616 and TC order is now G1, G2, O1, O2.</li> <li>Headers haven't changed</li> </ul>"},{"location":"nmeg/soilsensor_history/#2010-05-28","title":"2010-05-28","text":"<ul> <li>New program that should now measure new CS616s and thermocouples in the order they are actually wired (see above).</li> <li>It may be worth swapping headers in the processing code to account for mistaken probe identity over the last 2 months.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2011-06-23","title":"2011-06-23","text":"<ul> <li>New program for new TCAV added (installed on 5-24 - in open pit)</li> </ul>"},{"location":"nmeg/soilsensor_history/#2014-01-17_1","title":"2014-01-17","text":"<ul> <li>Some header changes (fairly minor)</li> </ul>"},{"location":"nmeg/soilsensor_history/#pj","title":"PJ","text":"<p>PJ is different - soil, sapflow, and CO2 data are logged by a separate CR23x at the site. Data from this logger is periodically downloaded and saved.</p>"},{"location":"nmeg/soilsensor_history/#2007-11-09-first-toa5","title":"2007-11-09 (first TOA5)","text":"<ul> <li>No soil water or temperature measurements yet.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2008-06-19","title":"2008-06-19","text":"<ul> <li>New headers for Echo probes and Soil T arrive, but values are invalid.</li> <li>24 Echo probes: <code>echo_wcr(x,y)</code></li> <li>25 Tsoil probes: <code>soilT_Avg(x)</code></li> </ul>"},{"location":"nmeg/soilsensor_history/#2008-08-12","title":"2008-08-12","text":"<p>Values finally valid for Echo probes and Tsoil on main logger (fixed by someone - in logbook).</p> <ul> <li>I still don't know anything about the pit structure/depths for these SWC or Tsoil probes</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-03-22","title":"2009-03-22","text":"<p>Thermocouples installed, not logging yet</p>"},{"location":"nmeg/soilsensor_history/#2009-05-12","title":"2009-05-12","text":"<ul> <li>TDR probes installed and connected to CR23X</li> <li>Soil CO2 probes wired in</li> <li>Heatflux plate locations noted.</li> <li>Soil measurements on the cr23x begin logging at this date. Data for 2009-2013 are in cr23x compilation files originally assembled by Laura Morillas. Find these in the \"yearly_cr23x_compilations\" direcory</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-06-17","title":"2009-06-17","text":"<p>Echo, Tsoil headers removed from main datalogger (data was junk for the past while...)</p>"},{"location":"nmeg/soilsensor_history/#2013-07-12","title":"2013-07-12","text":"<p>IRT sensors added to main datalogger</p>"},{"location":"nmeg/soilsensor_history/#2014-01-01","title":"2014-01-01","text":"<p>cr23x data now comes in as monthly files in the \"cr23x_files\" directory</p>"},{"location":"nmeg/soilsensor_history/#2014-01-10_1","title":"2014-01-10","text":"<p>New program to main datalogger includes some header changes (but no soil sensors)</p>"},{"location":"nmeg/soilsensor_history/#2014-03-21","title":"2014-03-21","text":"<p>Sent new program to main datalogger</p>"},{"location":"nmeg/soilsensor_history/#2014-03-26","title":"2014-03-26","text":"<p>Sent new program with added IRT measurements</p>"},{"location":"nmeg/soilsensor_history/#pj_girdle","title":"PJ_girdle","text":"<p>PJ_girdle also has soil, sapflow, and CO2 data are logged by a separate CR23x at the site. Data from this logger is periodically downloaded and saved.</p>"},{"location":"nmeg/soilsensor_history/#2009-02-23-first-toa5","title":"2009-02-23 (first TOA5)","text":"<ul> <li>No soil water or temperature measurements on main datalogger.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-04-28","title":"2009-04-28","text":"<ul> <li>TDR probes installed and connected to CR23X</li> <li>Soil CO2 probes wired in</li> <li>Heatflux plate locations noted.</li> <li>Soil measurements on the cr23x begin logging at this date. Data for 2009-2013 are in cr23x compilation files originally assembled by Laura Morillas. Find these in the \"yearly_cr23x_compilations\" directory</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-05-28-to-2009-06-09","title":"2009-05-28 to 2009-06-09","text":"<p>Problems with cr23x data - many sensors not reading.</p>"},{"location":"nmeg/soilsensor_history/#2013-07-03","title":"2013-07-03","text":"<p>NDVI and IRT sensors added to main datalogger</p>"},{"location":"nmeg/soilsensor_history/#2014-01-01_1","title":"2014-01-01","text":"<p>cr23x data now comes in as monthly files in the \"cr23x_files\" directory</p>"},{"location":"nmeg/soilsensor_history/#2014-01-10_2","title":"2014-01-10","text":"<p>New program to main datalogger includes some header changes (T, RH, flux vars, but no soil sensors)</p>"},{"location":"nmeg/soilsensor_history/#2014-03-26_1","title":"2014-03-26","text":"<p>Sent new program to main datalogger adds a few measurements (Wood psychrometers?)</p>"},{"location":"nmeg/soilsensor_history/#ppine","title":"PPine","text":"<p>PPine soil data were logged by....</p>"},{"location":"nmeg/soilsensor_history/#2006-08-30-first-toa5","title":"2006-08-30 (first TOA5)","text":"<ul> <li>No soil water or temperature measurements yet.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2006-11-18","title":"2006-11-18","text":"<ul> <li>4 CS107 thermistors were added - presumably these are measuring the soil?</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-05-22","title":"2009-05-22","text":"<p>3 new soil heat flux measurements added (<code>shf_Avg(1-3)</code>)</p>"},{"location":"nmeg/soilsensor_history/#2010-10-07","title":"2010-10-07","text":"<ul> <li>TCAV and Hukesflux shf installed</li> </ul>"},{"location":"nmeg/soilsensor_history/#2013-12-28","title":"2013-12-28","text":"<p>New soil data headers appear and major new header changes begin</p>"},{"location":"nmeg/soilsensor_history/#2014-01-03","title":"2014-01-03","text":"<p>Some minor rearranging of sensor order in output</p>"},{"location":"nmeg/soilsensor_history/#2014-01-09","title":"2014-01-09","text":"<p>SWC and Tsoil (107) measurements disappear</p>"},{"location":"nmeg/soilsensor_history/#2014-05-23","title":"2014-05-23","text":"<p>SWC and Tsoil (107) measurements reappear in a slightly different order</p>"},{"location":"nmeg/soilsensor_history/#mcon","title":"MCon","text":"<p>MCon soil data were logged by....</p>"},{"location":"nmeg/soilsensor_history/#2006-08-31-first-toa5","title":"2006-08-31 (first TOA5)","text":"<ul> <li>No soil water or temperature measurements yet.</li> </ul>"},{"location":"nmeg/soilsensor_history/#2006-12-04","title":"2006-12-04","text":"<ul> <li>4 CS107 thermistors were added - presumably these are measuring the soil?</li> </ul>"},{"location":"nmeg/soilsensor_history/#2009-06-01","title":"2009-06-01","text":"<p>3 new soil heat flux measurements added (<code>shf_Avg(1-3)</code>)</p>"},{"location":"nmeg/soilsensor_history/#2010-06-18","title":"2010-06-18","text":"<ul> <li>TCAV and echo probe installed (<code>echo_h2o_Avg</code> &amp; <code>TCAV_Avg</code>)</li> </ul>"},{"location":"nmeg/soilsensor_history/#2013-11-12","title":"2013-11-12","text":"<p>Site rebuilt after the fire. Now logging Tsoil, SWC,  soil CO2 and new soil heat flux plates/TCAVs</p>"},{"location":"nmeg/soilsensor_history/#2014","title":"2014","text":"<p>Some minor rearranging of sensor order in output, and minor header changes</p>"},{"location":"procedures/14c_graphitization/","title":"14c graphitization","text":"<p>From Janet Hurley - last modified 090610</p>"},{"location":"procedures/14c_graphitization/#preparation-procedures-for-14c-analysis-of-cocaine","title":"Preparation procedures for 14C analysis of Cocaine","text":""},{"location":"procedures/14c_graphitization/#note-recent-changes-to-procedure","title":"NOTE - recent changes to procedure","text":"<ul> <li>Janet B. has slightly changed placement of the Zn/Ti tube and the small Fe tube that fit within the reaction tube.</li> <li>Also, there is a three strikes target pounding procedure.</li> </ul>"},{"location":"procedures/14c_graphitization/#precombustion-procedures","title":"Precombustion procedures","text":"<ul> <li>Cut 6 mm quartz tubes at 8 inches long (200 mm). To cut, score tube at 8 inches and snap along score.</li> <li>Turn on the torch by turning on gas supply at wall valve and oxygen supply at tank regulator. Be sure the torch is pointed away from you and any combustible equipment or materials. Turn the red knob on the torch for the gas feed and ignite with spark. Once the gas flame is steady and about 5 \u00bd inches in length, delicately turn the green knob to introduce oxygen. Adjust until there is a bright blue triangle at the base of the flame. The more oxygen, the smaller and hotter the triangle. Adjust to get a 7 inch flame with at least \u00bc inch bright blue.</li> <li>Holding a quartz tube at an angle and wearing #6 protective eyeshades (to prevent retinal burning) and leather gloves (to prevent finger burns), seal one end of each quartz tube while continually rotating the tube in the flame of the torch. Rotate slowly and hold just above the small blue triangle in the flame. Be sure that the hole is sealed and there is a solid glass bottom to the tube.</li> <li>Place hot sealed tubes upside down in a beaker to cool.</li> <li>Once the tubes are cooled, fire polish the open end of each in the flame.</li> <li>Place hot sealed tubes in a beaker to cool.</li> <li>Turn off the torch by turning the green knob to stop oxygen flow, then the red knob to stop gas flow. Turn of the gas supply at the wall valve and the oxygen supply at the tank regulator.</li> <li>Dry the tubes by placing in a muffle furnace at 500\u00baC for about a half hour. Allow to cool slowly and completely (a few hours or overnight). Cover with aluminum foil until ready to preload.</li> <li>Preload 200 mg CuO (about 2 spatulas full) into tubes using precut plastic funnel.</li> <li>Precombust quartz tubes loaded with CuO at 900\u00baC for 2 hours. Place tubes in a steel basket or other high-temperature container for precombustion. Do not place them in Pyrex beakers, which will begin to melt at about 800\u00baC, or aluminum baskets or racks, which will melt at around 650\u00baC.</li> <li>Wear gloves to prevent contaminating precombusted tubes.</li> <li>To minimize static problems, store precombusted quartz tubes in batches wrapped in aluminum foil in bubble desiccator with water in bottom.</li> </ul>"},{"location":"procedures/14c_graphitization/#loading-combustion-tubes","title":"Loading combustion tubes","text":"<ul> <li>Wear gloves for loading combustion tubes to prevent contamination.</li> <li>Clean all surfaces and tools with a methanol wipe. Clear tools of any dust particles after methanol wiping with an aerosol duster.</li> <li>Standards are loaded for combustion at 0.8 to 1.2 mg carbon. Standards should be kept in individual zip lock bags with designated spatulas or curettes. Spatulas and curettes should not be exchanged between standards and unknowns and should be cleaned with methanol and cleared with aerosol duster prior to each use. Samples are also loaded for combustion at 0.8 to 1.2 mg carbon \u2013 for cocaine HCl this is about 1.7 mg. Load into 3 x 5 mm silver capsules (precombusted at 900\u00baC for 1 hour). </li> <li>Record weight and crimp loaded silver capsule slightly so that it will drop into the precombusted 6 mm quartz tube.</li> <li>Label precombusted quartz tubes with high temperature marker.</li> <li>Once tubes are loaded, fill the large dewar flask with liquid nitrogen and freeze the trap between the pump and vacuum line.</li> <li>Secure tubes to vacuum line and open valves on each tube. Once vacuum gauge indicates all tubes are at baseline pressure, they can be sealed. </li> <li>Wear #6 protective eyeshades and seal each tube in turn with a torch. Light the torch as described above. At about 1 to 1 \u00bd inch below the Ultra Torr fitting, hold the flame so that the blue triangle almost touches the quartz. Rotate the flame around the tube until the quartz becomes soft enough to turn. Twist the tube and pull down until the two pieces separate. Melt the tips of the tube and the vacuum stub into smooth blobs to seal each, preventing tip leaks and breakages. Set sealed tubes on the Hardibacker board to cool.</li> <li>Turn off the torch as described above.</li> <li>Place tubes into protective steel sleeves and into the muffle furnace. Place tubes in systematic order and record sample names on ordered worksheet in case tube labeling is baked off.</li> <li>Combust at 900\u00baC for 2 hours and allow to cool overnight.</li> <li>Once combustion is complete and the tubes have cooled, remove from steel sleeves and retrace labels with a Sharpie marker. Wear gloves to handle combusted sample tubes, and wipe steel dust off of tubes with Kimwipes.</li> <li>If not performing gas transfer right away, store combusted samples in a beaker in a clean cabinet for up to one week.</li> </ul>"},{"location":"procedures/14c_graphitization/#preparation-of-reaction-tubes","title":"Preparation of reaction tubes","text":"<ul> <li>Cut 9 mm Pyrex tubing at 6-inch lengths. To cut, score tube at 6 inches and snap along score.</li> <li>Seal the end of each tube with the torch. Note that the torch cannot be as hot as with quartz tubing. Use the glass pulling technique to form an evenly sealed bottom.</li> <li>Fire polish the open end of each tube with the torch.</li> <li>Burn a small dimple into the side of each tube at \u00be inch (2 cm) from the base of the tube.</li> <li>Precombust the 9 mm Pyrex tubes and the 6 mm Kimble culture tubes at 500\u00baC for 3 hours and 550\u00baC for 4 hours.</li> <li>Store precombusted Pyrex tubing wrapped in aluminum foil in a bubble desiccator with dry desiccant. Always handle precombusted reaction tubes with gloves to prevent contaminating them.</li> </ul>"},{"location":"procedures/14c_graphitization/#loading-reaction-tubes","title":"Loading reaction tubes","text":"<ul> <li>Wear gloves while handling reaction tubes and reagents to prevent contamination.</li> <li>Place a white sheet of paper under the glass work surface. Clean all surfaces and tools with methanol wipe. Clear tools after methanol wipe with aerosol duster.</li> <li>With the designated curette, load 30 mg of zinc powder into the 9 mm Pyrex tube (1 rounded scoop). Try not to get Zn on the sides of the tube. Knock any powder to the bottom of the tube by tapping the tube on the work surface. Do this for as many tubes as the number of samples to be prepared, plus a few extra.</li> <li>With the curette designated for titanium hydride, add 10 to 15 mg of TiH2 to each of the 9 mm tubes (2 packed scoops). Avoid getting powder on the sides of the tube by tapping it on the work surface to knock any TiH2 to the bottom.</li> <li>Now, using the small curette designated for iron, load 5 mg (exactly) of Fe powder into the 6 mm Kimax tubes (1 rounded scoop).</li> <li>Place an iron-loaded 6 mm tube into each preloaded 9 mm tube, by tilting the 9 mm tube and allowing the 6 mm tube to slide gently to rest on the dimple.</li> <li>Bake the tubes and loaded reagents at 300\u00baC for 1 hour.</li> <li>Cover the preloaded reaction tubes with foil to prevent contamination.</li> <li>Store preloaded tubes under vacuum in bubble dessicator for up to 2 weeks before use. If not used before two weeks, tubes may be prebaked again at 300\u00baC for 1 hour and used.</li> </ul>"},{"location":"procedures/14c_graphitization/#co2-gas-transfer","title":"CO2 gas transfer","text":"<ul> <li>Fill two 4 liter dewars with liquid nitrogen.</li> <li>Wear leather work gloves to protect hands and fingers from freezes and burns throughout the process. Wear safety glasses as well.</li> <li>Freeze the trap between the pump and vacuum line by filling the large dewar flask with liquid nitrogen. This trap prevents oil from entering the vacuum line and water from entering the pump. The dewar may require topping-off with liquid nitrogen throughout the gas transfer procedure.</li> <li>Once the trap is frozen, open the entire vacuum line to the pump. Check the line for any leaks by monitoring the gauge, which should show a baseline reading after a short time.</li> <li>Prepare an ethanol and dry ice slush and freeze the water trap. The water trap will remain frozen throughout the extraction procedure and will require refreshening with dry ice every few hours.</li> <li>Isolate the tube cracker and clean the tube cracker interface. Add grease if necessary.</li> <li>Score a quartz sample tube at the appropriate height and place it in the tube cracker apparatus.</li> <li>Open the tube cracker to the vacuum.</li> <li>Isolate the reaction tube section of the line, remove Pyrex stub, and replace with preloaded and labeled reaction tube.</li> <li>Open the newly placed reaction tube to the vacuum slowly so that reagents are not mobilized and mixed.</li> <li>Open all valves to the pump and wait for gauge to return to baseline.</li> <li>Close the valve from the tube cracker to the pump. Once vacuum gauge reads baseline, close valves to isolate the water trap and CO2 trap.</li> <li>Freeze the bottom 3 inches of the CO2 trap with a dewar flask full of liquid nitrogen.</li> <li>Once the bottom of the CO2 trap is frozen, crack open the sample tube and open the valve to the water trap.</li> <li>Close the valve between the CO2 trap and the reaction tube.</li> <li>Open the valves from the water trap to the CO2 trap and allow the CO2 to freeze in the CO2 trap. </li> <li>Monitor N2 or other combusted sample gases on the analog pressure gauge. Open the valve from the CO2 trap to the vacuum pump to draw off these gases. Once the gauge has returned to baseline, isolate the CO2 trap by closing the valves on either side.</li> <li>Isolate the tube cracker and water trap by closing the valves on either side of the water trap.</li> <li>Open the valve between the reaction tube and the vacuum pump, and the cold finger and the reaction tube.</li> <li>Once the gauge shows a baseline reading, close the valve leading to the vacuum pump. </li> <li>Fill a mini-dewar with liquid nitrogen and place on the cold finger.</li> <li>Remove the liquid nitrogen from the CO2 trap and allow to warm. Use a water bath to speed the warming process. </li> <li>Open the valve between the CO2 trap and the reaction tube, allowing the CO2 sample to collect in the cold finger. Wait for the gauge to reach baseline. If baseline is not achieved, open valve to vacuum pump to draw off excess gasses.</li> <li>Isolate the cold finger by closing the valve between the cold finger and reaction tube. Also isolate the reaction tube at this time by closing the valve between the reaction tube and CO2 trap.</li> <li>Remove the liquid nitrogen from the cold finger and use a water bath to warm the finger.</li> <li>Reaction tubes are prepared to accept 1 milligram of carbon for graphitization. The cold finger gauge will have been calibrated (and labeled) to indicate a pressure reading that is equivalent to 1 mg C. If indicating about 1 mg C, record the pressure reading. If there is excess CO2 gas, some may need to be partitioned so that the reaction tube accepts only 0.8 to 1.2 mg C. Use Boyle\u2019s law, p1V1 = p2V2.</li> <li>Freeze the reaction tube with liquid nitrogen, then open the valve between the cold finger and reaction tube.</li> <li>Wearing safety glasses to protect from sodium flare and once the CO2 has frozen in the reaction tube, seal-off the reaction tube at about 11 cm with the torch. The torch should not be too hot. Prepare a 3 \u00bd-inch gas flame and then add oxygen to get a steady triangle. Do not subject the Pyrex to the hottest part of the flame. As one point on the tube softens, move the flame to another point until the tube is soft enough to turn. Twist the tube and pull to seal.</li> <li>Anneal the tube by turning off the oxygen and allowing the tube to cool slowly at the top of the gas flame until the top of the tube is black with carbon residue.</li> <li>Once cooled, be sure each sealed reaction tube is clearly labeled and place into a 20-place aluminum block.</li> <li>Repeat steps 6 through 30 until finished collecting CO2 samples for the set.</li> <li>Open all valves to the vacuum and allow gauges to return to baseline.</li> <li>Remove the ethanol slurry from the water trap and secure the dewar flask cover over the slurry mixture. The mixture may remain cold enough for use the next day.</li> <li>Remove dewar flasks containing liquid nitrogen and replace any remaining into one of the 4-liter storage dewars.</li> <li>Close the main valve (large red valve) between the pump and the vacuum line.</li> </ul>"},{"location":"procedures/14c_graphitization/#graphitization","title":"Graphitization","text":"<ul> <li>Once 2 aluminum blocks are stocked with 40 sealed reaction tubes, complete a graphitization worksheet listing sample names for each position in each block. Each set of 40 samples must contain at least one 14C-dead blank such as acetanilide, six primary standards (OX2), and three secondary standards (2 ANU-sucrose, 1 IAEA C3).</li> <li>Place loaded aluminum blocks into the muffle furnace and bake at 500\u00baC for 3 hours, then 550\u00baC for 4 hours.</li> <li>Allow reaction tubes to cool overnight.</li> </ul>"},{"location":"procedures/14c_graphitization/#preparation-of-aluminum-cathodes","title":"Preparation of aluminum cathodes","text":"<ul> <li>Clean undrilled aluminum cathodes with acetone in a beaker by sonication for 30 min. Decant the acetone and repeat the process until the decanted acetone is clear.</li> <li>Spread the cathodes in a tray lined with a Durx cleanroom wiper towel, and let them dry in the laminar flow hood.</li> <li>Wear safety glasses while drilling cathodes.</li> <li>Check that the drill press is set to the correct depth (0.160\u201d). The drill press has an electronic depth reading mechanism that is very sensitive to o-ring compression on the face cover. Tightening or loosening the screws may be necessary if the depth reader is not reading 0.160\u201d at fully pulled handle position. If the reading is unstable and adjusting the face cover does not resolve it, the battery made need to be replaced.</li> <li>Place a clean cathode funnel-side up under the drill bit, and secure the cathode with spring arm.</li> <li>Test positioning of the stage by pulling down on the arm. The drill bit should come to the center of the cathode funnel. If not properly centered, manipulate the stage to center the drill bit on the funnel.</li> <li>Once properly centered, turn on the drill press and slowly pull the arm down until the depth reading is 0.156 to 0.158\u201d (we have found that this gives us a depth of 0.160\u201d), release the arm, and turn off the drill press.</li> <li>Remove the cathode from the drill press and tap it upside-down on the clean work surface to expel aluminum ribbons, then use aerosol duster to remove any residual aluminum from the hole.</li> <li>Remove aluminum ribbons from the drill bit and stage before proceeding to drill additional cathodes.</li> <li>Inspect drilled cathodes under a dissection microscope to be sure that the holes are centered in the funnel.</li> <li>Clean the work area after desired amount of cathodes are drilled.</li> <li>Store clean cathodes in a clean plastic container with tight fitting lid.</li> </ul>"},{"location":"procedures/14c_graphitization/#pressing-graphite-into-cathodes","title":"Pressing graphite into cathodes","text":"<ul> <li>We submit pressed graphite targets in sets of 40 to the Keck AMS lab at UC-Irvine. Pressed targets will be placed into 96-well plates and can therefore contain up to 2 sets of 40 samples per plate. As noted above, each set must contain 1 blank, 6 primary standards, and 3 secondary standards. Before pressing, prepare sample worksheets showing placement, cathode number, target number, sample ID, estimated mg carbon, d13C values and any relevant comments for each target.</li> <li>The order of pressing for each sample set is as follows: Blanks, OX2, ANU-Sucrose, C3, and unknowns. Clean in between each sample type, and change gloves as they become contaminated with graphite material. NOTE: while pressing multiple targets of the same material, it is only necessary to dust the pin head with the aerosol duster in between targets.</li> <li>Turn on the laminar flow hood and clean all surfaces with methanol and Durx cleanroom wipes.</li> <li>To clean the portable press pin, sand the pin head in a circular motion on 600 grit silicon carbide sandpaper, then sand the sides of the pin by folding a small piece of the sandpaper and rotating the pin against the paper. Finally, wipe the pin and pin holder with methanol and dust with aerosol duster. Notice that the pin will need replacing after every few sets of 40 samples are hammered.</li> <li>Clean the cathode holder, sleeve, and magnetic forceps with methanol and the aerosol duster.</li> <li>Check the target number and write it on a drilled cathode using an ultra fine Sharpie marker.</li> <li>Place the labeled cathode into the cathode holder.</li> <li>Score the 9 mm reaction tube about 2.5 cm above the dimple. Hold the tube horizontally, gently scoot the Kimax tube above your score line and break the 9 mm tube to access the 6 mm tube containing the graphite-Fe powder. Discard the bottom of the tube containing the zinc and TiH2 into the plastic waste container under the flow hood. Discard the empty top of the tube in a large beaker containing glass waste.</li> <li>Carefully wipe the outsides of the Kimax tube with a clean Kimwipe.</li> <li>Knock the graphite-Fe powder from the 6 mm tube into the funneled cathode. </li> <li>Using the portable pin, gently crush the graphite into a powder (if necessary), and work it into the hole.</li> <li>Once the graphite powder has been worked into the hole, slide the pin sleeve over the cathode and insert the pin through the top to fit into the drilled cathode hole. </li> <li>Place the assembly on the lab bench and give it 3 firm strikes with the hammer.</li> <li>Disassemble the cathode holder, remove the cathode and tap it upside down firmly against the top surface of the cathode holder to eliminate any residual graphite from the cathode funnel.</li> <li>If a flat shiny surface is not apparent, inspect the graphite target under a dissection microscope. Each graphite target should be flat and shiny, and contain no loose powder. Repeat pressing if necessary. Excessive loose powder can be picked up with clean magnetic forceps and returned to the cathode hole.</li> <li>Once a flat and shiny surface is achieved, place the cathode into the appropriate position in the 96-well plate.</li> <li>Before moving on to the next sample, clean the cathode holder, the sleeve, forceps, and the press pin as above.</li> <li>Clean and put away all equipment when finished pressing and turn off the laminar flow hood.</li> </ul>"},{"location":"procedures/belowsnow_soilresp/","title":"Below-snow soil respiration","text":"<p>Wintertime soil CO~2~ fluxes are measured below a snowpack using a diffusional measuremnt. Gas from the soil-snow interface is collected through a inlet and tubing system, tranferred into evacuated vials (Labco Exetainers), and measured for CO~2~ concentrations in the lab using a Li-Cor 7000 IRGA. Fluxes are then calculated using Fick's law in conjunction with atmospheric CO~2~ measurements and measured snowpack characteristics (density, depth, tortuosity).</p> <p>In use at:</p> <ul> <li>Hidden Canyon</li> </ul>"},{"location":"procedures/belowsnow_soilresp/#gas-sampling-inlets-and-installation","title":"Gas sampling inlets and installation","text":"<p>Gas from the soil-snow interface is collected in winter and spring using an inlet and tubing system. 10cm diameter cylindrical inlets covered on both ends with fine (50\u03bcm) mesh are placed on the soil surface in fall. Each inlet is tubed to a central location where measurement activities will not disturb the CO~2~ diffusion gradient near the inlet. Tubing is 1/4 inch Eaton Synflex\u2122 tubing. Tubing terminates with a sampling port fitted with a Swagelok stainless coupling and plug. THe coupling can be connected to a complementary fitting on the pump/sampling assembly. Inlets are located in what are intended to be spatially representative replications in each plot. Each soil temperature/moisture profile currently is associated with 3 inlets. There are thus 9 inlets each in the control and treatment plots in the Hidden Canyon snowpack manipulation.</p>"},{"location":"procedures/belowsnow_soilresp/#pump-time-calculation","title":"Pump time calculation","text":"<p>Once inlets are installed and ready to be measured, the volume of air between inlet and the sampling port must be known. This volume must be pumped out using an electric pump prior to each measurement. After pumping away this volume, the below-snow air is then sampled with the pump/sampling assembly. To calculate this volume and the time needed to pump it away follow this procedure:</p> <ul> <li>Measure the length of the tubing (<code>l</code>) between the inlet and sampling port in centimeters (to the nearest 50cm is ok).</li> <li>The tubing has an inner diameter of 1/4 inch, which is 0.635cm (this is <code>r</code>).</li> <li>The volume of this tubing (<code>V</code>)is calculated as: $\u03c0r^2^ * l$</li> <li>The pump claims to pump at 4000ml per minute, but we will reduce this a bit with the pin valve on a flowmeter</li> <li>Calculate the time needed to pump out air in the tubing as <code>V/flowrate</code>, so if we set the valve to allow 500ml per minute the time needed to pump out the stagnant air in the tubing is <code>V</code>/500.</li> <li>Have these pump times for each inlet calculated before heading into the field to sample.</li> </ul>"},{"location":"procedures/belowsnow_soilresp/#pumpsampling-assembly","title":"Pump/sampling assembly","text":"<p>A pump/sampling assembly is connected to the 1/4 inch Swage fitting at the end of each sampling tube and is used to draw sample air to a sampling port. This assembly consists of a small diaphragm pump (KNF UNMP50), a Swagelok 1/4 inch gastight valve, a sampling port with septa, a flowmeter/pinvalve assembly, and a filter. The pump is powered with a LiCor rechargeable battery. The tubing at the end of the filter is attached to the sampling tube, and the pump can then be used to draw air samples from the inlet at a rate monitored and controlled at the flowmeter/pinvalve. Once a sufficient volume of air has been drawn through the sample tube to bring inlet air into the pump/sampling assembly, the valve in front of the pump is closed, the pump powered down, and a sample is withdrawn through the septa of the sampling port using a syringe and needle.</p> <p>FIXME - add photo of this device</p>"},{"location":"procedures/belowsnow_soilresp/#field-procedure-for-filling-exetainers","title":"Field procedure for filling exetainers","text":""},{"location":"procedures/belowsnow_soilresp/#checklist","title":"Checklist","text":"<ul> <li>Pump and associated tubing and sampling fittings</li> <li>2 charged Li-Cor batteries</li> <li>Flowmeter/mass flow controller</li> <li>List of calculated pump times (see above)</li> <li>Stopwatch</li> <li>Evacuated exetainers (evacuated on Bowling Lab vacuum line prior to departure)</li> <li>Luer-lock 20ml+ syringe</li> <li>Luer-lock needles (bring spares)</li> <li>Sharpie for marking exetainers</li> <li>Spare septa</li> <li>5/8 and 9/16 wrenches</li> </ul>"},{"location":"procedures/belowsnow_soilresp/#procedure","title":"Procedure","text":"<ul> <li>Ski to the first set of sampling ports to be sampled (they are in threes).</li> <li>Loosen the plug from each sampling port using the wrenches.</li> <li>Assemble the syringe and needle, replace the septum on the pump/sampling assembly, and attach the flowmeter (or MFC) to the pump/sampling assembly.</li> <li>Remove the plug from the first port and attach to the lower flowmeter inlet.</li> <li>Set stopwatch to zero.</li> <li>Open pump inlet valve, start pump by attaching Li-Cor battery, and start stopwatch - all at roughly the same time.</li> <li>While pump is pumping label the exetainer that you will use to hold the sample.</li> <li>When the pumping time is up, shut the pump inlet valve and unplug the battery from the pump.</li> <li>Uncap the needle on the syringe and insert it through the septum</li> <li>Flush the syringe once or twice</li> <li>Fill syringe to the necessary volume - exetainers are 12ml, but fill to at least 15ml at high elevation sites (see site recommendations below).</li> <li>Insert needle and empty syringe sample into the labeled exetainer.</li> <li>Repeat steps 4-12 for the rest of the ports, them move to the next set and remove again.</li> <li>Be sure to collect three atmospheric samples (through the syringe) during the sampling campaign.</li> </ul>"},{"location":"procedures/belowsnow_soilresp/#measuring-the-exetainers","title":"Measuring the Exetainers","text":"<ul> <li>Exetainers are returned to the lab and measured for CO~2~ concentration and \u03b4^13^CO~2~ using a LI-7000 (CO~2~) open path system, and a tunable diode laser spectrometer (C isotopes).</li> <li>See the protocol for this here.</li> </ul>"},{"location":"procedures/belowsnow_soilresp/#calculating-respiration-rates-and-isotope-ratios","title":"Calculating respiration rates and isotope ratios","text":"<p>Soil respiration rate can be calculated using Fick's law, adjusted for snowpack properties:</p> <p>$$ F = \\rho_a \\eta \\tau D \\frac{dC}{dz} $$</p> <p>where $\\rho_a$ is the molar density of air (adjusted for temperature and pressure using the Boyle-Charles law, $\\eta$ and $\\tau$ are the air-filled porosity and tortuosity of the snowpack, respectively , $D$ is the molecular diffusivity of CO~2~ in air (adjusted for temperature and pressure following Massman 1998), and $C$ is the mole fraction of CO~2~ at height $z$.</p> <p>FIXME - add isotope ratios?</p>"},{"location":"procedures/belowsnow_soilresp/#notes-for-hidden-canyon-samples","title":"Notes for Hidden Canyon Samples","text":"<ul> <li>Fill syringe to 15ml so that exetainers are slightly pressurized before being brought down to a lower elevation (and higher pressure).</li> <li>Many of the inlets are plugged, so there may need to be a revision of the install procedure.</li> <li>Tubing lengths and pump times for each inlet are located in the HC_inlets spreadsheet.</li> </ul>"},{"location":"procedures/canopyphotos/","title":"Hemispherical canopy photos","text":"<p>Hemispherical photos can be used to calculate a number of important parameters about a forest canopy: canopy area, canopy radiation transmission, leaf area index (with adjustments), and other information. Here are our field and data analysis procedures.</p>"},{"location":"procedures/canopyphotos/#field-procedure","title":"Field procedure","text":""},{"location":"procedures/canopyphotos/#equipment-checklist","title":"Equipment checklist","text":"<ul> <li>Sigma EX 8mm/F3.5 Fisheye lens</li> <li>Canon Digital SLR camera body with EX lens mount</li> <li>MUST have a full frame (35mm) sensor - The APS-C sensor in most dlsr cameras have a smaller field of view and crop photos from the lens above.</li> <li>We are renting a Canon EOS 5d Mark II camera</li> <li>Tripod capable of being leveled, or a leveling device.</li> <li>A compass</li> <li>Marker to place directly north of the camera (ski pole, probe, etc)</li> </ul>"},{"location":"procedures/canopyphotos/#procedure","title":"Procedure","text":"<ul> <li>Select photo location and set up tripod and camera so that lens is at 1m above ground level.</li> <li>Using a compass place a North marker a few meters due north(magnetic) of the camera.</li> <li>Orient the top of the camera so that the top of the photo will be at magnetic north (the marker will appear here in the photo).</li> <li>Level the camera using a carpenter's level or the inclinometer of the compass.</li> <li>Check the north orientation one more time, duck, remove lens cap, and take photo</li> </ul>"},{"location":"procedures/canopyphotos/#photo-analysis-software-gla","title":"Photo analysis software (GLA)","text":"<p>Here is the gap light analyzer (GLA) software (and helpful manual):</p> <p>http://www.rem.sfu.ca/forestry/publications/downloads/gaplightanalyzer.htm</p>"},{"location":"procedures/canopyphotos/#configuration-for-gla","title":"Configuration for GLA","text":"<p>Configurations specifying the camera/lens used, the site where they were taken, and parameters used to model incoming radiation, are stored in a file called a //.scn// file. These can be loaded for multiple photos. The settings used for Hidden Canyon are shown below as an example.</p> <ul> <li>The top of the photos that I take are referenced to magnetic North and need to be adjusted with the appropriate declination. Calculate this at this NGDC site.</li> <li>Lens projection is Polar (azimuthal equidistant) for Sigma lenses.</li> <li>Topographic mask: This is a vector of azimuth and zenith angles that masks the horizon line if there are ridglines, mountains, etc that block the sun at the photographic location for part of the day. These can be generated with a GIS, or with the provided software. They can be saved in a //.msk// file for use with multiple photos. Currently I just pick a photo and fuss with the array values until it looks right.</li> </ul> <pre><code>--- IMAGE --- Initial Cursor Point: 11.88 degrees. # Declination\nProjection Distortion: Polar # For sigma lens\n\n--- SITE --- Latitude: 40:367 North Longitude: 111:342 West Elevation:\n2890 Slope: 21 # Measured Aspect: 200 # Measured Topographic Mask: Yes\n# Created in the configuration interface\n\n--- RESOLUTION --- Solar Time Step: 2 mins Growing Season Start: 4:15 #\nBeginning of growing season for seasonal calculations. Growing Season\nEnd: 10:31 # End of growing season for seasonal calculations. Azimuth\nRegions: 36 Zenith Regions: 9\n\n--- RADIATION --- Data Source: Modelled Solar Constant: 1367 Output\nUnits: Mols m-2 d-1 Cloudiness Index (kt): 0.5 BeamFraction: 0.5\nSpectralFraction: 0.5 Sky Brightness Dist.: UOC Model Clear-Sky Trans.:\n0.65\n</code></pre>"},{"location":"procedures/canopyphotos/#procedure-for-photo-analysis-in-gla","title":"Procedure for photo analysis in GLA","text":""},{"location":"procedures/canopyphotos/#1-classify-an-imageregister-mask-and-threshold","title":"1. Classify an image(register, mask, and threshold)","text":"<ul> <li>Create or load the appropriate configuration file (//.scn// file) for your photos.</li> <li>If necessary, load a //.msk// file or create a topographic mask and save it to the configuration. This mask can created or edited by changing azimuth and zenith angles within GLA, or (somehow) with a GIS.</li> <li>This mask can be saved with the //.scn// file, and thus automatically loaded each time that configuration is used. </li> <li>Open a hemispherical photo into GLA.</li> <li>Register the photo by marking the top (North) of the photo, and dragging the cursor to the bottom (south) of the photo, taking care to create a circle that squarely encompasses the extent of the fisheye photo.</li> <li>The registration coordinates can be saved and applied to subsequent photos.</li> <li>After registering the photo, windows with \"Registered\" and \"Working\" versions of the photo will be shown. Apply the overlays of the sky region grid and the topographic mask to the \"Registered\" photo to make sure the mask looks good.</li> <li>For photos taken under a clear sky, it is wise to select the blue color plane before applying a threshold. It removes some glare and lens flare and improves the contrast between vegetation and sky. This change, and the threshold will appear in the \"Working\" image.</li> <li>Once registered, the\"Working\" photo's properties, such as contrast, color depth, brightness, etc., can be modified using the menu system. These options may help improve the threshold process in the next step.</li> <li>Apply a threshold to the photo and inspect photo using the zoom tool. If the threshold tool did a good job, vegetation and sky are accurately represented by black and white.</li> <li>If the threshold looks good, calculate canopy structure and/or transmitted gap light. There are also several modeling routines that can calculate other aspects of canopy transmission.</li> </ul>"},{"location":"procedures/canopyphotos/#2-compute-results","title":"2. Compute results","text":"<p>Once the configuration, registering, and threshold of the image is complete, the calculations for the image can be made. There are 2 options for this, the first calculates only canopy structure (6 parameters), and the second calculates canopy structure plus transmitted gap light calculations (about 15 more parameters). Once the calculations are complete, the calculations can be appended to a table. Do not forget to append data for each image to the table! There is also an option to save detailed data for each sky region to a text file.</p> <p>Some interesting calculations:</p> <ul> <li>Sky Area is the percent area of the sky hemisphere found above the effective horizon. If the effective horizon is at 90o (i.e., no topographic mask), then % Sky Area will equal 100 percent. However, if the effective horizon is less than 90o, then the area of visible sky will be less than 100 percent.</li> <li>Mask Area is the percent area of the sky hemisphere that is obstructed by the surrounding topography.</li> <li>Canopy Openness is the percentage of open sky seen from beneath a forest canopy. This measure is computed from the hemispherical photograph only, and does not take into account the influence of the surrounding topography.</li> <li>Site Openness is the percentage of open sky seen from beneath a forest canopy given the additional influence of an effective horizon that is less than 90o (topographic shading)</li> <li>Trans Direct is the amount of direct solar radiation transmitted by the canopy and topographic mask (if one has been defined).</li> <li>Trans Diffuse is the amount of diffuse solar radiation transmitted by the canopy and topographic mask (if one has been defined).</li> <li>Trans Total is the sum of Trans Direct and Trans Diffuse.</li> <li>% Trans Direct is the ratio of Trans Direct to Above Direct Mask multiplied by 100%.</li> <li>% Trans Diffuse is the ratio of Trans Diffuse to Above Diffuse Mask multiplied by 100%.</li> <li>% Trans Total is the ratio of Trans Total to Above Total Mask multiplied by 100%.</li> </ul> <p>After calculating and saving these values you can move on to the next image, OR...</p>"},{"location":"procedures/canopyphotos/#3-compute-other-things","title":"3. Compute other things","text":"<p>There are a few options, including detailed information for each sky region (values above for every sky region), a solar intensity model for the site, percent transmittance for each solar zenith value, etc.</p>"},{"location":"procedures/dustonsnow/","title":"Snowmelt acceleration by dust deposition","text":"<p>Artificial dust deposition on snow surfaces is hypothesized to lower the albedo of the snow surface, increase shortwave energy absorption, and accelerate the spring snowmelt cycle. This is a procedure to generate dust similar to naturally deposited dust in the western U.S. and distribute it on a snowpack.</p> <ul> <li>Prodedure in use at Hidden Canyon(and expected to influence ecohydrology(plant water balance, soil moisture, C cycling) in plots with enhanced dust deposition.</li> <li>Regional dust sources</li> <li>The snowmelt measurements page for Hidden Canyon.</li> </ul>"},{"location":"procedures/dustonsnow/#dust-collection-and-preparation","title":"Dust collection and preparation","text":"<ul> <li>Dust is dug from the ground at an appropriate source site. Care is taken not to dig large amounts of organic matter, seeds, roots, etc. </li> <li>Dust dried in drying ovens for at least 24 hours</li> <li>Sift to 500\u00b5m or less.</li> </ul>"},{"location":"procedures/dustonsnow/#dust-application","title":"Dust application","text":"<p>Dust is applied to the snow surface in treatment plots during the spring snowmelt season. Dust application begins before peak snowpack (April 15), and dust is reapplied each time new snow covers the prior dust layer until the snowpack has melted completely.</p>"},{"location":"procedures/dustonsnow/#application-amounts-hidden-canyon-experiment","title":"Application amounts (Hidden Canyon experiment)","text":"<ul> <li>Target deposition rate: ~5g per m^2^</li> <li>Application area: 10m by ~70m = 700 m^2^</li> <li>Probable number of applications: 8</li> <li>Total dust needed: 5g x 700 m^2^x 8 = 28000g</li> </ul>"},{"location":"procedures/dustonsnow/#application-equipment","title":"Application equipment","text":"<ul> <li>Blower method Dust is blown onto the snowpack using a small ShopVac modified to blow air through a PVC extension. The PVC pipe is fitted with a dust \"cartridge\" (a mason jar) that feeds dust into the airstream at a steady rate. The blower is powered by a 12V battery and inverter carried in a backpack. Each mason jar holds 1200g of dust. Roughly 3 should cover a 700m^2^plot. This method was used in 2010 at Hidden Canyon.</li> <li>Throwing method Dust is scattered tossed by hand onto the snowpack.</li> <li>The best days for either method are relatively calm days with little wind.</li> </ul>"},{"location":"procedures/dustonsnow/#useful-measurements","title":"Useful measurements","text":"<ul> <li>Measuring snowmelt (SWE disappearance) is best done by sampling the snowpack with a Federal Sampler as described on the SWE measurement page.</li> <li>Measuring sublimation at the snow surface(multiple methods).</li> <li>Snowpack albedo measurements</li> <li>Measuring snowpack dust loading(natural or artificial).</li> </ul>"},{"location":"procedures/ea-irms_soilprep/","title":"Soil sample prep for EA-IRMS analysis","text":"<p>This procedure describes the grinding and loading procedure for soils to be run in an EA-IRMS (Elemental Analyzer coupled to an Isotope Ratio Mass Spectrometer). This analysis gives carbon and nitrogen concentration (percent by weight) and the C and N stable isotope ratios (^13^C/^12^C, and ^15^N/^14^N) of a soil sample. Before samples can be loaded into this analyzer they must be very finely ground and homogenized, and small amounts (2-20mg) must be weighed into small tin capsules. These capsules are then loaded into the analyzer by SIRFER staff.</p>"},{"location":"procedures/ea-irms_soilprep/#ball-mill-grinding","title":"Ball mill grinding","text":""},{"location":"procedures/ea-irms_soilprep/#materials-needed","title":"Materials needed","text":"<ul> <li>Rough ground original soil or organic samples (in yellow topped plastic sample cups).</li> <li>Steel grinding capsules, with balls (borrow from Ehleringer lab)</li> <li>Clean glass subsample vials.</li> <li>Labeling tape</li> <li>Sharpie</li> <li>Inventory sheet (Greg's)</li> <li>SIRFER Lab supplies - you will be doing this in the SIRFER lab weighing area.</li> </ul>"},{"location":"procedures/ea-irms_soilprep/#procedure","title":"Procedure","text":"<ul> <li>After selecting a sample from the pile, label a clean, glass subsample vial with the site code, plot number, and sample type from the original sample cup. Always make sure the all label information follows a sample from the original container to the subsample container.</li> <li>Using a lab spoon, mix the sample in the cup, then scoop it into a steel grinding capsule. Fill the capsuleabout 2/3 to the top, unless the original sample is very small. Try to leave at least a bit of original sample in its cup for archiving.</li> <li>Add a steel grinding ball to the capsule, then put the steel lid on the capsule, and wrap labeling tape all the way around the lid. </li> <li>Label the grinding capsule with the sample information (again - all info follows the sample).</li> <li>Take the labeled capsules upstairs to the Ehleringer ball mill and securely fasten 2 capsules into the holders.</li> <li>Run the capsules on the ball mill for at least 1 minute at 25+ cycles per second.</li> <li>Empty the now powdered subsample into its labeled glass vial. A plastic funnel is useful here. Use a clean funnel between each sample so that you don't cross contaminate samples while transfering them.</li> <li>These powdered samples can now be further processed, or weighed into tin capsules.</li> </ul>"},{"location":"procedures/ea-irms_soilprep/#acid-treating-soil-samples","title":"Acid treating soil samples","text":"<p>Soils that have, or may have, carbonates present should be acid treated to remove carbonate carbon, as this does not come directly from organic matter in the soil. Such soils include alkaline soils or dry soils, but could also include soils that derived from carbonate rocks.</p> <ul> <li>Procedure for acid treatment of soil</li> </ul>"},{"location":"procedures/ea-irms_soilprep/#weighing-samples-into-tin-capsules","title":"Weighing samples into tin capsules","text":""},{"location":"procedures/ea-irms_soilprep/#materials-needed_1","title":"Materials needed","text":"<ul> <li>Milled soil or organic samples (in glass subsample vials - see grinding procedure above).</li> <li>Pencil</li> <li>Inventory sheet (Greg's)</li> <li>Sample loading sheet from SIRFER</li> <li>Tin capsules</li> <li>Sample tray (96 well \"Nunc\" type)</li> </ul>"},{"location":"procedures/ea-irms_soilprep/#procedure_1","title":"Procedure","text":"<p>Samples must be carefully weighed into pressed tin capsules, then folded up and placed in the wells of a sample tray. Be sure to keep samples organized as you measure them, and clean all tools and work surfaces before each sample.</p> <ul> <li>Turn on the balance, clean off the tray with the vacuum, and calibrate it (press F1, or instructions are on the wall).</li> <li>Using tweezers, take a new tin capsule from the box and place it on the tray of the scale. Press TARE.</li> <li>When the scale reads 0.000 again, open the door, remove the tin capsule, and place it on the glass work surface in front of you.</li> <li>Open the sample vial and use a small scoop to transfer the sample to the capsule. Put the capsule back on the balance and then add or remove sample as needed to meet the target weight on the inventory sheet.</li> <li>Once you are close to the target weight (\u00b15%), close up the capsule. The best way to do this is to use the flat ended tweezers to \"clamp and roll over\" the opening to the tin capsule. You can then pin down the rolled edge with the other tweezers and squeeze together the corners, forming a squared off bundle. This takes practice.</li> <li>Once you have a solid looking package, pick it up with tweezers and tap it on the glass a couple times and look for leaks. If it is not leaking, brush it off and weigh it on the balance a final time.</li> <li>Don't forget to record the weight on the inventory sheet AND the SIRFER loading template (with its sample ID)</li> <li>Clean your tools and work surface with a Kimwipe and alcohol before moving to the next sample.</li> <li>Remember to skip (leave empty) all wells labeled with COND or standard codes on the SIRFER loading template. They will be filled by SIRFER staff.</li> <li>You may mix 2-3 repeated unknowns into each SIRFER run.</li> </ul>"},{"location":"procedures/exetainer_co2/","title":"Exetainer Injection Method","text":"<p>This procedure is adapted from Nicole Trahan's and Andrew Moyes's procedures for measuring CO~2~ in exetainers returned to the lab. Concentration is measured on the LI-7000 IRGA, and CO~2~ isotope ratios on the Campbell TDL in the Bowling lab using syringe injections from the exetainers. The original method of measuring CO~2~ on the LI-7000 (still shown below) developed by Nicole and Andrew was deprecated because it was producing double peaks, and the sample integrals were often being lost after injection of a sample. This was unacceptable for small samples that also needed to be run for C isotopes. For this reason, the LI-7000 was connected to a datalogger, and the injection integrals are recorded by the datalogger now. The timeseries produced by this can now be analyzed in MATLAB and bad peaks can be dealt with there.</p> <p>Used for:</p> <ul> <li>Hidden Canyon soil respiration measurements (in winter)</li> </ul> <p>(last revised on 120112 by Allison Chan)</p>"},{"location":"procedures/exetainer_co2/#li-7000","title":"LI-7000","text":"<ol> <li>Turn on Li-7000 about 20-30 minutes before starting injections</li> <li>While it is warming up, press <code>Shift, 6</code> to change the bottom display on the Li-7000.  Press <code>Pump Off</code> and then change the septum.</li> <li>After the septum is replaced, press <code>Pump Fast</code> (a star should show up next to it).</li> <li>Change the bottom display back to the <code>Reference/Calibration</code> mode by pressing <code>Shift, 1</code> and press <code>2</code> to switch the top display to show integrals.</li> <li>Press <code>calib</code> on the bottom display and then choose <code>edit</code> (if <code>edit</code> is not an option, select <code>more</code>, until it becomes a choice in the bottom left corner).  In the edit menu, use the arrows to scroll down, select <code>make cell B match cell A</code>, and press <code>Ok</code>.  Then press <code>DoCO2</code> and <code>Done</code>.  On the top screen, the CO2 A and CO2 B concentrations should match.</li> <li>This calibration should be done as often as needed throughout the injections.  The two cells should be within 0.01-0.03 \u03bcm/m.</li> </ol>"},{"location":"procedures/exetainer_co2/#datalogger","title":"Datalogger","text":"<ol> <li>Turn on the CR23X datalogger and connect to <code>Nebo</code> laptop (requires USB converter cord).  Open Loggernet, click on the <code>connect</code> icon and connect to datalogger.  </li> <li>Under the <code>Program</code> section of the Connect window, be sure the co2_inj_v1.dld program is selected, then press Send and agree to all further windows that pop up (this should overwrite the previous round of data collection)</li> <li>NOTE: Resending the program doesn\u2019t actually seem to erase the datalogger.  To manually erase the datalogger, press <code>A</code> on the CR23X keypad and then press <code>A</code> 4 times to get to the Allocation Program Bytes screen.  Then enter <code>98765</code> and then <code>A</code>. (Not erasing the datalogger is only a problem in that will take longer to download data).</li> <li>After sending the program, press 1 next to Graphs.  At this point, begin injections.</li> </ol>"},{"location":"procedures/exetainer_co2/#injections","title":"Injections","text":"<ol> <li>Once the machine has warmed up, inject standards, using tanks that fit the CO2 range of the samples.  Tanks being used for the Niwot samples are 15 (395.78 ppm), J39 (3144 ppm), and J29 (14,051 ppm).  Change the septa on the tanks about every 75-100 samples.  After opening the tanks, flush the headspace with a 1.0 ml syringe with a sideport needle.</li> <li>Be sure to support the needle when pushing into or pulling out of the septum to prevent the needle from bending.  Also be sure to hold the base of the needle when pulling out of the septum to prevent needle from being pulled off, and check periodically to make sure the plunger is screwed into the base.</li> <li>Use a 1.0 ml syringe with a sideport needle and flush it 3 times with the first standard using the open and close sliders on the syringe.  Then fill the syringe up to 1.0 ml, close the syringe, withdraw it from the septum and compress it to 0.6 ml.</li> <li>Bring the pressurized sample back to the Li-7000 and just before injecting into the septum, open the syringe and compress to 0.5 ml, then inject the 0.5 ml into the machine.  </li> <li>Inject one sample of each of the 3 standards before beginning injecting unknowns.  The three standards should be injected (in an alternating order) after about every 12-15 samples.  </li> <li>To inject unknowns, insert the needle into the exetainer and withdraw about 0.5 ml and flush.  Insert the needle again and flush about 3 times with the needle in the exetainer before pulling a 0.8 ml sample.  Close the syringe, withdraw the needle, and compress the sample to 0.6 ml.  As with the standards, open the syringe and compress to 0.5 ml and then inject 0.5 ml into the Li-7000.</li> <li>Finish with one injection of each of the 3 standards.</li> <li>To download the data, click <code>Custom</code>, under the Data Collection section of the Connect screen.  Name the data file and select location to save.  Then click <code>Collect</code>.</li> </ol>"},{"location":"procedures/exetainer_co2/#old-li-cor-7000tdl-tga-method-nicoleandrews","title":"Old Li-Cor 7000/TDL-TGA Method (Nicole/Andrew's)","text":"<p>See note at top of page about why this method was deprecated.</p> <ul> <li>Collect Data if a program was running (not needed for concentration measurements on the LI-7000).</li> <li>Open loggernet, click connect</li> <li>If a program was running, collect the data before you start injections. Click custom under data collection in the connect screen.</li> <li>If Lab_TGA_CR5000 (roof air data collection) was running (which usually it is if the laser is in the lab) check one.sec and ten.min. Highlight one.sec, click change file name, change the data to today\u2019s date and click save. Do the same to ten.min. Make sure file format is Binary Table Data TOB1 and collect mode is set to All the Data. Then click start collection.</li> <li>When all of the data has been collected (may be a while) click disconnect on the left hand corner of the connect screen.</li> <li>Start LI-7000 if the concentration of CO2 is desired</li> <li>Turn on the LI-7000, allow to warm up for about 5 minutes.</li> <li>While waiting, press 2 on the keypad on the right to move to the correct display screen. Press shift-6, turn the pump off.</li> <li>With the pump off, change the septum. Be sure to screw in swage nuts gently and center washers. Turn pump back on, and make sure there is a star next to pmp med. Press shift then 1 to get back to the display screen.</li> <li>Press calib on the bottom of the screen. Press edit, then use the arrows to highlight make cell B match cell A, press ok. Then press DoCO2 on the bottom of the screen. When CO2A\u00b5m/m matches CO2B\u00b5m/m (which should be zero) press done. <ul> <li>NOTE: Do this anytime during the procedure when CO2B\u00b5m/m is + or \u2013 0.05 (or even \u00b1 0.01-0.03). Check these values for drift before each injection.  Press 2 to return to the correct display screen when finished.</li> </ul> </li> <li>Pick 3 tanks to be standards. For example, Nicole\u2019s samples range in concentration from 400-3000ppm so she uses tanks J26 (433ppm), J59 (1504ppm), and J39 (3144ppm) as standards. Change the septum on the tanks. Open tanks and flush the septum headspace with a double ended needle. <ul> <li>NOTE: When using the syringe ALWAYS support the needle when pushing into or pulling out of the septa. If not the needle will bend. Also hold the needle base when pulling needle out of septa and check the syringe periodically to make sure the metal base with the plunger is tightly screwed in.</li> </ul> </li> <li>Take a 1.0ml syringe and flush it 3 times with your first standard using the open and close sliders on the syringe. Fill the syringe all the way up, close the syringe, withdraw it from the septum, and compress the standard gas until you feel resistance or until you get to 0.6ml. This pressurizes the syringe to avoid ambient contamination. </li> <li>Walk back to the LI-7000 and just before injecting into its septum, open the syringe and compress until you reach 0.5ml, then inject into the LI-7000. (NOTE: you will always inject 0.5ml into the LI-7000.)  Record the Integral value on the screen. Repeat this 3 times or more for each tank before you start and when you finish.  CO2 integral values should range within \u00b1 5% of one another.</li> <li>After injecting your standards you are ready to inject your unknowns. The procedure is similar, but less sample is flushed out of the vial. Insert the needle into the sample vial, fill the syringe to about 0.5, then withdraw and flush. Reinsert the syringe and flush several times in the sample vial. Then pull a 0.8ml sample, withdraw needle, compress to 0.6ml, inject 0.5ml and record the Integral as above.<ul> <li>Be mindful of how much sample you may need to do isotopes (up to 5ml) and keep that much in the vial.</li> </ul> </li> <li>Before starting injections on the laser I usually get about 15-20 samples ahead on the LI-7000. (See page 6 for lab notebook set-up) </li> <li>Starting syringe program on the laser</li> <li>On the valve relay controller switch 5 on and 1-4, 6-16 off. Unscrew the cap that reads remove for zero and change the septum where the samples are injected.</li> <li>Under stations on the connect screen highlight Syringe_CR5000 and press connect. Under program click send and highlight SYR_V10.CR5 then click open, then yes, then ok.</li> <li>Make sure graph 1,2 and numeric display 1 appear. If not make them appear using data displays on the connect screen (you may have to hit \u2018start\u2019 manually).</li> <li>Look at Numeric Display 2, use the checklist on the next page to make sure the laser is on the lines and running properly.<ul> <li>Checklist:</li> <li>LaserTemp: 95.60,  If off, check for L N2, notify Andrew or Dave</li> <li>RefDetTemp: 5</li> <li>RefDetTemp: 5</li> <li>RefDetTransA: around 47 (ranges between 40-50)</li> <li>RefDetTransB: around 37 (should be roughly 10 units away from TransA) (ranges between 30-40)</li> <li>DCCurrentA and DCCurrentB are exactly 4.5 units apart </li> <li>Cell T: about 30,  If off, heater may be off in box</li> <li>TGA pressure: about 22 ,  If off pump may have issues</li> </ul> </li> <li>If the numbers on the screen do not match up with the criteria on the checklist there is something wrong with the laser. If DCCurrentA and DCCurrent B are not 4.5 units apart then the laser is most likely off the lines. Get Dave or Andrew to put the laser back on the lines.</li> <li>Turn the handle from roof air to Inj zero.</li> <li>Finish step 2.</li> <li>Running Injections on the Laser</li> <li>Before starting syringe program make sure your notebook is set up properly (see page). Also make sure that you are at least 15-20 samples ahead if using LI-7000. <ul> <li>NOTE: You will be using tanks T1 (J39), T2 (J46), and T3 (J48) as standards. Always inject them in that order: T1, T2, T3. Always start with and end with the standards. Inject the standards after injecting 3-6 gas samples (see page 6). The amount of sample injected depends upon the concentration of CO2 of the sample or the integral if using LI-7000 (see page 7). Always write the injection amount next to the sample ID in your notebook (see page 6). You will always inject 0.8mL for the standards (T1,T2, T3) because they are ~3000ppm. The type of syringe used depends on injection amount (see page 7).</li> </ul> </li> <li>Change septum on standard tanks T1 (-8\u2030), T2 (-20\u2030), and T3 (-31\u2030). Open tank valves. Use a two-ended needle through septum and valve to flush the area where the septum is. Always use the long end with the sharper tip to minimize damage to the septa.</li> <li>Turn handle from roof air to Inj Zero.</li> <li>Look at the clocks on the connect screen. Sometimes the two clocks do not match up, if this happens click set station clocks. (NOTE Andrew would prefer if this step is ignored for now). </li> <li>Send Syringe_CR5000 program about 1.5 minutes before X:00, X:10, X:20, X:30, X:40, or X:50. This will give you enough time to prepare your first injection which is T1 and the syringe program always starts the first injection in multiples of 10 minutes starting from :00, :10, :20, :30, :40, or :50.</li> <li>You will inject every 2 minutes (120 sec.) There is a counter, Inj Time (Numeric Display 1), that counts up to 110 and then from -10 to 0. Keep your eye on Inj Time and always inject on time 0. You will see it count from 108, 109, -10, -9, -8, \u2026\u2026, -1, 0, 1.(At about -6 I open the syringe that contains the gas sample and push until I am at the injection size so that I can immediately inject the sample at 0). Pay attention to InjNumber, make sure it matches what is in your notebook (see pg 6).<ul> <li>NOTE: To collect the standard gas samples use the 5mL syringe. Flush the syringe with the standard at least 3 times. Then fill syringe all the way up and push down until you feel resistance. To collect a gas sample only flush the syringe once (if using the 5mL syringe only go up to 0.5 mL then flush) then fill the syringe all the way up and push down until you feel resistance.</li> <li>SYRINGE NOTES </li> <li>5mL Syringe: Used for injection amounts 0.5mL \u2013 max. Max (an integral &lt; 45) is anything over 2.5mL. For max, flush the syringe like normal, then fill up the 5mL syringe. Press down just until it stops. From my experience that is usually around the 2.8-3.2mL mark. Do not open syringe prior to injecting just simply place needle in septum open syringe and inject.</li> <li>1mL Syringe: Used for injection amounts 0.1mL-0.5mL.</li> <li>0.5mL Syringe: Used for injection amounts 0.01mL-0.1mL.</li> <li>IMPORTANT: Keep an eye on graph 2 and numeric display 1. If you see numbers drifting (especially the DC currents) stop immediately, the laseris off the lines.</li> </ul> </li> <li>At your last injection, which should be T3, do not start collecting data until that injection has run through (until injection number is 1 + your last injection).</li> <li>Finishing Injections</li> <li>After you have let your last sample run through click custom under data collection. Check onesec, tenHZ, and tenmin. Highlight onesec, click change file name, change the date to todays date, click save. Repeat this for tenHZ and tenmin. Make sure collect mode is All the Data, File Mode is Append to End of File, and File Format is Binary Table Data TOB1. Click start collection.</li> <li>After data has been collected click disconnect.</li> <li>If Lab_TGA_CR5000 was running prior to injections switch 1-5 to auto and 6-16 off on the valve relay controller. (Alternately, leave 5 on, and 1-4 off- check with Andrew).</li> <li>Turn handle from Inj zero to Roof air and place the cap back on.</li> <li>Click on Lab_TGA_CR5000 in stations, click connect, click send. Highlight ASB_roof_V1.CR5. Click open, then yes, then ok.</li> <li>NOTE: Remember to close all tanks that were used and turn off the LI-7000.  Leave lap top running.</li> <li>Running the Data File on MATLAB</li> <li>Collect the file hzdate.dat in the injection folder on the desktop. The os and tm files are only used as a record of the laser\u2019s performance. Save the file to you computer (or a computer that has MATLAB).</li> <li>Open MATLAB. In the editor window open SYR_10Hz_Process_v11.m. You will find this here: \\Bowling_nas\\Bowlinglab\\backed up\\matlab m files. This is the most current MATLAB program that will run your data file.</li> <li>Go to line 37 and change datapath to where you have saved your hzdate.dat. Save.</li> <li>In the command window type in or copy and paste the function SYR_10Hz_Process_v11.</li> <li>The datapath will come up, select your hzdate.dat file.</li> <li>Type in # of injections (use all injection times, even ones missed).</li> <li>Type in the injection numbers of all the T1 injections in order (ex: [1;8;12;18;25])</li> <li>If all injections aren\u2019t good or injections are missed, comment out line 119 and uncomment out line 120 in the code. Specify the good sets of injections.</li> <li>MATLAB will go through each injection individually and the output file will be an excel file.csv that will be in the same place as your input hzdate.dat file.</li> </ul>"},{"location":"procedures/exetainer_co2/#notes-for-hidden-canyon-samples","title":"Notes for Hidden Canyon samples","text":"<ul> <li>Tanks/concentrations used: J39/3144ppm, J59/1504ppm, J20/461.23ppm (used once), J25/395ppm, and the big red 6000ppm tank (6356.6ppm).</li> <li>Since isotopes are not being measured, I am flushing the syringe with 0.8ml of sample gas once before drawing the injection sample.</li> <li>Sample draw is to 0.8ml, which is then compressed to 0.6.</li> <li>Injection volume is still 0.5ml</li> </ul>"},{"location":"procedures/gps/","title":"Georeferencing with GPS","text":"<p>It is essential to know where things are - research locales and instrumentation, organisms or populations, geographic features, good lunch spots, etc - here are some tips on assigning coordinates to them in the field.</p>"},{"location":"procedures/gps/#datum-and-coordinate-conventions","title":"Datum and coordinate conventions","text":"<p>The WGS84 datum seems to be generally accepted for ecological research around the world and is the reference coordinate system used by the GPS system. The primary, and most accurate geodetic datum in North America is NAD83. So for survey grade work, it is best to use NAD83 in North America. For standard accuracy work (handheld GPS) WGS84 and NAD83 should be interchangeable in North America.</p> <p>Decimal degrees are usually the easiest to use, but UTM is also common. Utah is in UTM zone 12N.</p>"},{"location":"procedures/gps/#handheld-gps","title":"Handheld GPS","text":"<p>Most handheld GPS systems are capable of around 3m horizontal position accuracy.</p>"},{"location":"procedures/gps/#differential-gps-dgps","title":"Differential GPS (DGPS)","text":"<p>For survey grade georeferencing (&lt;1m horizontal accuracy), a DGPS can be used. In these systems there is a base station that records a static GPS position and a GPS rover that references its positions to the base station. On occasion we have borrowed a Trimble 4700 DGPS system from Geology and Geophysics at the U. of Utah. Information on its use is provided by them at:</p> <p>http://thermal.gg.utah.edu/facilities/gps/index.shtml (we used the RTK method).</p> <p>See the Hidden Canyon georeferencing page for info on how we used this system.</p>"},{"location":"procedures/gps/#data-formats","title":"Data formats","text":"<p>Unfortunately there is no standard output data format for GPS data among the many makers of GPS units. Luckily there are some open and interchangeable geodata formats and software tools to do the conversion.</p> <ul> <li>The GPX data format</li> <li>GPSBabel software- excellent for converting data formats.</li> <li>FIXME - currently have no idea how to use Trimble DGPS data with any other software besides Trimble Geomatics Office.</li> </ul>"},{"location":"procedures/leafarea/","title":"Leaf Area Index","text":"<p>There are many different methods for measuring and estimating Leaf Area Index. Other vegetation coverage and biomass indices, such as canopy transmissivity or Plant Area Index are similar to or can be related to LAI. See the wikipedia page for additional details. This is about some fairly easy methods to get at LAI (or similar measures) using digital images.</p>"},{"location":"procedures/leafarea/#plant-area-index-using-imagej","title":"Plant area index using ImageJ","text":"<p>ImageJ, software created for biological image analysis by NIH (available here, can be used to calculate leaf area at several scales if there is sufficient contrast between the leaves/canopy and the background. Below are instructions for calculating plant area index from aerial images, such as those available from Google Earth.</p> <ol> <li>Open the aerial photo in ImageJ.</li> <li>Set the scale for the photo.</li> <li>You must have some reference scale on the photo, such as a scale bar or an object of known dimensions.</li> <li>Select the straight line tool and draw a line along the known dimension, which should return a length in pixels.</li> <li>With that line drawn, open \"Analyze&gt;Set Scale\"</li> <li>The distance in pixels should be populated based on the line you drew, then type in the known distance of this line and the units. Check \"Global\" and select OK. This should reference the image in feet.</li> <li>If a color photo, make it grayscale with \"Image&gt;Type&gt;8bit\". There are other options for this, such as splitting the photo into RBG channels and using one channel.</li> <li>Threshold the photo with \"Image&gt;Adjust&gt;Threshold\" and then adjust the sliders until the vegetation in the photo has been highlighted appropriately. The \"Auto\" button will try to do this step in a standardized way, which may work.</li> <li>Draw a region of interest on the image now using one of the tools (Rectangular, elliptical, polygon, etc).</li> <li>Measure the area of the ROI using the \"Analyze&gt;Measure\" tool. If the scale is set correctly this should provide a Results box with the area of the highlighted ROI calculated in the chosen units.</li> <li>Now calculate the area of the vegetation (thresholded areas) using \"Analyze&gt;Analyze Particles\". In the dialog select \"Summarize results\". The calculated area of the vegetation particles (in chosen units) will appear in the \"Summary\" box.</li> <li>Divide the vegetation area by the ROI area to get Leaf/Plant Area Index.</li> </ol>"},{"location":"procedures/leafarea/#hemispherical-canopy-photography","title":"Hemispherical canopy photography","text":"<p>Hemispherical photos (using a \"fisheye\" lens) can be used to calculate a number of important values related to and approximating LAI.</p> <p>See the Canopy photography page</p>"},{"location":"procedures/litterbags/","title":"Litterbags","text":""},{"location":"procedures/litterbags/#litterbag-mass-loss","title":"Litterbag mass loss","text":"<p>Litterbags were constructed of 10 x 15cm pieces of black nylon and gray fiberglass fabric. The nylon fabric is approximately 0.2mm (No-See-Um mesh), and the fiberglass screen is approximately 1.7mm mesh. Both were purchased from Seattle Fabric Inc.</p>"},{"location":"procedures/manual_soilresp/","title":"Handheld soil respiration measurements","text":"<p>Growing season soil CO~2~ fluxes are measured at respiration collars using the Li-Cor 6400 and a handheld soil respiration chamber (LiCor 6400-09).</p> <p>In use at:</p> <ul> <li>Hidden Canyon</li> </ul>"},{"location":"procedures/manual_soilresp/#collar-installation","title":"Collar installation","text":"<p>PVC collars are inserted in the soil at field sites to make soil respiration measurements with the LI-6400. Collars should be made from ~4 inch (10cm) PVC tubing to accommodate the 9.55cm diameter LiCor-6400-09 chamber. The tubing should be cut at roughly 5cm lengths and then beveled on one edge. These are inserted 1.5-3cm into the soil surface in the location where respiration is to be measured. Because installation of the collars severs roots and upsets the CO~2~ gradient, they should be installed at least 12 hours prior to measurement.</p>"},{"location":"procedures/manual_soilresp/#measurement-procedures","title":"Measurement Procedures","text":""},{"location":"procedures/manual_soilresp/#materials-checklist","title":"Materials checklist","text":"<ul> <li>LiCor 6400 (in carrying case)</li> <li>LiCor 6400-09 handheld soil CO~2~ flux chamber (attaches to the IRGA)</li> <li>Cables/tubing for connecting items 1 and 2 (in carrying case)</li> <li>2 fully charged LiCor batteries</li> <li>2 full charged spare batteries</li> <li>Pack frame for carrying the case (if hiking into the field)</li> <li>Soil temperature probe (Omega Type-T thermocouple probe) assuming the LiCor 6400's is broken as usual.</li> <li>Bring the probe AND its console</li> <li>Notebook</li> <li>Pen</li> <li>Span gas tank - optional, see the notes in the measurement procedure.</li> </ul>"},{"location":"procedures/manual_soilresp/#making-measurements-with-the-li-6400","title":"Making measurements with the LI-6400","text":"<p>The general procedure for operating the LiCor 6400 is on the instrument page.</p> <ul> <li>This procedure will need to be adapted for each site (see below and notes on the instrument page)</li> </ul>"},{"location":"procedures/manual_soilresp/#general-site-considerations","title":"General site considerations","text":"<ul> <li>Number and location of collars</li> <li>Depends on the site</li> <li>Moving collars</li> <li>Collars may remain stationary through the growing season, or they can be moved prior to each measurement.<ul> <li>This depends on research objectives.</li> </ul> </li> <li>Allow 12+ hours of recovery after hammering collars in at a new location.</li> <li>Measurement frequency:</li> <li>Every two weeks during the growing season is excellent</li> <li>Less frequently during shoulder seasons?</li> <li>Time of measurements:</li> <li>Morning might be cold</li> <li>Afternoons might be too hot</li> <li>Night might exclude autotrophic components</li> <li>Timing should probably be similar between measurement periods</li> <li>Order of collar measurement:</li> <li>If measuring a control and treatment plot, alternate the measurements so that soil temperature changes during the measurement period occur evenly between treatments.</li> </ul>"},{"location":"procedures/manual_soilresp/#notes-for-hidden-canyon","title":"Notes for Hidden Canyon","text":"<ul> <li>Typical <code>Aux OP Parameters</code>:</li> <li>Extra drawdown = 5ppm</li> <li>Flow = 500</li> <li>Dead time = 20 sec</li> <li>Minimum measurement time = 30 seconds</li> <li><code>Delta</code> of around 5-6 usually works best, more if there are lots of roots.</li> <li>May need to modify things to stay within <code>Target +/- Delta</code> range.</li> <li>Measure the soil temperature at 5cm and 15cm depths using the Omega probe (the Li-Cor one is busted)</li> </ul>"},{"location":"procedures/snowpackdustloading/","title":"Measuring snowpack dust-loading","text":"<p>Many different samples from a snowpack can be returned to the lab. These are generally sampled on an ground-area or snow-volume basis, and the idea with this procedure is to filter out the dust in each sample to return a dust loading for each type of sample. Samples that can be treated this way include:</p> <ul> <li>Full or partial snow column samples taken with a coring device (like a PVC sampling tube).</li> <li>Kelly cutter samples - 1L volume samples taken along a snow depth profile.</li> <li>Surface scrapes - known-area surface snow collections.</li> </ul>"},{"location":"procedures/snowpackdustloading/#procedure","title":"Procedure","text":"<ol> <li>Thaw sample in a covered, leakproof container</li> <li>Snow samples are sometimes returned in a ziploc bag, and often these have small holes in them. Therefore they must be thawed in a leakproof container.</li> <li>There are numerous containers in the lab suitable for defrosting these bags, just be sure they are big enough not to overflow.</li> <li>It is important to cover the thawing sample with a lid to prevent evaporation (especially when water isotopes are being measured).</li> <li>Once thawed, empty all contents of the sample bag into its thawing container (make sure it is labeled with a sample ID).</li> <li>As you do this, be sure to suspend all dust in the sample bag by swishing it around</li> <li>Tare a 500ml (or larger) beaker on the balance.</li> <li>Transfer the sample from its thawing container into the tared beaker and record the weight of the sample on the datasheet.</li> <li>The sample is now ready to filter</li> <li>Record the weight of a clean, dry glass fiber filter (this can be done in advance) on the datasheet, and then place it in a clean ceramic filter funnel/sidearm flask assembly.</li> <li>Turn on the vacuum, swish the sample around to suspend the dust, and slowly pour the sample through the filter.</li> <li>The first portion of the filtrate can be saved for water isotope analysis. If this is done, the filtrate to be saved MUST be filtered into a clean, dry filter flask, then transferred to a plastic sample container. The remaining filtrate does not need to be saved.</li> <li>After all the sample has been filtered, carefully place  the filter on a labeled petri and dry it in the drying oven.</li> <li>It is important that the 500ml beakers and sidearm flasks are clean and dry when they come in contact with a new sample, so rinse well and let dry between samples. Having a queue of several beakers/flasks will make things proceed faster.</li> <li>When the filter is dry (24-48hrs), weigh it and record this weight on the datasheet.</li> </ol>"},{"location":"procedures/soilacidtreatment/","title":"Acid-washing soils to remove carbonates","text":"<ol> <li>This procedure should be done in a fume hood with safety glasses and gloves.</li> <li>Soil samples should be oven dry, homogenized, and finely ground.</li> <li>Prepare a 1N HCl solution in a 500 ml volumetric flask. You will need about 10-15ml of this per soil sample</li> <li>Add deionized water to a 500ml volumetric flask to 5 cm below the junction of the lower bulb and the neck (allowing room for the acid to be added).</li> <li>Using a graduated cylinder, or a glass pipette, slowly add 42ml of 36-38% by weight HCl to the water.</li> <li>Cover the top of the flask and gently swirl the solution until well-mixed.</li> <li>Add deionized water up to the 500ml mark on the neck.</li> <li>Swirl again</li> <li>Label the flask with a sharpie, cover the top, and wrap with Parafilm.</li> <li>Now mix 250ml of diluted solution (0.5N HCl). This will be the primary acid wash.</li> <li>Add 100ml of deionized water to a 250ml volumetric flask.</li> <li>Add 125ml of the 1N HCl to the flask.</li> <li>Fill the flask to the measurement line (250ml) with deionized water.</li> <li>Mix as above.</li> <li>Label the flask with a sharpie, cover the top, and wrap with Parafilm.</li> <li>Label a clean 50ml Erlenmeyer flask with a soil sample id number, and add about 2ml of that soil to the flask. This is roughly 1cm depth in a 2.5x5cm tall sample vial.</li> <li>Add 15ml of the weaker 0.5N HCl solution. It should bubble as CO~2~ is produced (if carbonates are present).</li> <li>Let sit in the fume hood for 8 hours, or overnight. Bubbling should stop.</li> <li>Use pH paper to test the pH of the solution. If it is acidic (below 6.5) then there was sufficient acid to neutralize dissolve the carbonates in the soil. If not, add 5ml of the stronger 1N HCl solution and wait until the reaction is complete.</li> <li>Filter the soil solution through a clean ceramic funnel with a #30 glass filter paper (5.5cm diameter), smooth side up, into a 1-2L filter flask (Erlenmeyer with a sidearm). The sidearm can be attached to a water aspirator or a vacuum to aid filtration.</li> <li>Rinse 5 times with ~10ml of deionized water each time. You an estimate, but be sure the soil is well rinsed.</li> <li>Label a clean petri dish with the correct soil sample id number (this label can be pulled from the Erlenmeyer flask above) and place the filter paper with the rinsed soil in it.</li> <li>Dry all the soil samples in a drying oven at 70\u00b0C overnight, then transfer to a new labelled storage vial.</li> <li>Samples may need to be ground again before loading for EA-IRMS analysis.</li> </ol>"},{"location":"procedures/soilextract_13c/","title":"\u03b413C analysis of soil extracts","text":"<p>A method to measure the \u03b4^13^C value of dissolved organic carbon in soil extracts (fumigated or unfumigated).</p>"},{"location":"procedures/soilextract_13c/#materials","title":"Materials","text":""},{"location":"procedures/soilextract_13c/#chemicals","title":"Chemicals","text":"<ul> <li>H~2~O~2~ solution (30% Hydrogen peroxide, diluted to 10%)</li> <li>FeSO~4~\u20227-H~2~O (Ferrous sulfate heptahydrate)</li> <li>1N H~2~SO~4~ (Sulfuric acid, stronger acid could be diluted)</li> <li>CaCl~2~or BaCl~2~</li> </ul>"},{"location":"procedures/soilextract_13c/#laboratory-equipment-and-consumables","title":"Laboratory equipment and consumables","text":"<ul> <li>Centrifuge suitable for 15ml tubes</li> <li>Balance for weighing chemicals.</li> <li>Volumetric flasks for mixing solutions (volume depends on how many samples you are processing).</li> <li>1 5 ml eppendorf centrifuge tube per sample/standard (mixing and precipitation vessel).</li> <li>Micropipetter (10-100\u00b5l and 100-1000\u00b5l instruments both come in handy).</li> <li>Pipette tips </li> <li>1 Exetainer for each sample (oxidation vessel).</li> <li>1ml syringe and needles</li> </ul>"},{"location":"procedures/soilextract_13c/#liquid-oxidation-and-irms-analysis","title":"Liquid oxidation and IRMS analysis","text":"<p>Initial mixing of the extract and reactant solutions takes place in a 5ml eppendorf centrifuge tube. Once reactants are mixed and sulfate is precipitated out of this solution, it is centrifuged and the supernatant liquid is measured into a glass exetainer for oxidation. Precipitation and centrifugation steps could (maybe) be omitted if cryotrapping is used to purify the CO~2~ prior to IRMS analysis. These are the basic steps, and the mass/volume of extract or reactant used in each step are explained below. I also made a spreadsheet that calculates the extract and reactant amounts for each sample.</p> <ol> <li>Add FeSO~4~ solution to centrifuge tube in required amount. (Optional)</li> <li>Dilute with 1 ml distilled H~2~O (optional)</li> <li>Measure extract volume with desired amount of dissolved C (currently equivalent to 5\u00b5mol C) into centrifuge tube.</li> <li>Add enough acid to create a final pH of ~3 with extract and dilution factored in.</li> <li>Precipitate SO~4~^2-^ using CaCl~2~or BaCl~2~ (Optional)</li> <li>Centrifuge sample to separate extract/reactant solution from sulfate precipitate. (Optional)</li> <li>Measure extract/reactant solution (equivalent to 1\u00b5mol C) to reaction vial</li> <li>Close vial and purge with He gas (Job 14 on PAL)</li> <li>Add H~2~O~2~ or Sodium persulfate reagent.</li> <li>Allow oxidation reaction to complete (~4 hours)</li> <li>Load into GasBench and analyze resulting CO~2~ by IRMS</li> </ol>"},{"location":"procedures/soilextract_13c/#calculate-volume-of-extract","title":"Calculate volume of extract","text":"<p>We wish to analyze roughly 1 \u03bcmol CO~2~ in the mass spectrometer. TOC/TN analysis yielded TOC values in mgC/L. To convert:</p> <p>$$ V = \\frac{1}{(TOC \\times \\frac{0.001mg}{g}) \\times mwC\\^{-1]$$</p> <p>where $V$ is the volume in \u03bcL of extract to oxidize to yield 1 \u03bcmol CO~2~, $TOC$ is the extract C concentration in mg/L, and $mwC$ is the molecular weight of carbon (12.0107). Note that the conversions from L to \u03bcL (/1e6) and mol to \u03bcmol(*1e6) cancel out. This same conversion should be usable for the C3 and C4 standard solutions.</p>"},{"location":"procedures/soilextract_13c/#calculate-amount-of-standard-sugar-raw","title":"Calculate amount of standard sugar (raw)","text":"<p>Sucrose (C~12~H~22~O~11~) has a formula weight of 342.296. The molecular weight of the C in sucrose is 144.128 (12 * 12.0107). So, sucrose is approximately 42.106% C (144.128/342.296). To measure the isotope ratio of pure sucrose, weighing 1mg into a tin should do. To calculate the amount to add to an exetainer for oxidation and IRMS analysis via the gasbench:</p> <p>$$ M = \\frac{342.296}{1 \\times 10\\^6} \\times \\frac{1 mol \\ \\mathrm{sucrose]{12 mol \\ \\mathrm{C] * 1000mg/g = 0.0285mg $$</p>"},{"location":"procedures/soilextract_13c/#calculate-amounts-of-fentons-reagent-and-other-reagents-to-use","title":"Calculate amounts of Fenton's reagent and other reagents to use","text":"<p>We need to oxidize the carbon in the sample as completely as possible. In the Benatti paper, experimentally determined optimum values are given for the ratio of the Fenton's reagent components ([H~2~O~2~]:[Fe++] = 4.5:1), the ratio of Fenton's reagent to oxidizeable C ([H~2~O~2~]:[COD] = 9:1), and reaction pH (4 or less). The highest [H~2~O~2~]:[COD] value tested was 9:1, but other literature suggests that additional H~2~O~2~ will increase the completion of the oxidation reaction. So, to be safe, we'll use these ratios to start</p> <ul> <li>[H~2~O~2~]:[Fe++] = 4.5:1</li> <li>[H~2~O~2~]:[COD] = 12:1, </li> <li>pH = 4 (or less)</li> </ul> <p>For each \u00b5mol of sample C, we need at least 12 \u00b5mol H~2~O~2~ and 2.6 \u00b5mol Fe++ (to get the 4.5:1 ratio).</p> <p>Remember:</p> <p>$$M_i V_i = M_f V_f$$</p>"},{"location":"procedures/soilextract_13c/#hydrogen-peroxide-solution","title":"Hydrogen peroxide solution","text":"<p>We have a 30% H~2~O~2~ (MW = 34.0147) solution with a density of 1.11g/cm^3^. We want a 10% solution. For every ml of this 10% solution there is 0.11g H~2~O~2~, or 0.00326 moles (3.3M solution). Adding 300\u03bcL of this will give 978 \u03bcmol H~2~O~2~. We hope to have at least 80umol C in each reaction vial, so this should be sufficient to oxidize that.</p> <ul> <li>To make 250ml, dilute 83.33ml of this to 250ml with distilled H~2~O.</li> <li>To make 100ml, dilute 33.33ml of this to 100ml with distilled H~2~O.</li> </ul>"},{"location":"procedures/soilextract_13c/#persulfate-reagent","title":"Persulfate reagent","text":"<p>Using roughly the recipe in Doyle et al 2004 (50 g K~2~S~2~O~4~ + 16.8 g NaOH + 30 g H~3~BO~3~ L^-1^), but with sodium persulfate. I adjusted this to 44 g Na~2~S~2~O~4~ + 18.8 g NaOH L^-1^ to raise the pH a little.</p>"},{"location":"procedures/soilextract_13c/#ferrous-sulfate-solution","title":"Ferrous sulfate solution","text":"<p>We want to have about 217\u03bcmol of Fe++ in each reaction vial, as this will be in a 4.5:1 ratio with the H~2~O~2~ (815umol/4.5) that will be added. The molecular weight of hydrated ferrous sulfate (FeSO~4~\u00b77H~2~O) is 382.95478g.</p> <ul> <li>To make 250 ml of a 1M solution, dissolve 95.739 g in 250ml of water. This will yield 1000\u03bcmol/mL.</li> <li>To make 100ml of a 1M solution, dissolve 38.295 g in 100ml water. Again, this yields 1000umol/ml.</li> </ul>"},{"location":"procedures/soilextract_13c/#acid-to-lower-the-ph","title":"Acid to lower the pH","text":"<p>We want a low pH (2-4) before adding the H~2~O~2~. We can use this equation to calculate the amount of acid to use:</p> <p>$$M_1V_1 + M_2V_2 = M_3(V_1 + V_2)$$</p> <p>where $M_1$ is the molarity of the acid, $V_1$ is the volume of the acidic solution, $M_2$ is the molarity of the water, $V_2$ is the volume of the water, and $M_3$ is the target molarity of the end solution. Converting this equation to solve for $V_1$ yields the following equation:</p> <p>$$V_1 = \\frac{(M_3V_2 - M_2V_2)}{(M_1 - M_3)}$$</p> <p>Or if we know the volume to be added but want to calculate its molarity...</p> <p>$$M_2 = -\\frac{M_1V_1 - M_3(V_1 + V_2)}{V_2}$$</p>"},{"location":"procedures/soilextract_13c/#caba-precipitation-solutions","title":"Ca/Ba precipitation solutions","text":"<p>We also need to precipitate all the sulfate out of our solution prior to oxidizing (adding the peroxide). We are adding one mole + 10% extra of Ca or Ba ions for each mole of sulfate in the solution.</p> <p>The molecular weight of calcium chloride (CaCl~2~\u00b72H~2~O) is 147.014g.</p> <ul> <li>To make 250 ml of a 2M solution, dissolve 73.507 g in 250ml of water. This will yield 2umol/uL. </li> <li>Could also do 3M solution: 44.1042g in 100ml</li> </ul> <p>The molecular weight of barium chloride (BaCl~2~\u00b72H~2~O) is 244.28g.</p> <ul> <li>To make 250 ml of a 2M solution, dissolve 127.64 g in 250ml of water. This will yield 2umol/uL.</li> </ul>"},{"location":"procedures/soilextract_13c/#notes-about-running-these-at-sirfer","title":"Notes about running these at SIRFER","text":"<ul> <li>Use the CO2_injection.seq sequence on Saltbush Bill instrument.</li> <li>Rename the job in the spreadsheet that pops up, select that line, click Save and then Start.</li> <li>In the dialoge that appears, rename the job (as above), copy this name to file export, and erase the \"Acquisition\" field from the export.</li> <li>Inject samples - 0.3ml for the 10% CO2 tank, 1ml for the samples. Seems like peaks above 800mV are best</li> <li>Turn on 30 seconds of CO2 wall gas after the last peak has been acquisitioned (top bar in Conflo interface).</li> <li>Stop the acquisition</li> </ul>"},{"location":"procedures/soilextract_13c/#sugar-standards","title":"Sugar Standards","text":"<p>We are using C3 and C4 sucrose (C~12~H~22~O~11~, table sugar) as a standard to test IRMS analysis of the extracts. The molecular weight of sucrose is 342.3, and 42.1% of this is C. To measure the isotope ratios of these sugars, we weighed 1 mg of sucrose (which contains 0.4mg of C) into tins and measured them with EA-IRMS analysis. We found that these C3 and C4 sugars have -24.5 and -11.5 per mil isotope ratios, respectively.</p> <p>\\^Getting the right amount of C from Sucrose\\^\\^\\^\\^ | CO~2~ production | 1 \u03bcmol C | = 12 \u03bcg C | = 28.6 \u03bcg Sucrose | | EA-IRMS | 35.1 \u03bcmol C | = 420 \u03bcg C | = 1000 \u03bcg Sucrose |</p>"},{"location":"procedures/soilextract_13c/#materials-needed","title":"Materials needed","text":"<ul> <li>K~2~SO~4~</li> <li>Sugar from C3 plants (beet sugar)</li> <li>Sugar from C4 plants (cane sugar)</li> <li>500 ml &amp; 50 ml volumetric flasks</li> <li>Weigh paper</li> <li>Labeling tape</li> </ul>"},{"location":"procedures/soilextract_13c/#procedure","title":"Procedure","text":"<ol> <li>Determine expected concentrations to create a test range. For the Niwot-FEF soils, see here.</li> <li>Using similar data gathered by Nicole Trahan we determined a rough range of concentrations of carbon that might be present in our soil samples, and that we would want to test with the sugar standards. Nicole had concentrations ranging from 200-2500 micrograms of carbon per gram of soil. Our extracts have 5 grams of soil per 25 milliliters, so the desired test range we calculated for our soil was 40 \u2013 500 micrograms of carbon per milliliter of extract. We divided the range into five, and included a low value to come up with the test concentrations of 20, 40, 150, 270, 380, and 500 micrograms of carbon.</li> <li>Make 1 L of a 0.5 M K~2~SO~4~solution</li> <li>Add 500 ml of DI water to a 500 ml volumetric flask </li> <li>Weigh out ~ 43.564g of K2SO4 and add to flask</li> <li>Mix using a stir plate</li> <li>Repeat for a total of 1 Liter of solution</li> <li>Exact values used: 43.5643g and 43.5646g</li> <li>Weigh out sugar</li> </ol> <p>Micrograms of Carbon Per Milliliter</p> <p>20 40 150 270 380 500 mg sucrose/ 50ml 2.37 4.75 17.81 32.06 45.13 59.38 actual suc c3 mg/50ml 2.34 4.73 17.84 32.2 45.05 59.32 actual suc c4 mg/50ml 2.42 4.71 17.88 32.07 45.09 59.26 mg C/ 50ml 1 2 7.5 13.5 19 25 actual C c3 mg/50ml 0.98514 1.99133 7.51064 13.5562 18.96605 24.97372 actual C c4 mg/50 ml 1.01882 1.98291 7.52748 13.50147 18.98289 24.94846</p> <p>Add sugar to 50 ml of potassium sulfate solution There should be a total of 12, 50 ml potassium sulfate solutions each with a distinct concentration of either c3 or c4 sugar Evaporate solutions in drying oven</p>"},{"location":"procedures/soilextract_13c/#references","title":"References","text":"<ul> <li>Benatti, C. T., C. R. G. Tavares, and T. A. Guedes (2006), Optimization of Fenton\u2019s oxidation of chemical laboratory wastewaters using the response surface methodology, Journal of Environmental Management, 80(1), 66\u201374, doi:10.1016/j.jenvman.2005.08.014.</li> <li>Benatti, C. T., C. R. G. Tavares, and E. Lenzi (2009), Sulfate removal from waste chemicals by precipitation, Journal of Environmental Management, 90(1), 504\u2013511, doi:10.1016/j.jenvman.2007.12.006.</li> </ul>"},{"location":"procedures/treecores/","title":"Laboratory mounting of tree cores","text":"<ul> <li>Reassemble the core on a sheet of white paper, making sure that the edges slot together.</li> <li>Mount the core to a piece of heavy, corrugated cardboard. </li> <li>Make sure that the wood fibers are oriented perpendicular to the field of view.</li> <li>Use water-soluble glue to attach the core to the cardboard piece. While drying, firmly keep it in place using scotch tape.</li> </ul>"},{"location":"procedures/treecores/#measuring-the-trees-age","title":"Measuring the tree's age","text":"<ul> <li>Count the number of rings  visible. Rings are characterized by a light part on the inside and a dark part on the outside. This renders them more visible.</li> <li>Use diameter of tree to figure out if the core has reached the center of the tree (marked by a oval shaped ring). If not, then determine the average number of rings per inch, and extrapolate in order to figure out how many rings would approximately be in the remaining space. If you had to do this, then mark on the data sheet.</li> <li>Put a ruler next to the core.</li> <li>Measure the distance between rings, and record on data sheet.</li> <li>Compare this data to that of other tree cores to find out more precisely which year each ring was formed (this is known as cross-dating). This is best if done with at least one core that reached the center.  -More specifically, compare the number of rings and look for specific patterns across the cores such as a narrower band of rings. This should allow comparison of age, and can increase the accuracy of the approximate age given by counting tree rings (approximate due to false or missing rings).</li> </ul>"},{"location":"procedures/xylempressure/","title":"Measuring Xylem Water Potential","text":"<p>Measurements of xylem water potentials can be made during the growing season in an effort to understand the seasonal pattern of soil moisture and tree water status under natural or experimentally manipulated conditions.</p> <p>Relevant links:</p> <ul> <li>Hidden Canyon ecohydrology(water balance and snowmelt) experiment page.</li> <li>The Hidden Canyon carbon cycle, which may be influenced by soil or plant water potentials.</li> </ul>"},{"location":"procedures/xylempressure/#objectives","title":"Objectives","text":"<ul> <li>To measure the predawn and midday xylem water potential of trees during the growing season.</li> <li>To identify the timing and magnitude of any periods of water stress.</li> <li>To measure the differences in growing season plant and soil water status between trees growing under different conditions.</li> </ul>"},{"location":"procedures/xylempressure/#methods","title":"Methods","text":"<p>Xylem water potentials of are measured using a pressure the pressure bomb method using a PMS instrument. All measurements are made on Abies lasiocarpa twigs from trees in a particular size class (large trees in 2010, probably saplings in the future). Twigs are excised from south-facing (downhill) branches at a consistent height above the ground. These twigs are stripped of phloem at the cut end and measured in the chamber within 10 minutes of being cut. Midday measurements are made between 1 and 5pm, and predawn measurements are made between 4 and 7am on consecutive days. Days immediately following rain events are avoided.</p> <p>FIXME - add an actual procedure</p>"},{"location":"redbutte/weather_log/","title":"Weather log","text":""},{"location":"redbutte/weather_log/#red-butte-weather-station-logbook-fall-2011-to-present","title":"Red Butte Weather Station Logbook (Fall 2011 to present)","text":"<p>Maintenance log and visits to Red Butte weather stations:</p> <p>(also see:)</p> <p>2-03-2014</p> <ul> <li>Modified new WS4 file (original data in WS4_140115_orig.dat):</li> <li>Changed errors (6999, -6999) to NaN</li> <li>Removed lines that were clear errors and changed dates back to 2013/14 (lines 174-end)</li> <li>Modified new WS6 file (original data in WS6_140115_orig.dat):</li> <li>Changed errors (6999, -6999) to NaN</li> <li>Added a column filled with 6 that is missing (a glitch in the new program).</li> <li>Removed a day that had a duplicate record (day 325)</li> <li>Removed lines that were clear errors and changed dates back to 2013/14 (lines 176-end)</li> <li>Plotted the data collected on 1/11/2014 and 1/15/2014</li> <li>WS1 (in the garden)<ul> <li>There was a power outage on day 341 (Dec 7, 2013) and all data is missing after that.</li> <li>T/RH = new sensor seems to be working fine</li> <li>Wind sensor 1 = still not working</li> <li>Wind sensor 2 = ok, and this has windspeed and wind direction</li> <li>Rain gauge = ok</li> <li>PAR = ok (ten minute only)</li> <li>Battery = ok? I suspect the battery is not functioning because nighttime periods are not recorded (no solar panel input?)</li> </ul> </li> <li>WS2 (by the reservoir)<ul> <li>T/RH = ok</li> <li>Wind sensor = New sensor working fine</li> <li>Rain gauge = still ok</li> <li>Battery = losing capacity, low and erratic voltages - needs to be replaced.</li> </ul> </li> <li>WS4 (halfway up canyon)<ul> <li>There was a power outage that reset the date around Nov 19, 2013 and it looks like data resumed a bit in early Jan (2014).</li> <li>T/RH = ok</li> <li>Wind sensor = OK</li> <li>Rain gauge = not working.</li> <li>Battery = losing capacity, low and erratic voltages - needs to be replaced.</li> </ul> </li> <li>WS6 (top of Parleys fork)<ul> <li>There was a power outage that reset the date around Dec 3, collection resumed, then stopped around Jan 1, 2014.</li> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = ok</li> <li>Battery = losing capacity, low and erratic voltages - needs to be replaced.</li> </ul> </li> <li>TO DO:</li> <li>Check WS1 and replace battery (probably)</li> <li>Fix WS6 program (missing fourth column in data output should be WS #)</li> </ul> <p>1-20-2014</p> <ul> <li>Downloaded data from all storage modules and uploaded to Dropbox</li> <li>Storage modules are cleared,tested, and have programs loaded.</li> </ul> <p>1-15-2014</p> <ul> <li>Pulled storage module from WS4 and WS6</li> <li>WS 4 initially seemed to be unresponsive, but after testing voltage it again seemed fine.</li> <li>Set the clocks for both.</li> <li>All 4 stations have modules with the program loaded in location 8.</li> </ul> <p>1-11-2014</p> <ul> <li>Pulled storage module from WS1 and WS2</li> <li>WS1 seemed to not be collecting data at the time, so I reloaded the program and it then seemed alright.</li> </ul> <p>7-6-2013</p> <ul> <li>Replaced wind sensor at WS2 with a replacement from an Ehleringer storage cabinet. Seems to work fine (Met One 014C).</li> <li>Sensor with the broken bearing was labeled and put back in the Ehleringer cabinet where the replacement came from.</li> </ul> <p>6-7-2013</p> <ul> <li>Replaced T/RH sensor on WS1 with an HMP45AC that susan found.</li> <li>I don't know much about this sensor (when it was last calibrated, etc.), and I used the standard multiplier and offset (in Campbell manual).</li> <li>Uploaded the new datalogger program to read this sensor - seems to be working</li> <li>The new program is in the project Dropbox (130606 version).</li> <li>Swapped the storage module to get data from 5-23 to today.</li> <li>The file for this period is in the Dropbox, and the data haven't been added to the aggregated files yet.</li> </ul> <p>5-31-2013</p> <ul> <li>Visited WS1</li> <li>The Vaisala HMP35 is broken - one wire lead for the thermistor is sheared off.</li> <li> <p>Temporarily put an old HMP35 (found at garden) on WS1 and removed the current one for inspection.</p> </li> <li> <p>Visited WS4 &amp; 6</p> </li> <li>Installed new storage modules</li> <li>Transferred datalogger programs from new modules to dataloggers</li> <li>Reset clocks</li> <li>Data is being collected again at these sites, and future power outages shouldn't cause a problem (programs are in location 8 in the SMs).</li> </ul> <p>5-29-2013</p> <ul> <li>Changed the date for all WS6 data files (that I have collected)</li> <li>I did this based on storage module dates - so I know the start/stop dates for each file</li> <li>As usual - there are <code>_orig</code> files in the directory that haven't been changed</li> <li>Plotted the data collected on 5-3 and 5-23</li> <li>WS1 (in the garden)<ul> <li>There was a power outage that reset the date over the winter - I fixed this in the datafile (and there is a <code>_orig</code> version saved).</li> <li>T/RH = Seems to have broken on 1-5-2013. It is reading a negative number all the time</li> <li>This affects the vapor pressure measurements also</li> <li>Wind sensor 1 = still not working</li> <li>Wind sensor 2 = ok, and this has windspeed and wind direction</li> <li>Rain gauge = ok</li> <li>PAR = ok (ten minute only)</li> <li>Battery = ok? (:!: This is also reading ~-46 like the T sensor, but I checked it with a multimeter on 5-23-2013 and it was ok - ~13V)</li> </ul> </li> <li>WS2 (by the reservoir)<ul> <li>T/RH = ok</li> <li>Wind sensor = Data are flat for the last 1-2 months. Dave E confirms that the sensor has frozen up (bearing?) and it will need to be replaced.</li> <li>Rain gauge = ok for the last year, though it looks like it had problems before that</li> <li>Battery = ok (still losing capacity, but voltage is &gt; 12)</li> </ul> </li> <li>WS4 (halfway up canyon)<ul> <li>There was a power outage that reset the date over the winter</li> <li>Only a few days of data were collected - all sensors looked ok in this short time, except maybe the rain gauge.</li> </ul> </li> <li>WS6 (top of Parleys fork)<ul> <li>There was a power outage that reset the date over the winter.</li> <li>Only a few days of data were collected, and data look ok in this short time.</li> <li>The date is wrong for this WS, and the last few datafiles - this was adjusted using dates of collection.</li> <li>:!: NOTE that there is a <code>_orig</code> file with unchanged data (datalogger dates say 2004).</li> </ul> </li> <li>TO DO:</li> <li>Swap storage modules for WS2-6 with modules that have programs loaded in location 8.<ul> <li>Load these on to 4 &amp; 6 after they are swapped</li> </ul> </li> <li>Test and replace T-RH unit on WS1</li> </ul> <p>5-23-2013</p> <ul> <li>Revisited WS1 and WS4.</li> <li>Cut the lock off of WS1 (never found the key), and swapped the storage module.</li> <li>There are some problems with WS1 data.</li> <li>There was a power outage that reset the date over the winter - I fixed this in the datafile (and there is a _orig version saved).</li> <li>Data from the temperature sensor is bad and it is affecting vapor pressure measurements.</li> <li>The battery voltage is reading ~-46 - this may be related to the temperature problem.</li> <li>Checked the WS4 battery and solar panel voltages and downloaded the running program from the CR10.</li> <li>Battery and solar panels seem fine.</li> <li>The running program looked invalid - was probably erased in a power outage.</li> <li>REMEMBER - When swapping out modules, store the datalogger program in location 8 (see here).</li> </ul> <p>5-3-2013</p> <ul> <li>Visited WS2, 4, &amp; 6 to swap storage modules</li> <li>WS4 &amp; 6 were not collecting data, and appeared to have lost power (and their programs)</li> <li>The data collection stopped soon after the fall 2012 visit.</li> <li>WS6 Still thinks it is 2031 - need to reset clock</li> <li>WS2 seems OK.</li> <li>WS1 was was inaccessible because the key was missing from the usual place.</li> </ul> <p>1-8-2013</p> <ul> <li>Went to WS1 (Garden) and downloaded all data (Austin needed early Nov. data)</li> <li>Reset clock on datalogger - it had reset itself to 4 years earlier for some reason. Date of this change was sometime after the last collection (early Nov).</li> </ul> <p>11-2-5-2012</p> <ul> <li>Visited weather stations 1-4 and swapped out storage modules.</li> <li>Took photos and checked wiring.</li> <li>Plotted data</li> <li>WS1 (in the garden)<ul> <li>T/RH = ok</li> <li>Wind sensor 1 = not working</li> <li>Wind sensor 2 = ok, and this has windspeed and wind direction</li> <li>Rain gauge = ok</li> <li>PAR = ok (ten minute only)</li> <li>Battery = ok? (:!: Big drop in voltage, but everything seems to be working ok - not sure I believe the drop)</li> </ul> </li> <li>WS2 (by the reservoir)<ul> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = ok for the last year, though it looks like it had problems before that</li> <li>Battery = ok (still losing capacity, but voltage is &gt; 12)</li> </ul> </li> <li>WS4 (halfway up canyon)<ul> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = looks like my fix helped, but it may be down again</li> <li>Battery = capacity is better now, but there may be a bad connection and a slow decline in capacity.</li> </ul> </li> <li>WS6 (top of Parleys fork)<ul> <li>This datalogger is having problems, looks like a short may be resetting the date. Most of the data seems intact though.</li> <li>NOTE that I doctored the data file (using my best judgement) to remove bad data. There is a raw data file marked <code>_orig</code> on the data download site that contains the un-doctored data. Missing or removed data are marked as \"NaN\" in all other files.</li> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = ok</li> <li>Battery = battery capacity back up to 12v? I think there must be a short somewhere.</li> </ul> </li> </ul> <p>4-9-2012 Plotted 4/4 weather station data today:</p> <ul> <li>WS1 (in the garden)</li> <li>T/RH = ok</li> <li>Wind sensor 1 = not working</li> <li>Wind sensor 2 = ok, and this has windspeed and wind direction</li> <li>Rain gauge = ok</li> <li>PAR = ok</li> <li>Battery = ok (losing capacity?)</li> <li>WS2 (by the reservoir)</li> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = ok for the last year, though it looks like it had problems before that</li> <li>Battery = ok (losing capacity?)</li> <li>WS4 (halfway up canyon)</li> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = still broken, but may have been fixed at last visit</li> <li>Battery = abruptly lost capacity (down to ~10V), seems to power the sensors though</li> <li>WS6 (top of Parleys fork)</li> <li>Datalogger needs time reset (thinks it is 2003)</li> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = ok</li> <li>Battery = low capacity (down to 9-10V), seems to power the sensors though</li> </ul> <p>4-4-2012</p> <ul> <li>Visited weather stations 1-6 and swapped out storage modules.</li> <li>Did some taping of wiring at weather stations 2 and 4 (insulation on wires is cracking)</li> </ul> <p>11-28-2011 - Greg and Susan visited WS6</p> <ul> <li>Uploaded WS6_111122.dld program from the laptop</li> <li>Verified that all sensors were working - all were, including the T/RH and precip gauge.</li> <li>Replaced T/RH sensor shelter with a new one and realligned/reattached precip gauge.</li> </ul> <p>11-21-22-2011</p> <ul> <li>SC32A jumper set as per CS instructions. It should be possible to connect to a CR10 with this in the field.</li> <li>Rebuilt WS6 program from Dropbox using Edlog (Document DLD File). The most recent file was called WS2_071030.dld. Leaving T/RH calibrations (multiplier and offsets) alone even though it is likely we will replace this sensor. May have to do a calibration of all sensors in spring.</li> <li>WS6_111122.CSI is now on Dropbox</li> <li>Went through calibration data on Dropbox - Looks like all T/RH sensors were calibrated (except maybe #4)in the recent past using a dewpoint generator.</li> </ul> <p>11-16-2011 - Plotted data from all sensors in matlab to look         at what sensors are working. This is what I found:</p> <ul> <li>WS1 (in the garden)</li> <li>T/RH = ok</li> <li>Wind sensor 1 = not working</li> <li>Wind sensor 2 = ok, and this has windspeed and wind direction</li> <li>Rain gauge = ok</li> <li>PAR = ok, though there is a weird seasonal jump in PAR (by about 300\u03bcmol m^-2^ s^-1^ in the middle of summer)</li> <li>Battery = ok (no charging problems)</li> <li>WS2 (by the reservoir)</li> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = ok for the last year, though it looks like it had problems before that</li> <li>Battery = ok (no charging problems)</li> <li>WS4 (halfway up canyon)</li> <li>T/RH = ok</li> <li>Wind sensor = ok</li> <li>Rain gauge = broken for the last 1.5 years</li> <li>Battery = seems to be losing capacity over time and may need replacement soon</li> </ul> <p>10-7-2011</p> <ul> <li>Was able to download data from all 3 returned storage modules (WS1, 2, &amp; 4)</li> <li>See procedure here</li> <li>Uplodaded data files to project Dropbox</li> </ul> <p>10-5-2011 - Susan and Greg visited WS6</p> <ul> <li>T/RH and rain sensors are both bent out of shape</li> <li>Station appears to have lost power and there is no program running.</li> </ul> <p>9-30-2011 - Collected storage module from WS1 at around 5:30pm</p> <p>9-27-2011 - Susan and Greg collected storage modules from WS2 &amp; 4, did not reach WS6.</p>"},{"location":"redbutte/weatherstations/","title":"Red Butte Canyon Weather Stations","text":"<p>This document describes the long-term weather stations located in Red Butte Canyon.</p> <p>Originally, this info was taken from /RedButte/RBCWeatherStations.doc on the project Dropbox, last modified on 11/5/2007. It has been subsequently changed to reflect recent activity.</p>"},{"location":"redbutte/weatherstations/#history","title":"History","text":"<p>For a history of these stations, begin by reading:</p> <p>Ehleringer et al. 1992. Red Butte Canyon Research Natural Area: History, Flora, Geology,Climate, and Ecology. Great Basin Naturalist 52(2), pp95-121.</p> <p>The present weather stations have been running since 1982 but are positioned in the same locations as much older weather stations. At this time, the historical weather station data (prior to 1982) have not been aggregated, quality checked, and made available to the public.</p>"},{"location":"redbutte/weatherstations/#weather-station-site-metadata","title":"Weather Station Site Metadata","text":"<p>\\^ Station \\^ Location description \\^ Latitude \\^ Longitude \\^ Elevation \\^ Historical data span \\^ Current data span \\^ | RB #1 | Biology Experimental Garden| 40\u00b0 46' | 110\u00b0 50' |1515 m | 1942 - 1964 | 1991 - present | | RB #2 | Head of RB Reservoir | 40\u00b0 47' | 111\u00b0 48' | 1653 m | 1942 - 1964 | 1982 - present | | RB #3 | Along RB Creek at Brush Basin | 40\u00b0 48' |111\u00b0 47' | 1865 m |1942 - 1952 | Retired | | RB #4 | Along RB Creek 100 m west of Beaver Cyn | 40\u00b0 48' | 111\u00b0 46' | 1890 m | 1942 - 1971 | 1982 - present| | RB #5 | Parleys Fork 100 m above inlet to RB Creek | 40\u00b0 47' | 111\u00b0 48' | 1753 m | 1942 - 1956 | Retired | | RB</p>"},{"location":"redbutte/weatherstations/#6-top-of-elk-fork-40-49-111-46-2195-m-1946-1971-1982","title":"6 | Top of Elk Fork | 40\u00b0 49' | 111\u00b0 46' | 2195 m | 1946 - 1971 | 1982","text":"<ul> <li>present | More information on geology, hydrology, ecology, vegetation, and other aspects of the canyon can be found at the Red Butte Canyon Research Natural Area website.</li> </ul>"},{"location":"redbutte/weatherstations/#recent-status-of-the-stations","title":"Recent Status of the Stations","text":"<p>Weather stations 1,2,4, and 6 are currently running. All four collect daily data for temperature, relative humidity, wind speed, and rainfall. Weather station 1 also collects data every ten minutes for all above mentioned sensors as well as for wind direction and photosynthetically active radiation (PAR).</p>"},{"location":"redbutte/weatherstations/#log-books","title":"Log books","text":"<p>Log books are maintained for each weather station and, as of 11/5/2007, these were located in ASB room 510 in the eastern most cubicle. These, however, only extend back to 2000 despite efforts to locate any historical documentation of these stations. Each log book has a copy of a topo map with the approximate location marked.</p>"},{"location":"redbutte/weatherstations/#online-maintenance-log","title":"Online maintenance log","text":"<p>There is an up-to-date maintenance log here.</p>"},{"location":"redbutte/weatherstations/#recent-sensor-history-1982","title":"Recent sensor history (&gt;1982)","text":"<p>The begin date of sensors on the current weather stations is shown here. Data could be present up to present, except for occasional gaps in the record. Some of this info comes from the README.TXT file in the CurrentData folder of the RBWS Dropbox.</p> <ul> <li>Weather station 1:</li> <li>02 Apr, 1992 - Begin air temp and precip sensors</li> <li>30 Jun, 1995 - Begin wind sensor</li> <li>08 Mar, 2000 - Begin RH and vapor pressure sensors</li> <li>Jun 2004 - Order of column headings changes, begin hourly data output</li> <li>22 May, 2005 - Change hourly data output to 10 minute data output, add PAR sensor</li> <li>Weather station 2, 4, 6:</li> <li>11 Aug, 1982 - Begin air temp, wind speed, PAR(?), and vapor pressure ( :!: see note above about problems with vp) sensors</li> <li>01 Jun, 1991 - Light measurements seem to end around this time for all stations.</li> <li>07 Apr &amp; 14 May, 1992 - Begin WS2 &amp; 4 precip sensor</li> <li>26 Apr, 1993 - Begin WS6 precip sensor.</li> <li>June 2004 - Order of column headings change</li> </ul>"},{"location":"redbutte/weatherstations/#missing-data","title":"Missing data","text":"<ul> <li>Oct 29, 2007 -&gt; May 26, 2009 (all data, WS2, 4, 6) - :?: The last data collection prior to Fall 2011 appears to have been in late 2007. Is there a missing datafile between these dates? Was available memory on the storage module exceeded?</li> <li>Nov 10, 2012 -&gt; May 31, 2013 (all data, WS4, 6) - The datalogger programs were erased, probably during a power outage. This is a hard lesson that programs need to be loaded on replacement storage modules (see the \"preventing data loss\" section here).</li> </ul>"},{"location":"redbutte/weatherstations/#current-data","title":"Current Data","text":"<p>These data are made available to the general public at http://ecophys.utah.edu/download/Red_Butte_Weather/. Some summary data is viewable on the RBC website</p> <p>Currently, the daily data for all weather stations for all years are stored in a single file called <code>RBWSall_yymmdd.txt</code> where the yymmdd is the most recent data collected. This daily file is also saved as an excel file that can be found online. Historically, the data were saved by year. These data can be found in an archived folder online as excel files in their original state (http://ecophys.utah.edu/download/Red_Butte_Weather/YearlyDataFiles/). After compiling the data for all years and all stations into one file and plotting all data, it was discovered that all relative humidity data (or vapor pressure as it were) were logged incorrectly between 1992 and 2000. These data have been removed from the currently updated data file. Peruse the <code>Quality Control of Web Files</code> folder located in the project Dropbox for more on this issue.</p>"},{"location":"redbutte/weatherstations/#equipment-and-maintenance-procedures","title":"Equipment and maintenance procedures","text":"<p>Weather stations are visited twice yearly (spring and fall), or more often as necessary. As of 3/5/2012, a tool box containing all necessary storage modules, keypads, tools and manuals could be found in the Bowling Lab, fourth floor of ASB.</p>"},{"location":"redbutte/weatherstations/#station-description","title":"Station Description","text":"<p>Each weather station consists of a tripod, enclosure, Campbell CR10 (or CR10x at WS1) datalogger, solar panel, battery/power supply, storage module, and a number of sensors (which vary slightly between sites).</p>"},{"location":"redbutte/weatherstations/#site-visit-protocol","title":"Site Visit Protocol","text":"<p>Materials list</p> <ul> <li>A tested, empty storage module for each station. Make sure it has the appropriate datalogger program loaded into location 8.</li> <li>CR10KD keypad</li> <li>Voltage/Multimeter for testing solar and battery voltages.</li> <li>Labeling tape and sharpie for labeling modules as they are swapped out.</li> <li>Screwdrivers, including a small standard (Campbell issue ones are easy to find)</li> <li>Electrical tape</li> <li>Any sensor hardware that needs to be replaced</li> <li>Extra serial cable (blue, three ended ones)</li> </ul> <p>Upon visiting each site, one should write the date of visitation in the log book along with the datalogger time, day of year, and year. With the keypad plugged in, press <code>\\*5</code> to view the time, then press <code>A</code> to view the day of year, then <code>A</code> to view year. One should also write down all currently measured values by pressing <code>\\*6</code> and then <code>A</code> to advance through each value. Once the integrity of the station has been established by visual inspection and datalogger inspection, remove the existing storage module and replace it with one that has been erased and tested in the lab.</p> <p>IMPORTANT NOTE - Remember to load the weather station program into location 8 of the replacement storage module. This will keep the datalogger from forgetting its program after a power outage. See Preventing Data Loss.</p> <p>Technical details on working with these dataloggers and storage modules can be found here.</p>"},{"location":"redbutte/weatherstations/#datalogger-programs","title":"Datalogger Programs","text":"<p>The current programs running on each weather station can be found on the project Dropbox at: RedButte/CR10_Programs/Current Programs. One can also obtain the program straight from the datalogger by dumping it to the storage module using the appropriate keystrokes\u2026see datalogger manual.</p>"},{"location":"redbutte/weatherstations/#data-reduction","title":"Data Reduction","text":"<p>After all storage modules have been collected, they should be downloaded using some version of Campbell Scientific software (currently LoggerNet). All data should be plotted to check for integrity before pasting into the <code>RBWSall_yymmdd</code> file and uploading to the web.</p>"},{"location":"snotelC/fieldprotocol/","title":"SNOTEL site sampling procedures","text":"<p>Conifer forests adjacent to SNOTEL sites are visited in the summer for extensive belowground (and aboveground) carbon stock sampling. Equipment lists and sample collection procedures are detailed below.</p>"},{"location":"snotelC/fieldprotocol/#site-selection","title":"Site selection","text":"<p>Mature conifer forests with minimal evidence of fire, harvest, or other disturbances are selected for soil and aboveground carbon stock sampling. These forests must be within a 1 km radius and 100m elevational distance of the SNOTEL site. SNOTEL sites are selected to represent some aspect of the climatic, topographical, and snowpack variability present in forests of the western U.S. More info on site selection can be found ???</p>"},{"location":"snotelC/fieldprotocol/#snotel-soil-survey-supply-list-for-each-site-visited","title":"SNOTEL soil survey supply list for each site visited","text":"<ul> <li>GPS</li> <li>Camera</li> <li>15cm^2^quadrat</li> <li>Soil knife</li> <li>Spatula or trowel</li> <li>DBH tape</li> <li>Plot Meter tape</li> <li>Compass and inclinometer</li> <li>Tent stake</li> <li>Tree corer and core tin (or envelopes)</li> <li>Small soil corer</li> <li>Pruners</li> <li>Rubber mallet</li> <li>Shovel</li> <li>6 soil tins (when intact cores are not being collected)</li> <li>15 paper lunch bags</li> <li>7 paper grocery bags</li> </ul> <p>Optional equipment</p> <ul> <li>3 soil gas cans &amp; sampling tubes</li> <li>2x4 block?</li> <li>Soil corer?</li> <li>15 4\" core liners and caps (2 each)</li> </ul>"},{"location":"snotelC/fieldprotocol/#samples-to-be-collected","title":"Samples to be collected","text":"<p>FIXME --- this is in need of an update.</p> <ul> <li>3 litter bags - 1 per plot, each is bulk of 5, 15x15cm samples</li> <li>This sample includes all surface leaf litter and any surface fine (&lt;5cm) CWD within the quadrat.</li> <li>15 O-horizon samples - 5 per plot, 15 or 7.5 sq cm, depending on depth</li> <li>These are cut out of the organic layer with a knife, roots and CWD are clipped out and</li> <li>3 mineral soil (to 10cm) bags - 1 per plot, each is bulk of 5, 10cm samples</li> <li>3 soil gas samples</li> <li>6 bulk density tins OR</li> <li>3-5 intact core sets, collected to at least 10cm depth from first mineral horizon.</li> <li>10 tree cores - 1 each from plots (largest tree) and 7 more from mature trees in stand.</li> </ul>"},{"location":"snotelC/log1/","title":"Log1","text":""},{"location":"snotelC/log1/#snotel-survey-site-visits","title":"SNOTEL survey site visits","text":"<p>See also: The Sample processing log, and sample tables for tracking samples.</p> <p>No sites were visited in 2010</p>"},{"location":"snotelC/log1/#2012","title":"2012","text":"<ul> <li>Sept 5 Little Bear visit - There are no conifers directly around the SNOTEL, so I visited a site around 6km to the WNW. All normal soil samples plus several leaf samples, tree cores, and 9 intact cores.</li> <li>Aug 17 Smith and Morehouse visit - All normal soil samples plus 3 needle samples, and 8 intact cores. The tree corer jammed on the first core, so I was unable to collect a usable set of tree cores.</li> <li>Aug 7 Snowbird visit - All normal soil samples plus 9 tree cores, 3 needle samples, and 8 intact cores. Intact cores were difficult to extract and most are incomplete. Gave up on many of the deeper ones.</li> <li>July 30-31 Hole in Rock visit - All normal soil samples plus 9 tree cores, 3 needle samples, and 8 intact cores. Intact cores were difficult to extract and most are incomplete. Could not extract one for Plot 3 20-30cm.</li> </ul>"},{"location":"snotelC/log1/#2011","title":"2011","text":"<ul> <li>August??? - Revisit to Ben Lomond Trail for soil cores, tree cores, needles. I forget which day I actually did this.</li> <li>** Aug 16-18 ** - Did Mosby Mountain and Chepeta (Eastern Uintas, south slope), and revisited Rock Creek SNOTEL to collect soil cores, tree cores, and needles.</li> <li>Aug1 - Hayden Fork and Trial Lake revisits. Collected tree cores (2 per plot at HF, 3 at TL), intact cores (0-10cm, 10-20cm, 20-30cm at each plot), and needles (bulked 3 trees per plot).</li> <li>2 or more of the cores from HF were from recently dead trees, and many of the trees in this stand are dead now due to beetles.</li> <li>Trial lake P2 core 20-30 is incomplete</li> <li>Noticed many large tree stumps at Trial Lake (these are quite old, just not sure if I noticed them last time)</li> <li>July 27 - Ben Lomond Peak revisit, with Richard. Sampled a 4th plot - all normal samples (including tree cores and intact cores). Also revisited plot 1 &amp; 2 to collect needles, cores, and intact cores (3 at each plot).</li> <li>July 19 - Timpanogos Divide, with Richard and Davis. All normal soil samples, plus 9 tree cores, 8 intact cores, and 6 tins.</li> </ul> <p>\\^ Site name \\^ Date \\^ Plots \\^ Leaves \\^ Litter \\^ Org \\^ Min 0-10 \\^ Tin1 \\^ Tin2 \\^ Soil Cores \\^ Tree cores \\^ Notes\\^ \\^ Ben Lomond Trail | Aug ??, 2011 | 1-3 | x | | | | | | x | x | Revisit - added soil/tree cores, needles | \\^ Rock Creek | Aug 18, 2011 | 1-3 | x | | | | | | x | x | Revisit - added soil/tree cores, needles | \\^ Chepeta | Aug 18, 2011 | 1-3 | x | x | x | x | | | x | x | | \\^ Mosby Mtn. | Aug 17, 2011 | 1-3 | x | x | x | x | | | x | x | | \\^ Trial Lake | Aug 1, 2011 | 1-3 | x | | | | | | x | x | Revisit - added soil/tree cores, needles | \\^ Hayden Fork | Aug 1, 2011 | 1-3 | x | | | | | | x | x | Revisit - added soil/tree cores, needles | \\^ Ben Lomond Pk. | July 27, 2011 | 1,2,4 | x | x | x | x | | | x | x | Revisit - added plot 4, soil/tree cores, needles in plots 1 &amp; 2 | \\^ Timpanogos Divide | Jul 19, 2011 | 1-3 | x | x | x | x | x | x | x | x | |</p>"},{"location":"snotelC/log1/#2009","title":"2009","text":"<p>\\^ Site name \\^ Date \\^ Plots \\^ Leaves \\^ Litter \\^ Org \\^ Min 0-10 \\^ Tin1 \\^ Tin2 \\^ Soil Cores \\^ Tree cores \\^ Notes\\^ \\^ Louis Meadow | 9/2/2009 | 1-3 | x | x | x | x | 1 | 1 | | x | Missing some tins | \\^ Lookout Peak | 8/25/2009 | 1-3 | x | x | x | x | 2 | 2 | | x | Missing some tins | \\^ Payson Rs. | 8/21/2009 | 1-3 | x | x | x | x | | | | x | No tins | \\^ Monte Cristo | 8/13/2009 | 1-3 | x | x | x | x | x | x | | x | | \\^ Hayden Fork | 7/13/2009 | 1-3 | x | x | x | x | x | x | | x | | \\^ Clear Creek 2 | 7/10/2009 | 1-3 | x | x | x | x | x | x | | x | | \\^ Beaver Divide | 7/9/2009 | 1-3 | x | x | x | x | x | x | | x | |</p>"},{"location":"snotelC/log1/#2008","title":"2008","text":"<p>Note that all plot Ohz and Min 0-10 samples came from 5 quads per plot (6 in 2009 on)</p> <p>\\^ Site name \\^ Date \\^ Plots \\^ Leaves \\^ Litter \\^ Org \\^ Min 0-10 \\^ Tin1 \\^ Tin2 \\^ Soil Cores \\^ Tree cores \\^ Notes\\^ \\^ Farmington Upper | 10/9/2008 | 1-3 | | x | x | x | | | | | | \\^ Farmington Lower | 10/3/2008 | 1-3 | | ? | x | x | | | | | No quad litter samples | \\^ Ben Lomond Pk. | 9/21/2008 | 1-3 | | x | x | x | | | | | | \\^ Ben Lomond Tr. | 9/20/2008 | 1-3 | | x | x | x | | | | | | \\^ Brown Duck | 9/18/2008 | 1-3 | | x | x | x | | | | | | \\^ Clear Creek 1 | 9/11/2008 | 1-3 | | x | x | x | | | | | | \\^ Trial Lake. | 9/4/2008 | 1-3 | | x | x | x | | | | | | \\^ Rock Creek | 8/31/2008 | 1-3 | | x | x | x | | | | | |</p>"},{"location":"snotelC/overview/","title":"Overview","text":"<p>FIXME</p>"},{"location":"snotelC/overview/#western-us-montane-soil-carbon-cycling","title":"Western U.S. montane soil carbon cycling","text":"<p>Soil samples are collected from montane and subalpine forests around the U.S. along with descriptive information about that forest.</p>"},{"location":"snotelC/overview/#some-research-objectives","title":"Some research objectives","text":""},{"location":"snotelC/overview/#related-projects","title":"Related projects","text":"<ul> <li>The SNOTEL soil climate project.</li> </ul>"},{"location":"snotelC/overview/#hypotheses","title":"Hypotheses","text":""},{"location":"snotelC/overview/#experimental-design","title":"Experimental design","text":""},{"location":"snotelC/overview/#design-changelog","title":"Design changelog","text":""},{"location":"snotelC/overview/#methods","title":"Methods","text":"<ul> <li>Growing season soil respiration with the Li-Cor 6400 system.</li> <li>Below-snow soil respirationmeasurements made with the inlet/tubing system and exetainers.</li> <li>Litterbag mass lossmeasurement of winter and summer decomposition.</li> </ul>"},{"location":"snotelC/sampleprocessing/","title":"Lab processing of SNOTEL site samples","text":"<p>3 circular plots (10m diameter) are sampled in each forest. 6 soil sampling locations are randomly selected in each plot. Note that in 2008 soil sampling happened at only 5 locations per plot and there were no bulk density measurements.</p> <p>See the SNOTEL field sampling page for more on sampling methods</p> <p>Samples returned</p> <ul> <li>3 litter bags --- 1 per plot, aggregates of 6 sampled locations per plot (15x15cm quads - 225cm^2^).</li> <li>3 O-horizon samples --- 1 per plot, aggregate of 6 sampled locations per plot. These may be in multiple bags. Area sampled varies but is known.</li> <li>3 Mineral soil (0-10cm depth) --- 1 per plot, aggregate of 6 sample locations per plot.</li> <li>3 bulk density tins (0-10cm depth) --- 1 per plot, absent at extremely rocky locations.</li> <li>3 bulk density tins (10-20cm depth) --- 1 per plot, absent at extremely rocky locations.</li> <li>9 tree increment cores --- 3 per plot, including 1 from largest tree in plot.</li> </ul> <p>Data returned</p> <ul> <li>Depth of organic horizon for each plot --- average of 4 measurements at each of 6 sampling locations in the plot</li> <li>Coarse woody debris (&gt; 5cm) volume --- length and center diameter of all CWD within plot</li> <li>Diameter at breast height of each tree in all three plot</li> <li>Estimate of percent cover of the understory in each plot (this is a rough estimate)</li> <li>Slope and aspect of each plot</li> </ul>"},{"location":"snotelC/sampleprocessing/#litter","title":"Litter","text":"<ul> <li>Oven dry sample bags until dry. This may take multiple days.</li> <li>Empty the bag into a tared weigh tray (a tupperware most likely).</li> <li>Remove any rocks and weigh them.</li> <li>Weigh the bulk litter remaining in the tared weigh tray.</li> <li>Sort out CWD (&gt; 0.5cm) and weigh remaining fine litter and leaves.</li> <li>Homogenize the remaining sample and take a representative subsample for grinding</li> <li>Grind this well-mixed subsample of litter and archive in a labeled sample cup</li> </ul>"},{"location":"snotelC/sampleprocessing/#o-horizon","title":"O horizon","text":"<ul> <li>Oven dry sample bags</li> <li>Empty the bag into a tared weigh tray.</li> <li>Remove rocks &amp; weigh them.</li> <li>Weigh the bulk organic soil remaining in the tared weigh tray.</li> <li>Sort out CWD (&gt;0.5cm) &amp; weigh the remaining organic soil sample.</li> <li>Sample fine roots if available and put in coin envelope (1 per site) for later analysis.</li> <li>Homogenize the remaining sample and take 2 representative subsamples:</li> <li>One sample sifted through 2mm mesh to remove fine roots, then save ~20g of each (&gt;2mm &amp; &lt;2mm fractions) for density fractionation.</li> <li>A larger bulk sample ground in coffee grinder and archived in a sample cup for further SOM analysis.</li> </ul> <p>The first sample will be separated with density fractionation and fractions then undergo prep for EA-IRMS analysis. The second sample can be subsampled for immediate prep for EA-IRMS analysis.</p>"},{"location":"snotelC/sampleprocessing/#mineral-0-10","title":"Mineral 0-10","text":"<ul> <li>Oven dry sample bags</li> <li>Weigh bulk sample in a tared weigh tray.</li> <li>Sift sample through 2mm screen.</li> <li>Weigh any rocks (&gt;2mm) remaining on the screen.</li> <li>If there are roots left on the screen, sample them and place in a coin envelope for later analysis (1 envelope per site/per depth)</li> <li>Homogenize the remaining (&lt;2mm) sample and take a representative subsample for archiving in labeled sample cup.</li> <li>Weigh or archive all remaining &gt;2mm material (roots and wood).</li> </ul> <p>Note that for 2008 samples half of the oven-dried bulk samples were immediately archived for rock, sand, silt, clay fraction analysis.</p> <p>The archived soil sample can be further subsampled for density fractionation (with fractions then undergoing prep for EA-IRMS), or for immediate prep and EA-IRMS analysis.</p>"},{"location":"snotelC/sampleprocessing/#fine-roots","title":"Fine roots","text":"<ul> <li>While sifting organic and mineral samples above, sample intact fine roots and place in a coin envelope.</li> </ul>"},{"location":"snotelC/sampleprocessing/#bulk-density-tins","title":"Bulk Density Tins","text":"<ul> <li>Oven dry tins</li> <li>Record tin weight written on bottom of tin</li> <li>Weigh dry sample tins (without lids)</li> <li>Sift to 2mm - wet sifting may be necessary for this.</li> <li>Measure volume of rocks remaining in screen (&gt;2mm sample) by volume displacement.</li> <li>Archive any roots, wood, or charcoal remaining in the screen (&gt;2mm sample).</li> <li>Weigh sifted sample (&lt;2mm)</li> </ul>"},{"location":"snotelC/sampleprocessing/#intact-cores","title":"Intact cores","text":"<ul> <li>Remove end caps and place cores (still in liners) on a tray or tin in the drying oven.</li> <li>Once dry, cores should slide out of the liners into a tared weigh tray. Be careful not to lose any soil.</li> <li>Weigh the cores.</li> <li>Sift to 2mm - wet sifting may be necessary for this.</li> <li>Measure volume of rocks remaining in screen (&gt;2mm sample) by volume displacement.</li> <li>Archive any roots, charcoal, or wood remaining in the screen (&gt;2mm sample).</li> <li>Weigh sifted sample (&lt;2mm) and archive for later analysis.</li> </ul>"},{"location":"snotelC/sampleprocessinglog_1/","title":"Sampleprocessinglog 1","text":""},{"location":"snotelC/sampleprocessinglog_1/#snotel-survey-sample-processing-log","title":"SNOTEL survey - Sample processing log","text":"<p>This logs any work done on samples returned from the visits to SNOTEL sites.</p> <p>See also:</p> <ul> <li>The sampling spreadsheet in the project directory logs EVERY sample returned from the field.</li> <li>The Sample tablespage summarizes the samples for each site and their processing/analysis history.</li> <li>The SNOTEL visit logfor notes on site visits.</li> <li>The initial processing proceduresfor returned samples. This links to other procedures.</li> </ul> <p>6-22-2012</p> <ul> <li>Litter mass samples are missing from Farmington Lower (though there were some samples sent to SIRFER)</li> </ul> <p>6-13-2012</p> <ul> <li>Created the sample tables page (above) and am slowly figuring out what samples have been analyzed and what still need to be.</li> <li>2008 O and M1 samples were treated a little differently than subsequent years. The 5 plot samples were processed individually rather than bulked, and then only a bulk acidified sample, which have different sample numbers, was run at SIRFER. See TO DO below.</li> <li>TO DO:</li> <li>2008 O and M1 samples - bulk the 5 plot samples into a single sample cup (1 per plot).<ul> <li>Done --- Greg Maurer 2012/06/15 21:57</li> </ul> </li> <li>Run the 2008 O and M1 samples (now bulked) at SIRFER (without acidifying).</li> <li>Run 2009 <code>acidified</code> O and M1 samples (mostly loaded into tray 1105) at SIRFER.</li> <li>Compare results for acidified and native O and M1 samples (2008/9) to see if this continues to be necessary.</li> <li>Find Rock Creek M2 samples (6-12cm) if they exist.<ul> <li>They don't exist, and my guess is the mineral samples were to 12cm, but not sure. --- Greg Maurer2012/06/15 21:31</li> </ul> </li> <li>Process remaining 2011 site samples (CHP, MM, RC).</li> <li>Subsample and acidify all intact core samples?</li> <li>Sand/silt/clay from 2008 mineral sample bags, 2009 tins, 2011 intact cores.</li> <li>Should I weigh the roots/wood samples from the tins and cores and add them to the bulk density spreadsheet?</li> <li>Need to enter the data into the processing sheet (to include all rock weights, etc.) <ul> <li>Done --- Greg Maurer 2012/06/22 05:31</li> </ul> </li> </ul> <p>10-14-2011</p> <ul> <li>Looking through 2008 and 2009 CWD and litter sampling</li> <li>There are missing litter values from 2008 and I either marked them as \"Not collected\" or \"No Litter\" since one or the other is likely (there must have been litter at some sites that are missing a sample).</li> <li>Difficult to decide where to put the CWD measurements that are in the O horizon section of the field sheets. I am going to calculate volumes for these and add them to Litter layer wood numbers (based on volume x std wood density) for 2008 and to Organic horizon wood for 2009 and 2011 (I think this accurately reflects what I was doing in these different years).</li> </ul> <p>6-28-2011</p> <ul> <li>Mineral samples 2009 are being acidified today</li> <li>Root grinding for all samples (2008 and 2009) completed today - currently inventorying them</li> <li>Litter samples for 2008 were loaded into tins and put in a tray (by Richard) today.</li> <li>The three leftover 2009 Litter vials (Louis Meadow) were loaded into this tray also.</li> </ul> <p>6-25-2011</p> <ul> <li>2009 Mineral and Organic samples have been acidified, ground and weighed into tins - need to be organized and have the trays completed with more samples.</li> <li>Tray 1103: 2008 litter, ground, weighed into tins, ready - along with LM samples in same tray, this completes the 2008 2009 litter sampling.</li> <li>Tray 1103: also has most 2009 tin samples that are acidified ground and ready to go.</li> <li>Tray 1104: 2008 &amp; 9 roots have been ground and weighed into tins, plus remainder of 2009 tins.</li> <li>Acidified rerun of 2009 Mineral and Organic samples have been acidified, ground and weighed into tins - need to be organized and have the tray completed with more samples.</li> </ul> <p>6-20-2011</p> <ul> <li>Started Richard Malyn on bulk density mesurements for SNOTEL soil tins. This consists of screening the entire tin sample to &lt;2mm, weighing the sifted soil, measuring the volume of all rocks &gt;2mm, and separating out roots and wood &gt;2mm.</li> </ul> <p>Spring 2011</p> <ul> <li>Organic samples 2009 - Ground, weighed, and run at SIRFER</li> <li>Mineral 1 (0-10) samples 2009 - Ground, weighed, and run at SIRFER</li> <li>Litter samples 2009 - Ground, weighed, and run at SIRFER EXCEPT Louis Meadow (3 vials)</li> <li>Organic and mineral 1 samples should be acidified and run again</li> <li>There are some samples that need to be rerun</li> </ul> <p>A long time ago</p> <ul> <li>Organic horizon samples 2008 - Ground, weighed, acidified, and run at SIRFER</li> <li>Mineral 1 (0-10) samples 2008 - Ground, weighed, acidified, and run at SIRFER</li> </ul>"},{"location":"snotelC/sampletables/","title":"SNOTEL survey sample tracking tables","text":"<p>Many samples are returned from each SNOTEL site visit. These tables help track them as they are processed and analyzed.</p> <p>See also:</p> <ul> <li>The sampling spreadsheet in the project directory logs EVERY sample returned from the field.</li> <li>The Sample processing logprovides day-to-day accounts of work on these samples.</li> <li>The SNOTEL visit logfor notes on site visits.</li> <li>The field protocoldescribes field methods and samples collected during SNOTEL site visits.</li> <li>The initial processing proceduresfor samples. This links to other procedures.</li> </ul>"},{"location":"snotelC/sampletables/#inital-processing-and-sirfer-analysis-table","title":"Inital processing and SIRFER analysis table","text":"<p>\\^SiteID\\^No.Plots\\^Leaves\\^Roots\\^TreeCores\\^Litter\\^Org\\^=&gt;Acid\\^Min1\\^=&gt;Acid\\^Tin1\\^=&gt;Acid\\^Tin2\\^=&gt;Acid\\^SoilCores\\^=&gt;Acid\\^Notes\\^ \\^ CHP | 3 | 3 | | 9 | 3 | 3 | | 3 | | | | | | 3 | | | \\^sirfer? | | | | | | | | | | | | | | | | | \\^ MM | 3 | 3 | | 9 | 3 | 3 | | 3 | | | | | | 3 | | | \\^sirfer? | | | | | | | | | | | | | | | | | \\^ TD | 3 | 3 | | 9 | 3 | 3 | | 3 | | | | | | 3 | | | \\^sirfer? | | | | | | | | | | | | | | | | | \\^ LM | 3 | | 2 | 9 | 3 | 3 | | 3 | | | | | | | | | \\^sirfer? | | | 2 | | | | | | | | | | | | | | \\^ LP | 1-3 | | 2 | | | x | | x | | | | | | | | | \\^sirfer? | | | 2 | | | | | | | | | | | | | | \\^ PAY | 1-3 | | 3 | | | x | | x | | | | | | | | | \\^sirfer? | | | 3 | | | | | | | | | | | | | | \\^ MC | 1-3 | | 3 | | | x | | x | | | | | | | | | \\^sirfer? | | | 3 | | | | | | | | | | | | | | \\^ HF | 1-3 | | 3 | | | x | | x | | | | | | | | 2 sampling dates | \\^sirfer? | | | 3 | | | | | | | | | | | | | | \\^ CC2 | 1-3 | | 4 | | | x | | x | | | | | | | | | \\^sirfer? | | | 4 | | | | | | | | | | | | | | \\^ BVD | 1-3 | | 4 | | | x | | x | | | | | | | | | \\^sirfer? | | | 4 | | | | | | | | | | | | | | \\^ FU | 3 | | 3 | | 3 | 3 | //3// | 3 | //3// | | | | | | | | \\^sirfer? | | | 3 | | x | | //x// | | //x// | | | | | | | | \\^ FL | 3 | | 3 | | 3 | 3 | //3// | 3 | //3// | | | | | | | | \\^sirfer? | | | 3 | | x | | //x// | | //x// | | | | | | | | \\^ BLP | 3 | 3 | ? | 9 | 3 | 3 | //2// | 3 | //2// | | | | | 9 | | 2 sample dates, 4 plots, ignore #3, roots to come? | \\^sirfer? | | | | | x | | //x// | | //x// | | | | | | | | \\^ BLT | 3 | 3 | 3 | 9 | 3 | 3 | //3// | 3 | //3// | | | | | 9 | | 2 sample dates | \\^sirfer? | | | 3 | | x | | //x// | | //x// | | | | | | | | \\^ BD | 3 | | 3 | | 3 | 3 | //3// | 3 | //3// | | | | | | | | \\^sirfer? | | | 3 | | x | | //x// | | //x// | | | | | | | | \\^ CC1 | 3 | | 3 | | 3 | 3 | //3// | 3 | //3// | | | | | | | | \\^sirfer? | | | 3 | | x | | //x// | | //x// | | | | | | | | \\^ TL | 3 | 3 | 3 | 9 | 3 | 3 | //3// | 3 | //3// | | | | | 9 | | 2 sample dates | \\^sirfer? | | | 3 | | x | | //x// | | //x// | | | | | | | | \\^ RC | 3 | 3 | ? | 9 | 3 | 3 | //3// | 3 | //3// | | | | | 7 | | 2 sample dates | \\^sirfer? | | | | | x | | //x// | | //x// | | | | | | | |</p>"},{"location":"soilclim/analysislog_1/","title":"Soil temperature analysis log","text":"<p>This log mainly covers analysis of SNOTEL soil profile data.</p> <p>See also: The project overview.</p> <p>3/26/2013</p> <p>Looking for sites to use as example sites in multiple regression.</p> <ul> <li>366 (brighton)or 521 for winter Tsoil vs onset date.</li> <li>334 or 368 for winter VWC vs Dec SWE</li> <li>334 or 392 for growing season VWC vs peak SWE</li> </ul> <p>2/5/2013</p> <ul> <li>I made a change to the sigma filter which it fixes the ends of the running mean. This fixes some erroneous data rejection at the beginnings and ends of the timeseries. Re-running summarize_wateryear.m, and this may have an effect on some of the analysis, so plots should be rerun.</li> <li>Have mostly finished up the pca work, just need to figure out how to summarize and present it.</li> <li>Added may precipitation to the climatesummary data file (made a change in summarize_wateryear.m to do this)</li> </ul> <p>1/20/2013</p> <p>I'm going to merge this log with the soil moisture log because it doesn't make sense to me to maintain 2 anymore. The content that was at that log is shown below.</p> <p> <p>Oct 14, 2012</p> <p>A question: Is it valid to take the mean of all monthly means from a site? It should be fine if the months are the same length, but in the case of missing data during a month, these means wouldn't be valid.</p> <p>Oct 10, 2012</p> <p>Have spent lots of time recently removing bad data and improving filtering. A couple of current problems</p> <ul> <li>There were times when the sigma filter would not work. This occurred because the runningStd calculation sometimes produced an all NaN series after trying to calculate Std. deviation on the filled series (from interpseries.m). This only happened with timeseries that began or ended with NaNs and is the result of the interpseries ignoring the ends of a series if they are NaNs. I'm trying to figure out how to fix this.</li> </ul> <p>Feb 21, 2012 - A couple ideas:</p> <ul> <li>Mean soil moisture the week before snowpack onset and the day of snowpack onset.</li> <li>Does this correlate with winter soil moisture? Until when (relative to peak snowpack or snowmelt)?</li> <li>Does day of snowmelt correlate with late summer soil moisture?</li> <li>In monsoon vs non-monsoon areas? Perhaps monsoon areas are more correlated with summer precip?</li> </ul> <p>Feb 16, 2012 - Have done lots in the last few days:</p> <ul> <li>New 30-year mean data (SNOTEL SWE, SNOTEL Precip, and SnowCourse SWE) downloaded from NRCS website, error checked, and tabulated in a new spreadsheet. See <code>rawdata/longterm_averages/SnowSurvey_7100Avg_Master.gnumeric</code>.</li> <li>Makes some older spreadsheets obsolete, though there are some old copies.</li> <li>SWE and precip data are exported as .csv files to the curated data folder for use in data-analysis scripts</li> <li>New station inventory spreadsheet is located at <code>rawdata/station_inventory/NRCS_inventory_MASTER.gnumeric</code>.</li> <li>Created by downloading and editing .csv files from the NRCS data search webpage.</li> <li>This file is being used to keep track of what SNOTEL/Scan/SnowCourse data is available for snowpack and soil profiles, and what data has been downloaded already.</li> <li>Might replace the other inventory .csv files in this directory (some scripts still rely on these though).</li> <li>Both of these files have some documentation available in the first sheet.</li> <li>Some general reorganizing and cleaning out of old data and scripts in the rawdata, and data_analysis folders that contain this project.</li> <li><code>obsolete_scripts</code> holds old SNOTEL network analysis scripts that may still be useful but are being replaced by new ones. There is a corresponding collection of data files in the rawdata directory.</li> <li><code>curated_data</code> holds textfiles that are output from spreadsheets or scripts that operate on raw data (as it might be useful to version these).</li> <li>New script to calculate many metrics of climate and interannual variability (in snow, precip, temp) from the daily data. It outputs a large dataset to the <code>curated_data</code> directory and is read by plotting scripts. Will be documented here</li> <li>Downloaded a large amount of new data from the NRCS website to <code>allsensors_daily</code>, and <code>soilsensors_hourly</code> directories.</li> <li>Now have daily data for all SNOTEL sites with soil profiles from install to 2010 (working on 2011)</li> <li>AZ, NM, UT, and CO have complete sets of the SNOTEL soil profile data (NV, WY, ID, MT to go still).</li> <li>Met with Dave today, and we agreed that more daily data is needed - preferably all of it for the last 10 years or so.</li> </ul> <p>Feb 3, 2012</p> <p>![media/west_stationdata:plot_4sites_distrib.png?300|Sites in each column are Trial Lake, BL Trail, Chepeta, Little Grassy (Right to left)]</p> <ul> <li>plot_monthly_sm_distrib.m has been reworked and renamed to plot_soilsensor_distrib.m. It is a bit more generic and will plot soil temperature data in the same fashion.</li> <li>Have been working on some case-study type plots comparing high/low elevation/swe sites (in a factorial way). Results for Trial Lake, Ben Lomond Trail, Chepeta, and Little Grassy are to the right -&gt;. Chepeta seems to defy expectations a bit by being highly variable in spring. (Plot generated by plot_4sites_distrib.m)</li> <li>Met with Dave today and we agreed that a main message of this analysis so far is that soil moisture is highly variable inter-annually, between sites. and even within seasons. There are a few somewhat predictable patterns though: </li> <li>Low elevation sites seem to show more summer drought</li> <li>Soil moisture stays constant below winter snowpacks, but there is large inter-annual variability between winters. This probably depends on year-to-year differences in antecedent (fall) precip.</li> <li>In general, soil moisture variability is higher at low elevation sites (especially outside summer months). This could be shown with a CV vs elevation plot perhaps.</li> <li>TO DO:</li> <li>Explore the soil moisture CV vs elevation relationship (see above)</li> <li>Need greater power for analysing case studies, including binning sites into climate-space envelopes. First this requires that we define ranges in elevation, MAT, MAP, SWE, melt timing, summer precip, etc, across the SNOTEL network. Second, how could we group sites according to particular criteria (High elevation sites with low snowpacks for instance.)<ul> <li>Such data could be used to select appropriate sites for case studies (such as replacing Chepeta in the figure at right).</li> </ul> </li> <li>Some of the temperature data from PRISM should probably be used in climate-space analysis rather than SNOTEL temp data.</li> <li>There may be some interesting soil moisture patterns at monsoon sites that should be explored. The shape of the precip accumulation line might be useful for defining sites w/ summer rain.</li> <li>Collect data from more sites - MT, AZ, NM are left.</li> </ul> <p>Jan 28, 2012</p> <ul> <li>There are now some new testing functions in the SNOTELdata project. One that plots two datasets to view changes (useful for comparing filtered/unfiltered, or transformed/untransformed datasets) and one that will generate SNOTEL datsets with original values, or with missing columns or dummy values.</li> <li>The plot_monthly_sm_distrib.m file has been updated to show an all-years plot.</li> <li>Dave and I discussed doing some side-by-side comparisons of the quarterly histograms from several sites that would illustrate low/high elevation and snowpack scenarios.</li> </ul> <p>Jan 21-26, 2012</p> <p>![media/soilclim/plot_monthly_sm_distrib-828wy2010.png?300|plot_monthly_sm_distrib.m output for Trial Lake, WY2010]</p> <ul> <li>Created plot_monthly_sm_distrib.m to print a site's monthly histograms of all soil moisture data for each year in the analysis.</li> <li>Have been working on some testing functions for loadsnotel.m and the data cleaning/filtering routines.</li> <li>Possible test cases that involve replacement/addition of sensors between years are site 396 (Ts -2 disappears after 2004), and 435 (Ts -20 disappears after 2004).</li> </ul> <p>Jan 18, 2012 - Some initial goals:</p> <ul> <li>Each sensor needs to be normalized to 0-1 (after data cleaning)</li> <li>Plot histograms of the data during different time periods(months) of the year (some will be bi-modal)</li> <li>size of histogram modes indicate time sensor spends at particular value</li> <li>try plotting in 3d - see figure 6 in bowling_11 paper</li> <li>Next week - soil moisture distributions for 1 site, all months, 1 sensor depth.</li> </ul> <p></p> <p>10/24/2012</p> <p>Stuff that would be good:</p> <ul> <li>summary file that has averages not based on wateryear, but based on all data</li> <li>summary file that includes integrals of monthly/yearly temperatures and vwcs (probably using interpolation to fill gaps).</li> </ul> <p>10/18/2012</p> <p>Things to try for comparing mean annual air temp and mean annual soil temp:</p> <ul> <li>Integrating the timeseries of each, for a wateryear, and comparing them and the difference between them.</li> <li>Maybe this could be done with cumsum or trapz in MATLAB - but data will need to be interpolated and we need to make sure each wateryear is the same length.</li> <li>Calculate degree days - not sure how yet.</li> </ul> <p>2/21/2012</p> <p>Couple ideas:</p> <ul> <li>Mean soil temp the week before snowpack onset and the day of snowpack onset.</li> <li>Does this correlate with winter soil temp - especially early season?</li> <li>Mean air temp the week before, and after snowmelt day.</li> <li>or offset b/t air and soil temps during spring.</li> <li>what are the differences between early and late melt years?</li> </ul> <p>2/9/2012</p> <ul> <li>Read the Gubler et al 2011paper today.</li> <li>Cool boxplots for site data might be a good way to summarize data</li> <li>They have some interesting analysis of topography and elevation effects on ground surface temp, and we might be able to do something similar with our mean annual soil temps (year to year or aggregated by site).</li> <li>Almost all their data is of MAGST - Mean annual ground surface temperature - maybe we need to look at means more.</li> <li>Definitely see the Bartlett paper referenced here.</li> </ul> <p>2/03/2012</p> <p>![media/west_stationdata:828_distrib_allyrs.png?300|Trial Lake, histograms from all water years combined.]</p> <ul> <li>Plotting of soil temperature distributions is now done in plot_soilsensor_distrib.m, replacing the program that created <code>???</code>. The new output (for Trial Lake) is at right. This program plots histograms for individual water years, and all years combined.</li> <li>There is also a program that will combine quarterly distributions from several sites into one display for comparison of case-study scenarios (plot_4sites_distrib.m).</li> <li>Working on compiling SNOTEL climate-space data, including temperature, snowpack, etc, into a form that is usable for comparing contrasting sites, or groups of sites.</li> </ul> <p>12/9/2011</p> <ul> <li>Figures from AGU 2011 poster:</li> </ul> <p>11/29/2011</p> <ul> <li>Added a <code>_baddata.txt to allsensors_daily</code> file. Data is marked by site and year (calendar) mostly based on whether outliers showed up in the plot_ts_vs_snowpack.m plots. I looked closely at all data I marked to be certain that there were actual sensor irregularities (not just outlier data).</li> <li>Filtered out these datasets in <code>plot_ts_vs_snowpack.m</code>.</li> </ul> <p>11/23/2011</p> <ul> <li>Should I plot some of these vs time? Show temp offset in each month, then year to year?</li> <li>At a site, what is the difference in mean below-snow or monthly below-snow soil temperature between years? Are these differences correlated with any snowpack characteristics (onset, depth, SWE, air T)?</li> <li>Is there a generalized soilT timeseries shape that varies according to snowpack (onset, depth, SWE, airT)?</li> </ul> <p>11/17/2011</p> <p>![media/west_stationdata:soiltvssnowpack_wu.png?300|February soil temps vs snowpack at 2 depths in the Wasatch and Uinta mountains]</p> <ul> <li>Might make sense to create a function that generates lists of sites based on certain criteria like: mountain range, state, elevation, latitude, etc.</li> <li>Created plot_ts_vs_snowpack.m to examine the monthly relationship between soil temp, SWE, and snow depth in individual years (data are not averaged across years). Also examines wasatch vs uintas.</li> <li>Looks like soil temp needs some additional filtering routines.</li> </ul> <p>11/11/2011</p> <ul> <li>Some thoughts about where to go from here</li> <li>Complete what was  said in part 1 above (10-7)</li> <li>There are equations that describe the sinusoidal pattern of seasonal temperature and we could probably base these on air temperature for SNOTEL sites - how do these differ for a snow-dominated and non-snow dominated soil</li> <li>Winter soil temps are only half of a sinusoid, so maybe there is a way to model seasonal soil temp in both snow-free and snowcovered periods separately.</li> <li>How does that seasonal pattern change when you change snowpack parameters (depth, accumulation timing, meltout, etc) and can we model that accross our sites?</li> <li>For Moisture:</li> <li>Does damping depth vary seasonally and interannually and is this controlled by moisture content?</li> <li>How far out of phase are precip and soil moisture recharge? - maybe for this investigate that elbow between winter and summer in accumulated precip you can see in SNOTEL data.</li> <li>Where does summer rain come in (again, those accumulated precip graphs, especially their slope, might be useful)</li> </ul> <p>10/7/2011</p> <ul> <li>Need to separate different years of data for each site and evaluate influence of snowpack at each site - identify whether low-snowpack years (or late snowpack years) are correlated with low soil temps (for a month or the entire year), length of time with stable winter soil temps</li> <li>But say sites show varying influence of snowpack size or timing - how do we demonstrate the importance of this dependence across the landscape?</li> </ul> <p>9/13/2011</p> <p>![media/west_stationdata:snotelsoiltvssnowpack.png?300|]</p> <ul> <li>It appears that there is a relationship between soil T and snowpack that is present in February - sites with low snowpacks (in depth or SWE) have colder temperatures and temperatures increase as snowpack increases. The strength of this relationship seems to drop off as soils reach 1-2\u00b0C. The graph at right illustrates this. Note that these datapoints are aggregated data for all soil and snowpack data from the selected months for a site. Individual years may show a stronger pattern.</li> </ul> <p>9/6/2011</p> <ul> <li>A next step for temperature analysis: what is MAT for soils vs air temp along an elevation gradient?</li> <li>Can we add PRISM MAP and MAT to this analysis?</li> </ul> <p>8/24/2011</p> <ul> <li>Added daily, all-sensor data from ID, WY, and CO to the analysis</li> <li>Generated a file with all long term(71-00) SWE and Precip values for UT, NV, MT, ID, WY, CO, NM, AZ. This amounts to 494 sites total.</li> <li>Now need mean soil and air temperatures for different months to go with the 71-00 averages above.</li> <li>Working to produce a graph of Lom</li> </ul> <p>8/10/2011</p> <ul> <li>Figures from my commitee meeting (that have to do with SNOTEL data analysis)</li> </ul> <p>8/9/2011</p> <ul> <li>New file: //filterseriestest_scr.m// should be able to test different filtering regimes and show the changes in data</li> <li>New file://tempswegradient_scr.m//plots February soil temps against a variety of things including SWE, Snow depth, and elevation.</li> </ul> <p>8/5/2011 - TO DO list for west_stationdata programs:</p> <ul> <li>//loadsnotel.m//</li> <li>Decide what to do with quality control arrays</li> <li>What filtering should be included, some of the bad data removal seems redundant, and might be better put in filterseries.m</li> <li>//filterseries.m//</li> <li>Filtering thresholds should probably be set here, or there should be an option that pops up for each filter pass.</li> <li>There could also be a way of determining the optimum filter threshold for each sensor (separate from this program).</li> <li>Filtering thresholds should also be different for winter or summer</li> <li>//swe_snowcover.m//</li> <li>Should probably be changed to reflect that this is a daily-hourly conversion - and it may be useful for more than snow data</li> <li>//sitesoiltempvariability_scr.m//:</li> <li>//monthlysoiltempgradients_scr.m//:</li> </ul> <p>8/4/2011</p> <p>![media/west_stationdata:monthlysoiltempgradient2.png?350|]</p> <ul> <li>//sitesoiltempvariability_scr.m//:</li> <li>Added routine to convert daily SWE data to hourly values matching the hourly soil data. This can then be used to create a logical snowcover test.</li> <li>//swe_snowcover.m// - New - Uses routine developed above to take hourly decday vector and SNOTEL site id and returns a logical snowcover array for the data.</li> <li>//monthlysoiltempgradients_scr.m//:</li> <li>using swe_snowcover.m to test for snowcover now</li> <li>adding site data and a \"havedata\" routine to the program to use this data. Should be more extensible.</li> <li>Common error: if hourly datasets begin prior to daily datasets there is an empty matrix error in the daily-to-hourly snowpack conversion searches based on hourly data first.</li> <li>Still missing data in September - and this may be due to the filtering used (it was pretty restrictive when the figure above was produced).</li> </ul> <p>8/3/2011</p> <ul> <li>//loadsnotel.m//: </li> <li>No longer removing data marked in qc column because it removes too much good data, just leaving qc for reference. Other filters should do a good enough job of getting rid of this problematic data.</li> <li>Inserting routine for removing repeated, non-daily rows from daily datafiles</li> <li>//filterseries.m//:</li> <li>Fixed this so that mean and shift diff filters should both work - but needs checking</li> <li>//sitesoiltempvariability_scr.m// Experimenting with different filtering calls and making sure they work ok. Need to decide on one that works for the most sites.</li> </ul> <p>6/14-15/2011</p> <ul> <li>Downloaded lots of new daily datafiles from NRCS site</li> <li>Created a filelist (filelist.txt) and a bash script to create this list (filelistgenerator.sh). The textfile lists the site number once for each yearly datafile in the directory. Thus it can be read to inventory how many sites have data, and how many years of data each site has.</li> <li>Created a new spreadsheet with 30-year average SNOTEL data (SWE and precip) and exported a textfile to the SNOTEL_data folder for use in scripts (see below).</li> <li>Created a new script (for now called tempsweelev_scr.m) to plot air temps (MAX and AVG), soil temps, average SWE, and average precip for July and January and plot them across the elevation gradients chosen.</li> </ul> <p>5/18/2011</p> <ul> <li>The Snowbird(766) -2in sensor is messed up - during summer its ranging from 45 to 10 degrees C. Maybe the sun hits it or there is a heat conduction problem or something.</li> </ul> <p>4/29/2011</p> <ul> <li>Filtering routines (in sitesoiltempvariability_scr.m) moved to filterseries.m, and added interpolation after the filtering</li> <li>sitesoiltempvariability_scr.m still checks mean differences and shifted differences to test the effectiveness of filtering</li> <li>Interpolation should come before filtering since filtering generates a runmean with larger gaps.</li> <li>Interpolation takes place in sitesoiltempvariability_scr (to Ts) then after filtering in filterprofileseries.m</li> <li>FIXME There is missing data in Sept in monthlysoiltempgradients_scr.m outputs - why?</li> <li>FIXME Interpolated data leads to low standard deviations in the running calculation, so these need to be corrected somehow </li> <li>This can lead to errors in separating snowcover from snowfree periods</li> <li>Perhaps a logical matrix that accumulates interpolated datapoints should follow the Tsoil array around.</li> <li>FIXME Also, there are still periods in winter that read as snow-free, so maybe a test that adds pillow SWE readings to determine snowcover.</li> <li>FIXME Shift difference filter doesn't seem to be working right (see histograms compare unfiltered and filtered data)</li> </ul> <p>Apr 26, 2011 - Fixing runmean nan problem</p> <p>![media/soilclim/ts_interp.png?250|Interpolated points (red) in Ts data]</p> <ul> <li>Interpolate the data using interp1</li> <li>generate the mean with something besides filter (something with better nan handling) - can't get these to work well.</li> <li>Have added a routine to soilTvar.m that can interpolate missing data in a soil temp timeseries - see figure to right. This is better than interpolating the runmean. Should probably add this to NWCC_load.m or create a separate function to call so that programs could be run with or without interpolation</li> <li>Doesn't seem to work well for some sites (run with 972) - seems to interpolate to values that aren't there.</li> <li>These wierd interpolations happen at the beginning of dataseries if the data starts with long stretches of bad data.</li> <li>Filtering routines:</li> <li>remove sub-hourly rows - loadsnotel</li> <li>remove datalogger error signals -99,9 and -273 (via qc column) - loadsnotel</li> <li>FIXME - this often removes good data (see 971 or 972)</li> <li>remove SM values that are 0 in winter(via qc column) - loadsnotel</li> <li>mean difference filter (Ts-run mean &gt; ???) - loadsnotel</li> <li>remove Ts values greater than 30 and less than -10 - loadsnotel</li> </ul> <p>TO DO for 4-27-11 meeting:</p> <ul> <li>Separate filtering functions - loadsnotel.m removes errors, filter function uses mean and shift filters on a data series.</li> <li>Make soilTvar present data for unfiltered data, and both filters separately and together</li> <li>Get an interpolation function working (must remove or handle leading NaNs)</li> <li>Then run soilTelev with new filtering routines.</li> </ul> <p>Feb 23, 2011</p> <p>![media/west_stationdata:828_soildist.png?250|Trial Lake -2cm soil temp distributions at various time periods (~2002-2010)]</p> <ul> <li>Downloaded and edited inventory files for NWCC SCAN/SNOTEL stationsin Intermountain west states (those listed above)</li> <li>First figures came from plot_seasonal_ts_distrib.m program. Figure for Trial Lake snotel site at right </li> <li>soilTvar helps with:</li> <li>Selecting thresholds for distinguishing snow and snow-free periods</li> <li>Choosing thresholds for diff filtering</li> <li>Choosing thresholds for shift filtering</li> <li>PROBLEMS </li> </ul> <p>![media/soilclim/ts-snowtestfailinwinter.png?300|]</p> <ul> <li>The program doesn't appear to separate snowcovered from snowfree periods very well. This might be because of NANs in the running standard deviation array used to create a logical test.</li> <li>There are also imaginary numbers in runstd. These result from taking the square root of a variance array that includes negative numbers. Variance should always be positive (it is the sum of squared values), but apparently there can be problems when calculating variances over large arrays of near-zero numbers, or numbers that are similar (see hereor here). So, I need a better algorithm for calculating variance of winter Ts and then its StdDev.</li> <li>Maybe... RUN A TEST AGAINST THE IMAGINARY NUMBERS AND SET TO NAN... didn't seem to do much. Looks like imaginary numbers test as less than the threshold (0+0.342i &lt; 0.1 = true), so they should be flagged as snowcovered anyways.</li> <li>Setting nans to be snowcovered periods helped, but now there is \"snowcovered\" data where there are NaNs in summertime.</li> <li>NaN's always test as not snowcovered (NaN &lt; 0.1 = false), so this would explain why there are \"snow-free\" periods in winter - its the NaN's that are flagged as snow-free no matter what.</li> <li>Ways to fix this:</li> <li>There are nans in snowcovered and snowfree seasons, so need at least two logical tests - one for nans in each season.</li> <li>Test out nans in snow season only</li> <li>Create a logical test matrix for nans and then backfill the nans into the Ts array after running the test.</li> <li>Reduce the number of nans</li> <li>RUNMEAN function generates lots of nans from a few in the soil temperature array! FIXME!!!</li> </ul>"},{"location":"soilclim/data/","title":"Western U.S. NRCS data","text":"<p>Collection of environmental data, including met and soils data, from NRCS, Ameriflux, and other research stations and its processing for analysis is covered here.</p> <p>See also: Overview page, Data program documentation, Data analysis log</p> <p>Start date: 25 Jan, 2010</p>"},{"location":"soilclim/data/#objectives","title":"Objectives","text":"<ul> <li>Gather continuous measurements of soil moisture, soil temperature, weather/climate, snowpack dynamics, and metrics of soil biological activity for western U.S. mountain sites.</li> <li>Quantify spatial variability (by elevation, topography, lat/lon) in climate, snowpack dynamics, SM/ST.</li> <li>Quantify inter-annual variability in climate, snowpack dynamics, SM/ST.</li> <li>Assess the relationships between spatial or inter-annual variability in SM/ST, climate, and snowpack dynamics among sites.</li> </ul>"},{"location":"soilclim/data/#data-sources","title":"Data Sources","text":""},{"location":"soilclim/data/#national-water-and-climate-center-snotel-scan-and-snow-course-data","title":"National Water and Climate Center - SNOTEL, SCAN, and Snow Course data","text":"<p>Most data, including SNOTEL (mountain) and SCAN (lowland) automated station data comes from the National Water and Climate Center. This division of the USDA's Natural Resources Conservation Service runs the Snow Survey (Snow Courses), SNOTEL network, and SCAN network and provides a few online interfaces for accessing this data.</p> <ul> <li>The NWCC station inventory lists SCAN/SNOTEL sites by state and element (sensor or data type). This provides good inventory and indexing data.</li> <li>Individual station siteslike this one allow generation of element (sensor or data) reports for particular time periods, including historical periods.</li> <li>The Snow Course data portalallows access to historical snow measurements. Many of these measurements extend back to the 1950's or earlier, and many snow courses are still measured today. They provide calibration data for adjacent SNOTEL stations.</li> </ul>"},{"location":"soilclim/data/#other-potentially-useful-sites","title":"Other (potentially useful) sites","text":"<p>The Ameriflux network, Fraser Experimental Forest, Tenderfoot? New Mexico flux gradient?</p>"},{"location":"soilclim/data/#snotel-and-scan-raw-data-files","title":"SNOTEL and SCAN raw data files","text":"<p>Files are downloaded as .csv (usually), and are often edited using various text editing and data file handling tools.</p>"},{"location":"soilclim/data/#soil-station-inventory-files","title":"Soil station inventory files","text":"<p>Files are downloaded from the NWCC station inventory site. The original column headers are:</p> <p>network,station id,start_date,end_date,lat,lon,elev,element,state,county,basin,site_name,site_info_link,cdbs_id,shef_id,gmt_offset,</p> <p>Several inventory files, selected using the dropdowns, are useful:</p> <ul> <li>Selecting precipitation accumulation gives pretty much all SNOTEL and SCAN sites.</li> <li>Selecting soil moisture and temperature gives a file containing only sites with soil sensor profiles (SCAN and SNOTEL).</li> </ul> <p>These inventory files have been put into a master spreadsheet that keeps track of sites and the data collected for them (see the inventory directory).</p>"},{"location":"soilclim/data/#hourly-smst-data","title":"Hourly SM/ST data","text":"<p>Files downloaded from individual station sites by specifying download of hourly wateryear data for SM/ST sensors. One file for each year that the sensors have been in operation must be downloaded.</p> <p>Fields in these files: \\^ Variable name \\^ Measurement \\^ Units/Format \\^ Measurement period \\^ | Site Id | Site identification number | integer | - | | Date | Date | yyyy-mm-dd | - | | Time | Time of measurement | hh:mm | - | | SMS.I-1:-2(-8, -20) | Soil moisture @ -2 in (or -8in, -20in) | VWC (%) | hourly average | | STO.I-1:-2(-8, -20) | Soil moisture @ -2 in (or -8in, -20in) | \u00b0C | hourly average | | STO.I-1:0 or STO.I-2:0 | Calculated values are sometimes present at odd depths (0) or marked with I-2 (ignore these) |||</p>"},{"location":"soilclim/data/#data-collected","title":"Data collected:","text":"<p>All datafiles for UT, ID SNOTEL sites with soil profiles.</p>"},{"location":"soilclim/data/#daily-summaries-all-sensors","title":"Daily summaries - all sensors","text":"<p>Files downloaded from individual station sites by specifying download of daily wateryear data for all sensors. One file for each year that the sensors have been in operation must be downloaded. Instantaneous measurements in these files are for 12AM (data is the same as 12am files - see below). These files include daily calculated temperatures (TMAX, TMIN, TAVG).</p> <p>Fields in these files: \\^ Variable name \\^ Measurement \\^ Units/Format \\^ Measurement period \\^ | Site Id | Site identification number | integer | - | | Date | Date | yyyy-mm-dd | - | | Time | Time of measurement | hh:mm | - | | WTEQ.I-1 | Snow water equivalent (pillow) | inches | Instantaneous | | PREC.I-1 | Accumulated Precipitation | inches | Instantaneous | | TOBS.I-1 | Observed temperature | \u00b0C | Instantaneous | | TMAX.D-1 | Maximum daily temperature | \u00b0C | Calculated daily | | TMIN.D-1 | Minimum daily temperature | \u00b0C | Calculated daily | | TAVG.D-1 | Average daily temperature | \u00b0C | Calculated daily | | SNWD.I-1 | Snow depth (Judd sensor) | inches | Instantaneous | | SMS.I-1:-2(-8, -20) | Soil moisture @ -2 in (or -8in, -20in) | VWC (%) | Instantaneous | | STO.I-1:-2(-8, -20) | Soil temp @ -2 in (or -8in, -20in) | \u00b0C | Instantaneous | | SAL.I-1:-2(-8, -20) | Soil salinity @ -2 in (or -8in, -20in) | grams/l | Instantaneous | | RDC.I-1:-2(-8, -20) | Real dielectric constant (each soil sensor) | unitless | Instantaneous | | BATT.I-1 | Battery voltage | voltage | Instantaneous | | STO.I-1:0 or STO.I-2:0 | Calculated values are sometimes present at odd depths (0) or marked with I-2 (ignore these) ||| | WTEQ.I-2 | **Some sites have additional (backup?) measurements - Ignore these ** |||</p> <p>Note that the calculated values (for temperature) are suspect prior to hourly recording of measurements (see note below).</p>"},{"location":"soilclim/data/#data-collected_1","title":"Data collected:","text":"<p>All datafiles for AZ. CO, ID, MT, NM, NV, UT, WY SNOTEL sites.</p>"},{"location":"soilclim/data/#instantaneous-measurement-files","title":"Instantaneous measurement files","text":"<p>Instantaneous measurements for individual or all sensors can be downloaded hourly, or for every third hour (12AM, 3AM...6PM, 9PM). These contain instantaneous measurements for any sensors chosen (including all) at the time period chosen. They do not include summary data (TMIN, TMAX, TAVG) like the files above).</p> <p>NOTE: At many snotel sites, true hourly measurements did not         begin until well after the site was established (mid to late         90's at many).</p>"},{"location":"soilclim/data/#sensors-and-calibrations","title":"Sensors and calibrations","text":"<p>Soil sensors at these sites are Stevens Hydra-Probe sensors. Some appear to be the analog version of the sensor which measures soil moisture, temperature, and electrical conductivity. Some may be the digital version (HydraProbe II), but I'm not sure about this.SM/ST sensors are generally located at 2, 8 and 20 inch depths (5, 20, 50cm), but there seems to be some variability among sites here. SCAN sites may have measurements from more depths.</p>"},{"location":"soilclim/data/#soil-moisture-equation-change-feb-2010","title":"Soil moisture equation change - Feb, 2010","text":"<p>The NRCS issued a notice about a change in the equation used to convert sensor output to soil moisture in the winter of 2009/10. This will take effect for all data, including already collected data, beginning in February 2010. The change in the equation is documented in an article in the Vadose Zone Journal:</p> <p>Seyfried, M.S., L.E. Grant, E.Du, and K Humes. 2005. Dielectric loss and calibration of the hydra probe soil water sensor. Vadose Zone J. 4:1070-1079</p> <p>Data currently available on NRCS websites should be updated with this change.</p>"},{"location":"soilclim/data/#sensor-movesreplacements","title":"Sensor moves/replacements","text":"<p>See the sensor history page (linked on individual SNOTEL site pages) for detailed info on sensor maintenance, movement, or replacement.</p> <ul> <li>Louis Meadow site had sensors moved on 12/15/2003. Apparently sensors at -4 and -8 were moved to -8 and -20 respectively. Not sure if sensors were physically moved, or just renamed in the datalogger output.</li> </ul>"},{"location":"soilclim/data/#other-notes","title":"Other Notes:","text":"<ul> <li>WVF is volume fraction of water in field moist soil (comparable to percent of field capacity)</li> <li>At least some files start or end with a few lines of data collected at odd intervals (bad data).</li> </ul>"},{"location":"soilclim/data/#data-processing-and-filtering","title":"Data processing and filtering","text":"<p>For info on how the data were quality checked and filtered, see the data quality control page.</p> <p>Programs that load and process the raw data, as well as the outputs they produce, are documented here. Some of the development of these programs and results are documented here.</p>"},{"location":"soilclim/data/#older-data","title":"Older data","text":"<p>Some data was downloaded from the NRCS ftp site with help from Tim Bardsley, Dana Kuiper, and Maggie Dunklee.</p> <p>Data from all SNOTEL sites with soil moisture and temperature profiles installed was obtained from the NRCS in the spring of 2009. This included data from 252 sites, including SCAN sites, for the period from install date to May 29, 2009. Data streams include soil moisture at up to 3 depths, soil temperature at up to 3 depths, avg air temp, max air temp, min air temp, snow depth, and snow water equivalent (SWE). Data were received as .csv files, one for each data stream at each site, with these fields and formats:</p> <p>\\^ Station ID \\^ Date sampled \\^ Variable code \\^ Value \\^ Quality control flag \\^ | 3-4 digit int | yyyy-mmm-dd hh:mm:ss | String | Float | Character | | 345 | 2003-Sep-06 21:00:00 | WTEQ | -0.3 | V | | 345 | 2003-Sep-06 22:00:00 | WTEQ | -0.3 | V | | 345 | 2003-Sep-06 23:00:00 | WTEQ | -0.2 | E |</p> <p>Soil moisture and soil temperature files do not have a QC control column. The station ID's are indexed along with their name, state, elevations, lat/lon, install dates, and other info in the soil_stations.csv file.</p>"},{"location":"soilclim/data_qc/","title":"Data quality control","text":"<p>This page helps track bad, incomplete, or otherwise excluded data from SNOTEL sites.</p> <p>See also: Project overview, Programming documentation, Data collection/prep, Data analysis log</p>"},{"location":"soilclim/data_qc/#procedure-for-screening-data-files","title":"Procedure for screening data files","text":"<ul> <li>Run testfiles.m</li> <li>Run testsensors.m</li> <li>Flag any sensors that look like the data being read is in error.</li> <li>Flag any sensor that appear to have a large level shift in the middle of a water year</li> <li>DO NOT flag missing data (gaps), or short timeseries, this will be taken care of by testfiles.m and nancheck.m`</li> </ul>"},{"location":"soilclim/data_qc/#directories-and-files","title":"Directories and files","text":"<p>Both the NRCSdata/allsensors_dailly/ and the NRCSdata/soilsensors_hourly/ directories contain a <code>sitelist.txt</code> file that is generated by the sitelistgenerator.sh shell script. When changes are made to these directories (files added or removed) this script should be re-run.</p> <ul> <li>baddata.txt is present in allsensors_daily/. It lists all datafiles, by siteID and year, that should be excluded from analysis. The actual exclusion is performed by <code>loadsnotel.m</code> (for now).</li> <li>In both data directories (above) there are subdirectories that hold incomplete, empty, or otherwise problematic files.`</li> </ul>"},{"location":"soilclim/data_qc/#bad-data-tracker","title":"Bad data tracker","text":"<p>\\^ SiteID \\^ Years \\^ Problem \\^ in baddata.txt? \\^ Other \\^ | 306 | 1998-2004 | Sensors at wrong depth, plus an I-2 header | Yes | Seems to get excluded by loadsnotel | | 2029 | 2000,2001 | | | WARNING - I edited these files, removing the last line of data, because it was effing up my scripts | | 1008 | 2001 | | | This file has an empty time column which effs everything up | | 802 | 2006 | | | Not filtering with sigma filter for some reason | | 896 | 2005-6 | | | \"\" | | 1015 | 2005 | | | \"\" | | 1053 | 2005 | | | \"\" | | 828 | 2006 | | | Excluded because it has values over 45 degrees C | | 742 | multiple | | | Sensors out of order | | 926 | multiple | | | \"\" | | 969 | multiple | | | \"\" | | 592 | multiple | | | Very high temps in soil | | 593 | multiple | | | \"\" | | 766 | multiple | | | \"\" |</p>"},{"location":"soilclim/overview/","title":"Western U.S. snowpacks, soils, and climate","text":"<p>Analysis of environmental data, including snowpack, soil moisture, and soil temperature data, is posted here along with a log of research activities on this project.</p> <p>Project begin date: 23 Feb, 2011</p> <p>Relevant links:</p>"},{"location":"soilclim/overview/#objectives","title":"Objectives","text":"<p>The objective of this project is to understand the influence of seasonal snowpacks on montane and subalpine soil temperature and moisture. Possible influences include:</p> <ul> <li>Insulation of soil during winter, including air-soil temperature offsets and elevational gradients in air vs soil temperature.</li> <li>Interannual variability in mean annual soil temperature, winter soil temperature, and spring soil temperature due to year-to-year differences in snowcover duration, snowpack size, timing of snowcover onset or snowmelt, and air temperature during these transitions.</li> <li>Timing and amount of soil moisture recharge during snowmelt</li> <li>Interannual variability of winter soil moisture due to fall conditions</li> <li>Interannual variability in growing season soil moisture due to preceding winter snowpack size.</li> </ul>"},{"location":"soilclim/overview/#hypotheses","title":"Hypotheses","text":""},{"location":"soilclim/overview/#experimental-design","title":"Experimental Design","text":""},{"location":"soilclim/overview/#methods","title":"Methods","text":"<ul> <li>Data sources/collection for western U.S. climate and soils data.</li> <li>Data quality control - Bad data removal procedures</li> <li>Data analysis log </li> <li>Programming documentation</li> <li>Statistics - Multiple regression and PCA.</li> <li>Data will be analyzed for mountain and lowland sites in the interior western U.S., including the stats of AZ, CO, ID, MT, NM, NV, UT, and WY.</li> </ul>"},{"location":"soilclim/overview/#results","title":"Results","text":"<ul> <li>Preliminary data and activity log</li> <li>Publication outline - Journal article outline, including figures and discussion.</li> </ul>"},{"location":"soilclim/programdocs/","title":"Program documentation for western U.S. station data analysis","text":"<p>These are program file descriptions for programs used to analyze data supplied by the NRCS. Analysis of soil profiles at these data stations relies heavily on techniques described here.</p> <p>See also: Project overview, Data collection/prep, Data analysis log, Data quality control</p>"},{"location":"soilclim/programdocs/#scripts","title":"Scripts","text":"<p>Run as a list of commands that result in some data analysis result. They generate plots or some kind of report output that interprets the data in some way. They usually call functions (like those below) to accomplish this.</p>"},{"location":"soilclim/programdocs/#soildataqcm","title":"soildataQC.m","text":"<p>FIXME - NEEDS TESTING</p> <p>Plots data and 24hr standard deviations for 5cm soil temperature. Performs statistics on these data and 2 filter types. Useful for checking data filtering/filling procedures, determining snowpack presence thresholds, and filtering thresholds.</p> <ul> <li>File inputs: Loads hourly and daily data for a selected site.</li> <li>User Input: Takes one user input: site ID (integer)</li> <li>Output: ??</li> <li>Other: formerly plot_site_ts_variability.m</li> </ul>"},{"location":"soilclim/programdocs/#summarize_wateryearm","title":"summarize_wateryear.m","text":"<p>Takes daily raw data files, and long term averages for all sites, calculates a variety of climatic metrics for each water year, and summarizes these data in a text output file.</p> <ul> <li>File input: Reads <code>rawdata/allsensors_daily/sitelist.txt</code>, and <code>rawdata/soilsensors_hourly/sitelist.txt</code>, <code>7100Avg_SWEmm.csv</code>, <code>7100Avg_Precipmm</code>, and <code>SNOTELinventory.csv</code> in processed_data directory.</li> <li>User input: The user selects whether to run with hourly/daily data, and normalized/non-normalized soil VWC data. </li> <li>Output: <code>wyear_climatesummary.txt</code>, <code>wyear_soiltempsummary_XXX.txt</code>, <code>wyear_soilwatersummary_XXX.txt</code>, and <code>wyear_soilwatersummary_XXX_smnorm.txt</code> (XXX = <code>daily</code> or <code>hourly</code>).</li> <li>Associated files: I use the <code>headersXXX.txt</code> files to keep track of what the headers for the output files should be.</li> </ul>"},{"location":"soilclim/programdocs/#plot_snotel_summarym","title":"plot_snotel_summary.m","text":"<p>Reads the outputs from summarize_wateryear.m (see above)and makes a number of plots characterizing variability in soil moisture and soil temperature in the SNOTEL network.</p> <ul> <li>Input files: <code>wyear_climatesummary.txt</code>, <code>wyear_soiltempsummary_XXX.txt</code>, etc.</li> <li>User input: The user selects whether to run with normalized/non-normalized soil VWC data. </li> </ul>"},{"location":"soilclim/programdocs/#plot_soilsensor_distribm","title":"plot_soilsensor_distrib.m","text":"<p>Plots distributions of soil sensor data for one site in individual months and years. Data for all years (available) is represented.</p> <ul> <li>File inputs: Loads hourly data for a selected site.</li> <li>User Input: Takes three user inputs: site id, sensor type (vwc or temp), and sensor depth (1=5cm, 2=20cm, 3=50cm).</li> <li>Output: First produces a plot with timeseries of selected site sensor data at each available depth, then a 16 panel plot for each wateyear, and one using data for all years. Each 16 panel plot has 4 quarterly histograms and 12 monthly histograms of the soil sensor data selected.</li> <li>Other: Can optionally be run in normalized soil moisture mode (make selection in code).</li> </ul>"},{"location":"soilclim/programdocs/#plot_examplesitesm","title":"plot_examplesites.m","text":"<p>This makes a number of figures that highlight snow-soil interactions at particular sites.</p> <ul> <li>File input: The first 4 figures directly load hourly or daily data. The last 2 use the summary files</li> <li>User input: User can select different sites to plot in figs 2, 4, 5, 6 in the code</li> <li>Output: Produces 6 figures, all are ready for publication:</li> <li>Fig 1 = 2 year SWE/temp comparison at Mosby Mtn.</li> <li>Fig 2 = Multi-year SWE/Tsoil/VWC time series at a site (changeable)</li> <li>Fig 3 = Full VWC timeseries of 4 sites - the data for Fig 4 </li> <li>Fig 4 = VWC Frequency histograms for 4 contrasting sites (changeable)</li> <li>Fig 5 = Scatterplot/Regression of snowcovered Tsoil vs pre-onset T at 1 site (changeable)</li> <li>Fig 6 = Scatterplot/Regression of summer (JAS) VWC vs meltday at 1 site (changeable)</li> </ul>"},{"location":"soilclim/programdocs/#plot_t_elevgradientm","title":"plot_t_elevgradient.m","text":"<p>Plots elevational gradients in air and soil temperature, and the offset between the two, including comparisons of growing season vs winter months</p> <ul> <li>Input files: Reads <code>filelist.txt</code> from the daily and hourly rawdata directories, climate and Tsoil summary files, and SNOTELinventory.csv</li> <li>Output: Many plots are produced, some for publication.</li> </ul>"},{"location":"soilclim/programdocs/#plot_belowsnow_tsm","title":"plot_belowsnow_ts.m","text":"<p>Plots Tsoil and Tsoil-Tair vs SWE for SNOTEL sites (and subsets of SNOTELS), and fits some lines to the data.</p> <ul> <li>Input files: Reads <code>filelist.txt</code> from the daily and hourly rawdata directories, climate and Tsoil summary files, and SNOTELrangelists.</li> <li>User input: None</li> <li>Output: About 7 plots are produced, some ready for publication</li> </ul>"},{"location":"soilclim/programdocs/#plot_belowsnow_smm","title":"plot_belowsnow_sm.m","text":"<p>Plots below-snow soil VWC vs pre-onset VWC, snowpack, etc for SNOTEL sites (and subsets of SNOTELS), and fits some lines to the data.</p> <ul> <li>Input files: Reads <code>filelist.txt</code> from the daily and hourly rawdata directories, climate and VWC summary files, and SNOTELrangelists.</li> <li>User input: </li> <li>Output:</li> </ul>"},{"location":"soilclim/programdocs/#plot_growseas_smm","title":"plot_growseas_sm.m","text":"<p>Reads SNOTEL data files and makes plots characterizing growing season variability in soil VWC (entire network or individual sites), and its relationship to snowpack.</p> <p>Input files: Reads climate and VWC summary files.</p> <p>User input: None, but individual site plotting can be changed.</p> <p>Output: About 8 plots are produced</p>"},{"location":"soilclim/programdocs/#functions","title":"Functions","text":"<p>Do things to the data: load it from file, filter it, calculate statistics or other values, generate secondary data to the original data downloaded from the NRCS. They usually take input arguments and generate an output, and can be called from scripts or other functions.</p>"},{"location":"soilclim/programdocs/#loadsnotelinterval-siteid","title":"loadsnotel('interval', siteID)","text":"<p>Parses headers and loads desired fields from NWCC (SNOTEL/SCAN) yearly datafiles into one standardized array, removes bad data, returns array + headers.</p> <ul> <li>Needs a column order/number check routine...DONE --- Greg Maurer 2011/03/24 21:18</li> <li>FIXME - Needs a method to verify incoming data files for sample interval, and whether the file is complete (0ct1-Sept30).</li> </ul>"},{"location":"soilclim/programdocs/#load7100avgdesiredavg","title":"load7100Avg(desiredAvg)","text":"<p>Loads long-term average SWE or Precip data for all sites using a curated datafile and returns the array.</p> <ul> <li>Arguments: <code>desiredAvg</code> = 'swe' or 'precip'</li> <li>Input files: Reads <code>curated_data/7100Avg_Precipmm.csv</code> or <code>7100Avg_SWEmm.csv</code> \\</li> <li>Output: Outputs an array with site ID, elevation, monthly/bimonthly precip or SWE, and total precip/maxSWE (see input files).</li> </ul>"},{"location":"soilclim/programdocs/#filterseriesarray-type-threshold","title":"filterseries(array, type, threshold)","text":"<p>Filters soil profile data series using a difference from the mean, or difference from neighbor</p> <ul> <li>Arguments</li> <li><code>array</code> = input data series</li> <li><code>type</code> = 'mean', 'median', 'shift', 'sigma', or 'hampel' filter type</li> <li><code>threshold</code> = threshold difference above which datapoint is set to nan</li> </ul>"},{"location":"soilclim/programdocs/#interpseriesarray","title":"interpseries(array)","text":"<p>Interpolates over gaps in a timeseries. It should fill all NaN's with an interpolated value. The actual interpolation is done by the Matlab interp1 function using the \"pchip\" algorithm. This can be useful for running mean and variance calculations which tend to propagate NaN values in a timeseries (see documentation for filterseries.m). Ignores trailing and leading NaN. Adapted from FIXGAPS routine on Matlab Central file exchange, by R. Pawlowicz 6/Nov/99</p> <ul> <li>Arguments</li> <li><code>array</code> = input data series</li> <li>WARNING: If array contains only NaN's, interp1 throws an error.</li> </ul>"},{"location":"soilclim/programdocs/#soiltemp_snowcoverm","title":"soiltemp_snowcover.m","text":"<p>Takes soil temp array and returns a logical matrix indicating snowcover for the site (for all years with available data).</p>"},{"location":"soilclim/programdocs/#swe_snowcoverm","title":"swe_snowcover.m","text":"<p>Takes SWE array and returns a logical matrix indicating snowcover for the site (for all years with available data).</p>"},{"location":"soilclim/programdocs/#testing","title":"Testing","text":""},{"location":"soilclim/programdocs/#testfilesm","title":"testfiles.m","text":"<p>Tests all SNOTEL data files in a directory (hourly or daily) and determines how many measurements are missing in each file. If too many datapoints are missing, the file is listed in an \"exclude\" file that is then read by other scripts.</p> <ul> <li>Input: all SNOTEL .csv files in a directory</li> <li>Output: Histograms of the data, and <code>excludefiles.txt</code> file</li> </ul>"},{"location":"soilclim/programdocs/#testsensorsm","title":"testsensors.m","text":"<p>Open all SNOTEL data files in a directory (hourly or daily), creates plots of each sensor timeseries, and waits for user input. User input is appended to a file (excludesensors.txt) that can be used by other scripts to remove bad sensor data during data analysis.</p> <ul> <li>Input: all SNOTEL .csv files in a directory</li> <li>Output: Sensor timeseries plots, and an <code>excludesensors.txt</code> file</li> </ul>"},{"location":"soilclim/programdocs/#test_loadsnotelminterval-siteid-varargin","title":"test_loadsnotel.m(interval, siteID, varargin)","text":"<p>Loads a snotel site data in the same fashion as loadsnotel.m, but does not remove any data. Also has some facilities to test missing columns and introducing dummy data.</p> <ul> <li>Arguments</li> <li><code>interval</code> = 'hourly' or 'daily' string</li> <li><code>siteID</code> = integer site number</li> <li><code>varargin</code> = = cell array containing a test type ('delete' or 'dummyvalues') and a column number referring to the desired header/column number (1-20)</li> <li>Input: Reads data files from <code>rawdata/allsensors_daily/</code>, or <code>rawdata/soilsensors_hourly</code>. No user input.</li> <li>Output: A data matrix, no data removed.</li> </ul>"},{"location":"soilclim/programdocs/#plot_snoteltestsminterval-siteid","title":"plot_snoteltests.m(interval, siteID)","text":"<p>Loads data for chosen site into two arrays, one with raw data (using test_loadsnotel.m) and one with bad data removed (using loaddata.m). Then plots columns for these arrays with raw data in red and cleaned data in black to show what was removed by loaddata.m.</p>"},{"location":"soilclim/publicationoutline/","title":"Publicationoutline","text":""},{"location":"soilclim/publicationoutline/#snotel-network-soil-temperature-and-moisture","title":"SNOTEL network soil temperature and moisture","text":"<p>Project overview</p>"},{"location":"soilclim/publicationoutline/#introduction","title":"Introduction","text":"<ul> <li>Overview of SNOTEL network and its suitability for understanding ST and SM across the landscape.</li> <li>Papers - Goodrich, Bartlett papers.</li> </ul>"},{"location":"soilclim/publicationoutline/#methods","title":"Methods","text":"<p>We examined data from soil sensor profiles at 252 SNOTEL sites in Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah, and Wyoming. Measurements of snow water equivalent (SWE), precipitation, and air temperature were also used in this analysis. Data were downloaded directly from the NRCS Snow Survey/SNOTEL website (http://http://www.wcc.nrcs.usda.gov/snow/)in delimited text format.</p>"},{"location":"soilclim/publicationoutline/#figures","title":"Figures","text":""},{"location":"soilclim/publicationoutline/#figure-a","title":"Figure A","text":"<p>![media/west_stationdata:figa.png?200|] Data distributions for selected variables discussed in this paper. They are plotted for the entire SNOTEL network of the interior western U.S. (Black bars - 551 sites in AZ, CO, ID, MT, NM, NV, UT, WY) and the subset of those sites that have soil sensor profiles installed (Gray bars - 252 sites). Data for all available wateryears from 2001 to 2011 are included here, except where soil sensor installation occurred earlier, in which case a longer record is present. Mean of each distribution is plotted with a dashed vertical line in the same color.</p>"},{"location":"soilclim/publicationoutline/#figure-b-seasonal-soil-temperature-and-vwc-patterns","title":"Figure B - Seasonal soil temperature and vwc patterns","text":"<p>![media/west_stationdata:figb.png?200|] Time series of SWE, T~soil~, and soil VWC from multiple years at the Currant Creek SNOTEL site (UT). All available wateryears for each variable are plotted in light gray, and a mean of all water years is plotted in black. Mean air temperature over the same time period is also plotted in the middle panel (dashed black line) to illustrate that T~soil~ is decoupled from T~air~ and remains near zero C when a snowpack is present.</p> <ul> <li>Include a low elevation/low snowpack site in this (2 site comparison)?</li> </ul>"},{"location":"soilclim/publicationoutline/#figure-c-elevational-soil-temperature-gradients","title":"Figure C - Elevational soil temperature gradients","text":"<p>![media/west_stationdata:figc.png?200|] Mean January and July T~soil~ (20cm depth) plotted against site elevation at all Utah SNOTEL sites. Each point is a multi-year mean of the site's January or July temperature measurements from all available wateryears. T~soil~ remains near zero at all SNOTEL sites along an elevation transect during winter months. This is in contrast to the same transect of sites in July, where T~soil~ roughly follows the atmospheric temperature lapse rate.</p> <ul> <li>Combine with Fig D in 2 panel?</li> <li>These are actually a mean of all the wateryear means - might check the quality of this.</li> </ul>"},{"location":"soilclim/publicationoutline/#figure-d-air-soil-temperature-offsets-across-the-landscape","title":"Figure D - Air-soil temperature offsets across the landscape","text":"<p>![media/west_stationdata:figd.png?200|] Mean January T~soil~ (20cm depth) and T~air~ plotted against site elevation at all Utah SNOTEL sites. Each point is a multi-year mean of the site's January temperature measurements from all available wateryears. T~soil~ is plotted with bars showing 1 StdDev. An example point showing the mean StdDev of T~air~ is also plotted (inset). Wintertime offsets between air and soil temperature increase with elevation.</p> <ul> <li>FIXME - maybe the offset/elevation plot should be calculated for all 12 months</li> <li>These are actually a mean of all the wateryear means - might check the quality of this.</li> </ul>"},{"location":"soilclim/publicationoutline/#figure-e-snowpack-effect-on-mean-annual-soil-temperature","title":"Figure E - Snowpack effect on mean annual soil temperature","text":"<p>![media/west_stationdata:mast_mat_gradients.png?200|] ![media/west_stationdata:mast_diff_snowpackreg.png?200|] Comparison of mean annual T~soil~ (MAST) and mean annual T~air~ (MAT) across all SNOTEL sites (with soil sensor profiles. Each point is MAST (grey) or MAT (blue) for one site during one wateryear. MAST is higher than MAT, and this difference is related to snowpack duration. Lines are linear regression.</p> <ul> <li>FIXME - probably only need the duration plot, and may be more clear with boxplots or regressions lines.</li> <li>Needs error bars</li> </ul>"},{"location":"soilclim/publicationoutline/#figure-f-inter-annual-variability-in-snowpacksoil-temp","title":"Figure F - Inter-annual variability in snowpack/soil temp","text":"<p>![media/west_stationdata:figf.png?200|] T~soil~, T~air~, and SWE at Mosby Mountain SNOTEL site (UT) during two contrasting water years. In water year 2005, a large snowpack (SWE) accumulated early, leading to stable, near-zero T~soil~ during the remainder of the snowcovered period. In water year 2010, a small early-season snowpack led to colder T~soil~ for much of the snowcovered period.</p> <ul> <li>Should there be a similar figure with spring snowmelt/soil temps?</li> </ul>"},{"location":"soilclim/publicationoutline/#figure-g-early-season-snowpack-and-soil-temperature","title":"Figure G - Early season snowpack and soil temperature","text":"<p>![media/west_stationdata:figg.png?200|] Mean monthly T~soil~ plotted against SWE. Each point is the mean T~soil~ for one month at an individual SNOTEL site. Two sensor depths are plotted for each site in December and January of all available water years. The solid lines are the least-squares fit to a bounded exponential function. Low early season T~soil~ occurs more frequently at sites or during water years that have a small snowpack. A dotted line at zero degrees C is plotted for reference.</p>"},{"location":"soilclim/publicationoutline/#figure-h-effect-of-snowpack-variability-on-growing-season-soil-moisture","title":"Figure H - Effect of snowpack variability on growing season soil moisture","text":"<p>![media/west_stationdata:gsvwc_snowpack.png?100|] ![west_stationdata:gsvwc_snowrain.png?200|] Melt timing has a greater effect on growing season soil moisture than snowpack size.</p> <ul> <li>Panel A: Mean July, August, September soil moisture plotted versus the peak SWE of the prior spring snowpack.</li> <li> <p>Panel B: Mean July, August, September soil moisture plotted versus the wateryear melt day of the prior spring snowpack.</p> </li> <li> <p>FIXME - still some fixing to do - perhaps all 3 depths can go on one plot, and then have SWE and meltday side by side.</p> </li> </ul>"},{"location":"soilclim/publicationoutline/#figure-i-stability-of-winter-soil-moisture","title":"Figure I - Stability of winter soil moisture","text":"<p>![media/west_stationdata:figi.png?120|] Winter month soil VWC plotted against mean VWC of the 2 weeks period prior to snowpack onset. Soil VWC data is normalized to a value between 0 and 1. A one month datapoint for all sites and wateryears has been averaged into VWC units of 0.1 and plotted with the StdDev of this average. A 1:1 line (dotted) is also shown for reference. Soil VWC below a snowpack is closely related to soil VWC immediately prior to the onset of snowcover, and it remains stable until the beginning of the snowmelt cycle.</p>"},{"location":"soilclim/publicationoutline/#figure-j-soil-moisture-histograms-at-4-contrasting-sites","title":"Figure J - Soil moisture histograms at 4 contrasting sites","text":"<p>![media/west_stationdata:figj.png?200|] Histograms of soil water content (normalized to between 0 and 1) for four SNOTEL sites with contrasting profiles of elevation and mean snowpack size. Mean snowpack size is determined from NRCS 30-year mean datasets for the site. The same six years of data, 2006-2011, are shown for all sites. There are 4 histograms per site, representing the 4 quarters of the wateryear. Mean T~soil~ is for each quarter is plotted with a dashed vertical line.</p>"},{"location":"soilclim/publicationoutline/#optionalredundant-figures","title":"Optional/Redundant figures","text":""},{"location":"soilclim/publicationoutline/#figure","title":"Figure","text":"<p>![media/west_stationdata:monthlysoiltempgradient2.png?150|] Monthly avg soil-air T offset by elevation</p> <ul> <li>FIXME - this figure might be redundant, could be combined with air-soil T offsets (for 12 months)?</li> </ul>"},{"location":"soilclim/statistics/","title":"SNOTEL soil climate statistics","text":"<p>This is a place to collect the statistical approach, as well as specific statistical methods, for analysis of data from the SNOTEL soil climate project.</p> <p>See also: Project overview, Data analysis log</p>"},{"location":"soilclim/statistics/#hypotheses","title":"Hypotheses","text":"<p>The (working) hypotheses are something like this (see the publication outline for more):</p> <ul> <li>Seasonal snowpacks dampen elevational gradients in Tsoil.</li> <li>Snowpack variability leads to biologically important variability in Tsoil .</li> <li>Winter VWC is dependent on antecedent fall conditions and exhibits temporal stability until the beginning of snowmelt. </li> <li>Inter-annual variability in growing season soil moisture is correlated with snowpack size and snowmelt timing.</li> </ul>"},{"location":"soilclim/statistics/#multiple-regression","title":"Multiple regression","text":"<p>The original approach was to take the desired dependent variables and fit them to a multiple regression model of several environmental variables as predictors using forward model selection. There were three dependent variables: mean below-snow temperature, winter (jan, feb, mar) soil moisture, and growing season (Jul, Aug, Sep) soil moisture. There were any number of candidate independent variables that could be used as predictors in the models for the three dependent variables. It was difficult to know which predictors to add during the forward selection process, and unfortunately, many, if not most, of the independent variables are collinear. So, in short, though some useful information was gleaned from this, it is not a great statistical approach to testing our hypotheses.</p> <p>For more on this see:</p> <ul> <li>The four R scripts, two for vwc (<code>_simplelm.r</code> and <code>_multiplelm.r</code>) and two for mast_ (<code>_simplelm.r</code>and <code>_multiplelm.r</code>).</li> <li>A summary of these results in table form - <code>regstats.gnumeric</code>.</li> </ul>"},{"location":"soilclim/statistics/#pca","title":"PCA","text":"<p>The current approach uses principal components analysis. Briefly:</p> <ul> <li>Create matrices of environmental variables that may predict each the three dependent variables of interest (mean below-snow temperature, mean Jan+Feb+March soil moisture, and mean Jul+Aug+Sep soil moisture). These can be fairly large and include monthly means of the variables of interest (Tair, Precip, SWE, etc) as well as longer means (multiple months).</li> <li>Run a PCA analyses on these matrices and select the first few principal components based on variance explained.</li> <li>Try to summarize the variable loadings on these axes in a coherent way, if possible.</li> <li>Assemble a multiple regression model that uses the scores from these principal components as the independent variables.</li> </ul>"},{"location":"soilclim/statistics/#below-snow-pca-results","title":"Below-snow PCA results","text":"<p>Have run this for all years and for 2007, 2009 and 2011 individually. In general, the first 3 principal components are the most informative. The loadings and interpretations are something like:</p> <ul> <li>PC1</li> <li>Total snow-covered days, snow-free date load the highest.</li> <li>Apr and May SWE is important for all sites.</li> <li>In some years, individual months of SWE or Tair load high.</li> <li> <p>Interpretation: Spring snowmelt axis. Warm temps (positive values) mean less snow, earlier melt, shorter snow duration.</p> </li> <li> <p>PC2</p> </li> <li>Snow-covered Tair loads consistently high.</li> <li>Other winter SWE and Tair variables load high in some years.</li> <li>Peak SWE also loads pretty high</li> <li> <p>Interpretation: Winter temperature axis, with some interaction with winter SWE.</p> </li> <li> <p>PC3</p> </li> <li>Onset day loads positive</li> <li>Pre-onset Tair and Tsoil load negative.</li> <li> <p>Interpretation: Pre-snowpack axis. Pre-onset temperatures and snowpack onset timing are closely related.</p> </li> <li> <p>PC4</p> </li> <li>This doesn't account for much variance, but its StdDev is over 1 in 2 of the 4 PCAs (all and 2007)</li> <li>For all years PCA, Oct and Nov SWE load highest.</li> <li>For 2007-11 pre-onset Tair, Tsoil, and VWC load highest, but in different combinations in different years.</li> <li>Not sure whether to include this in regression analysis.</li> </ul>"},{"location":"soilclim/statistics/#growing-season-pca-results","title":"Growing Season PCA results","text":"<p>Have run this for all years and for 2007, 2009 and 2011 individually. The loadings and interpretations are something like:</p> <ul> <li>PC1</li> <li>April through September Tair load with high positive values (in all years)</li> <li>Elevation is negative</li> <li>Melt day and total snowcovered days load negative, but are minor</li> <li>Interpretation: Growing season Tair axis</li> <li>PC2</li> <li>Growing season precip loads positive or negative depending on year.</li> <li>Melt day and peak SWE load either positive or negative depending on year.</li> <li>Interpretation: Growing season precip and SWE/snowmelt timing axis</li> <li>PC3</li> <li>Winter soil temperature, soil moisture, and summer precip load positive or negative depending on year.</li> <li>Interpretation: Winter Tsoil and VWC, and growing season precip</li> </ul>"},{"location":"soilclim/statistics/#hypothesis-tests-with-pc-axes","title":"Hypothesis tests with PC axes","text":"<p>I used the principal components axes described above as independent variables for multiple regression predicting below-snow Tsoil and VWC, and growing season VWC. All 3 axes are significant when predicting all years of data. For individual years (2007. 2009, 0r 2011), their value varies.</p>"},{"location":"soilclim/statistics/#landscape-patterns-in-tsoil-and-vwc-variability","title":"Landscape patterns in Tsoil and VWC variability","text":"<p>Also am trying to make sense of patterns of variability across the landscape. For example:</p> <ul> <li>StdDev and CV of mean JFM soil VWC with elevation</li> <li>StdDev and CV of mean snow-covered Tsoil with elevation</li> </ul> <pre><code>cv &lt;- soilTData$snowcovTs20sd/soilTData$snowcovTs20mean\nplot(cv ~ climData.sub$elev)\n</code></pre>"},{"location":"wasatchsnowdep/activitylog_1/","title":"2010-11 Snow Deposition Project Activity","text":"<p>Moved this log to a notebook in the Bowling Lab</p> <p>Dec 8, 2010</p> <ul> <li>Storm boards deployed in Red Butte Canyon and Big Cottonwood Canyons</li> <li>Red Butte site codes: RBL, RBM, RBH (Low, Mid, and High elevations)</li> <li>Big Cottonwood site codes: BCL, BCM, BCH, and HC (Low, Mid, High elevation, and Hidden Canyon).</li> </ul> <p>Dec 6, 2010</p> <ul> <li>Storm boards deployed at ASB building roof (ASB)</li> </ul> <p>Dec 3, 2010</p> <ul> <li>Storm boards deployed at Salt Lake Center for Science Education (SLCSE) and the David Bowling House (DBH)</li> </ul>"},{"location":"wasatchsnowdep/analysislog_1/","title":"Analysislog 1","text":"<p>5/10/2011</p> <ul> <li>Cleaned up the pm2.5 data using find/replace in vim</li> <li>Rebuilt the sampletracking data file from the spreadsheet, removing some bad data/samples</li> <li>Made worksheet for filtering samples</li> <li>Generated timeline graphs of all samples taken (all and by category), and those sent/to-be-sent to Kiowa</li> </ul> <p>5/9/2011</p> <ul> <li>Created a sample log file for matlab (sampletracking_110505.txt).</li> <li>Downloaded hourly pm2.5 data provided by DAQ from PCAPS page.</li> <li>This data is in separate files for each day, so I concatenated them into one file with a bash script (see below)</li> <li>Created a matlab script that reads sample log file and pm2.5 data and outputs a timeline of samples taken, and which have been run.</li> </ul> <p>Bash code to concatenate the daily pm2.5 files (minus headers in each file)</p> <pre><code># Run from directory containing all files\n# Last 24 lines in each file contain the data (the top is all header stuff)\nfor i in *.txt; do tail -24 \"$i\" &gt;&gt; newfile.txt; done\n</code></pre>"},{"location":"wasatchsnowdep/labprocessing/","title":"Snow sample lab processing","text":""},{"location":"wasatchsnowdep/labprocessing/#filtering-samples-for-kiowa-lab","title":"Filtering samples for Kiowa lab","text":""},{"location":"wasatchsnowdep/labprocessing/#materials-list","title":"Materials list","text":"<ul> <li>Thawed snow samples in their nalgene bottles</li> <li>A data entry spreadsheet that includes their sample numbers and a place to enter notes, sample bottle tare weights, and sample weights.</li> <li>This can be derived from the sample log</li> <li>3 250ml Filter flasks (Erlenmeyer type with sideport)</li> <li>2 Millipore filters</li> <li>47mm Nuclepore filter membranes</li> <li>2 vacuum hoses</li> <li>Clean 125ml nalgenes</li> <li>Labeling tape</li> <li>Sharpie</li> </ul>"},{"location":"wasatchsnowdep/labprocessing/#filtration-procedure","title":"Filtration procedure","text":"<ol> <li>Thaw samples</li> <li>Put on clean gloves, and change them often</li> <li>Set up a clean lab bench:</li> <li>Clean all filter flasks with soap, and give them a good rinse in DI water.</li> <li>Clean both Millipore filters.</li> <li>Rinse rubber stoppers</li> <li>Lay out clean lab mats (2) across the bench.</li> <li>Place Nuclepore membrane on the plate of the Millipore filter.</li> <li>Screw the top onto the plate and slide the rubber stopper over the filtrate outlet</li> <li>Place the Millipore assembly on the opening of a filter flask.</li> <li>Attach the vaccuum hose to the sideport of the flask.</li> <li>Turn on the vaccuum about halfway</li> <li>Run 120-150 ml of DI water through the filter</li> <li>Detach the filter flask, throw this filtrate away, then reattach the flask.</li> <li>Turn the vaccuum back on</li> <li>Filter roughly 80-100 ml of sample through the filter</li> <li>Weigh and label a clean 125ml nalgene with the sample id number</li> <li>Also label the original sample nalgene with the sample id number</li> <li>Pour the filtered sample from the filter flask into the nalgene</li> <li>Weigh the nalgene + sample and record this on the spreadsheet.</li> </ol>"},{"location":"wasatchsnowdep/sampling/","title":"Sampling ion and dust deposition in snow","text":""},{"location":"wasatchsnowdep/sampling/#clean-sampling-materials-in-lab","title":"Clean sampling materials in lab","text":"<p>Do this the day before you depart for the field!!</p>"},{"location":"wasatchsnowdep/sampling/#acid-wash-snow-sampling-tools","title":"Acid wash snow sampling tools","text":"<ul> <li>Soak in 10% HCl (in fume hood) overnight</li> <li>Rinse with DI water 5 times.</li> <li>Air dry and place in previously rinsed Ziploc Bag (below)</li> </ul>"},{"location":"wasatchsnowdep/sampling/#serially-rinse-with-di-water-5-times-then-air-dry","title":"Serially rinse with DI water (5 times) then air dry:","text":"<ul> <li>Nalgene sample bottles</li> <li>Coring bag (big Ziplocs)</li> <li>Gallon Ziploc bags (for large samples)</li> </ul>"},{"location":"wasatchsnowdep/sampling/#to-make-10-hcl-acid-wash","title":"To make 10% HCl acid wash:","text":"<ul> <li>Stock solution is 36.4% (36.4g HCl in 100g soln). Density is 1.179g/ml.</li> <li>density^-1^/concentration = ml of stock soln for 1 g HCl = 2.330ml\u2022g^-1^\\</li> <li>To make 1000ml of 10% solution requires 100g HCl and 900g H~2~O.</li> <li>100g HCl = 100g x 2.330ml\u2022g^-1^= 233 ml stock soln</li> <li>For each liter of acid wash add 233ml of HCl stock solution to 767ml distilled water.`</li> </ul> <p>Leave this in a labeled(!) basin in the fume hood.</p>"},{"location":"wasatchsnowdep/sampling/#materials-checklist","title":"Materials checklist","text":"<ul> <li>PVC Snow coring tube (clean and in its clean coring bag)</li> <li>In the plastic sampling bin:</li> <li>Sampling knife (black plastic putty knife)</li> <li>Nitrile gloves</li> <li>Plastic funnel</li> <li>Snow and ice scraper</li> <li>DI Water spray bottle</li> <li>Field book</li> <li>Pencils</li> <li>Sharpie</li> <li>Metric measuring tape</li> <li>Clean Nalgene sample bottles (2 per site).</li> <li>1L bottles should be used for high elevation sites or big snow days.</li> <li>0.5L bottles are sufficient for low elevations/low snow days.</li> <li>Clean 60ml stream sampling bottle (2 per site) </li> <li>Clean gallon ziplocs</li> <li>1 liter of DI water (in a nalgene bottle)</li> <li>Altimeter (for mountain sites)</li> </ul>"},{"location":"wasatchsnowdep/sampling/#field-snow-sampling-procedure","title":"Field snow sampling procedure","text":"<ol> <li>Locate snow sampling board, or its center post.</li> <li>One corner of the board's center post has tickmarks.</li> <li>The 1ft square sampling surfaces cover 2 corners of the board. The first is directly out from the tickmarks and the second is 180\u00b0 around from these marks.</li> <li>If needed (in deeper snow) excavate the snowpack along an edge of each sampling surface for reference.</li> <li>Open sampling kit and PUT ON CLEAN NITRILE GLOVES.</li> <li>Remove  sampling tube from protective bags and place bags in a secure location. Don't get snow in them.</li> <li>Insert snow sampling tube straight down into the snowpack roughly in the center of the 1st sampling surface. If there is a crust, or ice on the sampling surface, give the tube a couple of steady turns to cut through it.</li> <li>Hold the sampling tube steady and excavate the snow from one side of the sampling tube with your other hand. </li> <li>Gently insert the sampling knife (black plastic putty knife - from the bin) between the bottom opening of the sampling tube and the sampling surface. Take care not to lose any of the sample.</li> <li>Pull the tube and knife up from the board and gently slide the opening of the tube over the mouth of a 1l Nalgene sample bottle. You may need a helper to steady the bottle.</li> <li>Gently tap the side of the tube to release the snow sample.</li> <li>If the samples don't easily slide into the Nalgene bottle, try to transfer them through the blue funnel. Again, you may need a helper to manage this.</li> <li>REPEAT steps 7-12 on the 2nd sampling surface (180\u00b0 around the center post), putting the second sample in the same Nalgene bottle.</li> <li>** IF THE SNOWPACK IS SMALL ** (esp at low elevation/urban sites):</li> <li>Take multiple cores from each sampling surface, ie. 2 per side, or 3 per side.</li> <li>This increases sample volume to something more useful (we are shooting for ~50ml or more).</li> <li>Put these additional cores in the same Nalgene bottle.</li> <li>ALWAYS write down the total number of cores taken on the sample bottle and in the field notebook.</li> <li>If the Nalgene bottles are not large enough (large snowpack days) a clean, labeled 1gal ziploc can be used to hold samples.</li> <li>RINSE the sampling tube with DI water using the spray bottle in the sampling bin and carefully put it back in its bags.</li> <li>RINSE the sampling knife and funnel (if used) with DI water and put them back in the bin.</li> <li>LABEL THE BOTTLE/BAGS with the site code and the date (yymmdd format). If multiple bottles/bags are used number them on the label.</li> <li>Using the tape measure in the bin, take 4 snow depth measurements from:</li> <li>The first coring location (1)</li> <li>The second coring location (1)</li> <li>The undisturbed snow on the other two corners of the board (2)</li> <li>Record these in the yellow field book.</li> <li>Clean the snow off of the rest of the board, taking care to scrape all the ice off of the sampling surfaces. Use the blue scraper tool and DI water if needed, and make sure these surfaces are clean.</li> <li>Rinse the scraper with DI and put it back in the bin</li> <li>Place the board a relatively flat area away from overhanging vegetation or structures. This can be the original location you found it, but ONLY if you didn't have to dig a pit to get to the sampling surfaces. Always situate the sampling board at or near the top of the snowpack to minimize surface roughness effects on snow deposition.</li> <li>Record any other pertinent notes about the sampling in the yellow field book. These should include:</li> <li>Date and time</li> <li>Weather (Is it snowing? About to snow? Raining? Is there a thick inversion? etc.)</li> <li>The snow you sampled (Was it saturated with meltwater? Was there an ice crust at the bottom of your sample column? Other signs of melt? etc.)</li> <li>Any disturbances to the snow or board (ski or animal tracks, blown over, etc)</li> <li>Record if you needed to move the board to a new location for any reason (further than a few feet).</li> <li>List the number of samples taken and containers used, any other notes needed to track these samples.</li> <li>IF THE SNOW IS DEEPER THAN 50cm, you will have sample in ~50cm increments until the the entire snow column is sampled. </li> <li>Dig a smooth pit wall down to the edge of the sampling surface.</li> <li>Insert the tube just behind this wall as deeply as you can (but still holding the top).</li> <li>Excavate along the tube by hand down to the bottom of the tube.</li> <li>Leave a mark (using the sampling knife) in the pit wall at the deepest point the tube reaches</li> <li>Using the sample knife, remove the tube and your first sample and transfer to your bottle (as in steps 7-12).</li> <li>Excavate the snow above your mark so that the tube can be inserted again.</li> <li>Insert the tube and collect the next sample, as above.</li> <li>Repeat these steps above until you reach the sampling surface (or bottom of pack).</li> <li>If you are sampling a full snowpack column DO NOT SAMPLE THE LAST 2 CENTIMETERS ABOVE THE GROUND. This prevents contamination from soil and vegetation.`</li> </ol>"},{"location":"wasatchsnowdep/sampling/#stream-sampling","title":"Stream sampling","text":"<p>The water in the main channel of the stream in Big Cottonwood and Red Butte Canyons is sampled immediately following each snow sample collection.</p> <ol> <li>After leaving the storm board label 2 60ml Nalgene bottles with the site code, date, the word \"Stream\", and number each bottle (1/2, 2/2) </li> <li>Wearing your sampling gloves, dip the bottles into the top 20cm of stream water at the designated location.</li> <li>Collect water that is running swiftly (not stagnant or eddy water).</li> <li>Leave about 1 cm of headspace at the top of each bottle.`</li> </ol>"},{"location":"wasatchsnowdep/sampling/#urban-transect-snow-samples","title":"Urban transect snow samples","text":"<p>These transects (Grandeur peak and Avenues) are sampled periodically during inversion events in Salt Lake City.</p> <p>Current procedure:</p> <ol> <li>Select a site with continuous, undisturbed snow, away from trees and other overhanging objects.</li> <li>Label and remove the cap from a clean 60ml Nalgene bottle.</li> <li>Scrape open end of the bottle along the surface of the snowpack, allowing the top 1cm of snow crystals to be pushed into the bottle. Try to sample only the top 1cm.</li> <li>Scrape in several paths along the undisturbed snow surface until the bottle fills up. Do not sample the snow you disturbed with your hand or the bottle in prior scrapings.</li> <li>To compress the snow and make more room in the bottle, screw the cap back on and whack the bottom of the bottle against something hard like your shoe.</li> <li>Do not push snow crystals into the bottle with your hand.</li> <li>Repeat this process until the bottle is full.`</li> </ol> <p>Deprecated procedure - only used once for the Avenues transect:</p> <ol> <li>Select a site with continuous, undisturbed snow, away from trees and other overhanging objects.</li> <li>Invert and insert a clean 0.5L Nalgene bottle into the snowpack up to its bottom, or to 2cm above the ground (whichever is first).</li> <li>Isolate the sample using the plastic putty knife, tip the bottle right side up, and put the lid on.</li> <li>LABEL the bottle with the site code and date.</li> <li>Make notes about snow depth, sampling environment, and snow pack characteristics in the yellow log book.`</li> </ol>"},{"location":"wasatchsnowdep/standards/","title":"Mixing standards for ion analysis","text":"<p>An example for NaCl:</p> <ul> <li>Molar mass of NaCl (the sum of its atomic masses - 22.9897 + 35.453) = 58.4427g</li> <li>Dissolving 1 mole (58.4427g) in 1L water gives a solution that:</li> <li>is 1 Molar NaCl (1 mole of each ion species)</li> <li>has 1 equivalent (molar mass/valence) each of Na^+^ and Cl^-^.</li> <li>is 1 Normal (1N) for each of Na^+^ and Cl^-^.</li> <li>contains 22.9898g Na^+^ and 35.453g Cl^-^ per liter</li> <li>Dissolving 2.5420 g sodium chloride in 1L water gives a solution that:</li> <li>is 0.043495 Molar (2.5420g/58.4427g\u2022mol^-1^).</li> <li>has 43.495 milliequivalents each of Na^+^ and Cl^-^.</li> <li>is 0.043495 Normal for each of Na^+^ and Cl^-^.</li> <li>contains 1000mg Na^+^ per liter (0.043495 mol x 22.9898g\u2022mol^-1^).</li> <li>contains 1542.03mg Cl^-^ per liter (0.043495 mol x 35.453g\u2022mol^-1^)</li> </ul>"},{"location":"wasatchsnowdep/standards/#stock-solutions","title":"Stock solutions","text":"<p>These are modeled after the Kiowa stock solutions found at the Kiowa procedures page. Generally these are mixed to yield a 1000mg/L solution for the desired ion.</p> <p>For each stock solution mix the following ingredients in a 500ml volumetric flask.</p>"},{"location":"wasatchsnowdep/standards/#sodium-chloride","title":"Sodium chloride","text":"<ul> <li>1.271g NaCl and 500ml DI H~2~O.</li> <li>Solution will be as above.</li> </ul>"},{"location":"wasatchsnowdep/standards/#potassium-sulfate","title":"Potassium sulfate","text":"<ul> <li>0.90702g K~2~SO~4~ and 500ml DI H~2~O.</li> <li>1000mg SO~4~^2-^ = 1g/96.0626g\u2022mol^-1^ SO~4~ = 0.010410mol K~2~SO~4~ </li> <li>0.010410mol x 174.2592g\u2022mol^-1^ K~2~SO~4~ = 1.81404g</li> <li>Divide by 2.</li> <li>This solution will:</li> <li>be 10.410 millimolar for SO~4~^2-^ and 20.818 millimolar for K^+^.</li> <li>have 20.820 meq of SO~4~^2-^ and K^+^.</li> <li>contain 1000mg SO~4~^2-^/L and 814.03mg K^+^/L</li> </ul>"},{"location":"wasatchsnowdep/standards/#potassium-nitrate","title":"Potassium nitrate","text":"<p>Note that this give a solution in mg/L of N0~3~^-^ - not Nitrate-N</p> <ul> <li>0.81528g KNO~3~ and 500ml DI H~2~O.</li> <li>1000mg NO~3~^-^ = 1g/62.0049g\u2022mol^-1^ NO~3~ = 0.016128mol KNO~3~ </li> <li>0.016128mol x 101.1032g\u2022mol^-1^ KNO~3~ = 1.6306g</li> <li>Divide by 2.</li> <li>This solution will:</li> <li>be 16.128 millimolar for NO~3~^-^ and for K^+^.</li> <li>have 16.128 meq of NO~3~^-^ and K^+^.</li> <li>contain 1000mg NO~3~^-^/L and 630.58mg K^+^/L</li> </ul>"},{"location":"wasatchsnowdep/standards/#ammonium-chloride","title":"Ammonium chloride","text":"<p>Note that this gives a solution in mgN/L (NH~4~^+^-N) - not in         the ammonium ion itself</p> <ul> <li>1.90948g NH~4~Cl and 500ml DI H~2~O.</li> <li>1000mg NH~4~^+^ -N* = 1g/14.0067g\u2022mol^-1^ N = 0.071394mol N.</li> <li>0.071394mol x 53.4915g\u2022mol^-1^ NH~4~Cl = 3.81897g</li> <li>Divide by 2.</li> <li>This solution will:</li> <li>be 71.394 millimolar for NH~4~^+^ and for Cl^-^.</li> <li>have 71.394 meq of NH~4~^+^ and Cl^-^.</li> <li>contain 1000mgN/L (as NH~4~^+^) </li> <li>contain 1287.74 mg NH~4~^+^/L and 2531.13mg Cl^-^/L</li> </ul>"},{"location":"wasatchsnowdep/standards/#calcium-carbonate","title":"Calcium carbonate","text":"<p>Note that the target here is 500mgCa^+2^/L</p> <ul> <li>Step 1:0.62434g CaCO~3~ and 50ml DI H~2~O.</li> <li>500mg Ca^2+^ = 0.5g/40.078g\u2022mol^-1^ Ca^+^ = 0.012476mol Ca.</li> <li>0.012476mol x 100.0869g\u2022mol^-1^ CaCO~3~ = 1.24868g</li> <li>Divide by 2.</li> <li>Step 2: add 5ml concentrated (36.46MW) HCl dropwise into the mixture above to dissolve the CaCO~3~.</li> <li>Step 3: dilute the mixture to 500ml with DI water</li> <li>This solution will:</li> <li>be 12.476 millimolar for Ca^2+^ and CO~3~^2-^.</li> <li>have 24.952 meq each of Ca^2+^ and CO~3~^2-^.</li> <li>contain 500mgCa^2+^/L and 748.67mg CO~3~^2-^ /L.</li> <li>contain .1823mol Cl^-^ (.005L*36.46g/L) or 6463mgCl^-^/L</li> </ul>"},{"location":"wasatchsnowdep/standards/#100ml-standards","title":"100ml Standards","text":"<p>Mixed using a 10ml graduated cylinder and several volumetric flasks (500ml, 250ml, 100ml).</p>"},{"location":"wasatchsnowdep/standards/#batch-1","title":"Batch 1","text":"<p>Target concentrations for standards:</p> <p>\\^ Ion \\^ Low Target \\^ Med Target \\^ High Target \\^ Notes \\^ | NH~4~^+^ | 0.5 \u03bcEq/L | 5 \u03bcEq/L | 10 \u03bcEq/L | | | NO~3~^-^ | 0.5 \u03bcEq/L | 5 \u03bcEq/L | 10 \u03bcEq/L | | | SO~4~^2-^ | 0.5 \u03bcEq/L | 5 \u03bcEq/L | 12 \u03bcEq/L | | | Na^+^ | 0.4 \u03bcEq/L | 4 \u03bcEq/L | 8 \u03bcEq/L | | | Ca^2-^ | 3 \u03bcEq/L | 12 \u03bcEq/L | 36 \u03bcEq/L | | | K^+^ | 0.4 \u03bcEq/L | 4 \u03bcEq/L | 8 \u03bcEq/L | set by NO~3~^-^ or SO~4~^2-^ | | Cl^-^ | 0.4 \u03bcEq/L | 4 \u03bcEq/L | 8 \u03bcEq/L | set by Na^+^ and NH~4~^+^ | (These ranges are based on Niwot 2009 concentration values from Chris Seibold)</p>"},{"location":"wasatchsnowdep/standards/#sodium-chloride_1","title":"Sodium Chloride","text":"<p>Stock solution is 43.495\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 5ml of stock solution and dilute to 500ml in a volumetric flask (434.95\u03bcEq/L)</li> <li>Rinse glassware, then sample 5ml of this, and dilute to 500ml again (this gives 4.3495 \u03bcEq/L).</li> <li>Rinse glassware, then sample 10ml of this, and dilute to 100ml. This is the 0.43495\u03bcEq/L standard solution for NaCl.</li> </ul> <p>High std</p> <ul> <li>Take 5ml of stock solution and dilute to 500ml in a volumetric flask (434.95\u03bcEq/L)</li> <li>Rinse glassware, then sample 10ml of this, and dilute to 500ml again (this gives 8.699 \u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#potassium-sulfate_1","title":"Potassium sulfate","text":"<p>Stock solution is 20.820\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 10ml of stock solution and dilute to 500ml in a volumetric flask (416.40 \u03bcEq/L)</li> <li>Rinse glassware, then sample 5ml of this, and dilute to 500ml again (this gives 4.164 \u03bcEq/L).</li> <li>Rinse glassware, then sample 10ml of this, and dilute to 100ml. This is the 0.4164\u03bcEq/L standard solution for K~2~SO~4~.</li> </ul> <p>High std</p> <ul> <li>Take 10ml of stock solution and dilute to 500ml in a volumetric flask (416.40 \u03bcEq/L)</li> <li>Rinse glassware, then sample 8ml of this, and dilute to 250ml (this gives 13.3248 \u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#potassium-nitrate_1","title":"Potassium nitrate","text":"<p>Stock solution is 16.128\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 10ml of stock solution and dilute to 500ml in a volumetric flask (322.56 \u03bcEq/L)</li> <li>Rinse glassware, then sample 8ml of this, and dilute to 500ml again (this gives 5.1610 \u03bcEq/L).</li> <li>Rinse glassware, then sample 10ml of this, and dilute to 100ml. This is the 0.5161\u03bcEq/L standard solution for KNO~3~.</li> </ul> <p>High std</p> <ul> <li>Take 10ml of stock solution and dilute to 500ml in a volumetric flask (322.56 \u03bcEq/L)</li> <li>Rinse glassware, then sample 8ml of this, and dilute to 250ml (this gives 10.3219 \u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#ammonium-chloride_1","title":"Ammonium chloride","text":"<p>Stock solution is 71.394\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 5ml of stock solution and dilute to 500ml in a volumetric flask (713.94 \u03bcEq/L)</li> <li>Rinse glassware, then sample 3.5ml of this, and dilute to 500ml again (this gives 4.99758 \u03bcEq/L).</li> <li>Rinse glassware, then sample 10ml of this, and dilute to 100ml. This is the 0.49976\u03bcEq/L standard solution for NH~4~Cl.</li> </ul> <p>High std</p> <ul> <li>Take 5ml of stock solution and dilute to 500ml in a volumetric flask (713.94 \u03bcEq/L)</li> <li>Rinse glassware, then sample 3.5ml of this, and dilute to 500ml again (this gives 4.99758 \u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#calcium-carbonate_1","title":"Calcium carbonate","text":"<p>Stock solution is 24.952\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 5ml of stock solution and dilute to 500ml in a volumetric flask (249.52 \u03bcEq/L)</li> <li>Rinse glassware, then sample 5ml of this, and dilute to 500ml again. This is the 2.4952\u03bcEq/L standard solution for CaCO~3~.</li> </ul> <p>High std</p> <ul> <li>Take 5ml of stock solution and dilute to 500ml in a volumetric flask (249.52 \u03bcEq/L)</li> <li>Rinse glassware, then sample 15ml of this, and dilute to 100ml (this gives 37.4925 \u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#lab-blanks","title":"Lab Blanks","text":"<p>These are filtered DI water, serving as the zero point for our samples. Just filter ~100ml DI water from the tap in the same way that a sample would be treated and put it in a clean nalgene to be included in the sample run. Put three of these in for each sample run.</p>"},{"location":"wasatchsnowdep/standards/#batch-2","title":"Batch 2","text":"<p>Decided to make combined standards this time - 3 low concentration, 3 high concentration, with concentrations based on the ranges for our first set of samples. For each standard (High and Low) follow the directions below and add the required amount of solution to a clean 500ml volumetric flask. Once all ion solutions are added, fill the flask to 500ml with DI water. This solution will fill 3 containers with identical High or Low standard.</p> <p>\\^ Ion \\^ Low Target \\^ High Target \\^ Notes \\^ | NH~4~^+^ | 2.5 \u03bcEq/L | 50 \u03bcEq/L | | | NO~3~^-^ | 0.5 \u03bcEq/L | 55 \u03bcEq/L | | | SO~4~^2-^ | 0.5 \u03bcEq/L | 25 \u03bcEq/L | | | Na^+^ | 1.5 \u03bcEq/L | 60 \u03bcEq/L | | | Ca^2-^ | 1.5 \u03bcEq/L | 65 \u03bcEq/L | | | K^+^ | 1.0 \u03bcEq/L | 80 \u03bcEq/L | set by NO~3~^-^ + SO~4~^2-^ | | Cl^-^ | 4 \u03bcEq/L | 110 \u03bcEq/L | set by Na^+^ + NH~4~^+^ | (These ranges are based on our PCAPS 2010-11 concentration values from the first run at Kiowa)</p>"},{"location":"wasatchsnowdep/standards/#sodium-chloride_2","title":"Sodium Chloride","text":"<p>Stock solution is 43.495\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 5ml of stock solution and dilute to 250ml in a volumetric flask (869.9\u03bcEq/L)</li> <li>Add 1ml of this to the Low Standard (diluted to 500ml = 1.74\u03bcEq/L Na and Cl).</li> </ul> <p>High std</p> <ul> <li>Take 9ml of stock solution and dilute to 100ml in a volumetric flask (3914.55\u03bcEq/L)</li> <li>Add 8ml of this to the High Standard (diluted to 500ml = 62.62\u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#potassium-sulfate_2","title":"Potassium sulfate","text":"<p>Stock solution is 20.820\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 1ml of stock solution and dilute to 250ml in a volumetric flask (83.28 \u03bcEq/L)</li> <li>Add 4ml of this to the Low Standard (diluted to 500ml = 0.67\u03bcEq/L).</li> </ul> <p>High std</p> <ul> <li>Take 10ml of stock solution and dilute to 100ml in a volumetric flask (2082.0 \u03bcEq/L)</li> <li>Add 6ml of this to the High Standard (diluted to 500ml = 24.98\u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#potassium-nitrate_2","title":"Potassium nitrate","text":"<p>Stock solution is 16.128\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 2ml of stock solution and dilute to 250ml in a volumetric flask (129.02 \u03bcEq/L)</li> <li>Add 2ml of this to the Low Standard (diluted to 500ml = 0.516\u03bcEq/L).</li> </ul> <p>High std</p> <ul> <li>Take 10ml of stock solution and dilute to 50ml in a volumetric flask (3225.6 \u03bcEq/L)</li> <li>Add 8ml of this to the High Standard (diluted to 500ml = 51.61\u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#ammonium-chloride_2","title":"Ammonium chloride","text":"<p>Stock solution is 71.394\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 2ml of stock solution and dilute to 250ml in a volumetric flask (571.15 \u03bcEq/L)</li> <li>Add 2ml of this to the Low Standard (diluted to 500ml = 2.28\u03bcEq/L).</li> </ul> <p>High std</p> <ul> <li>Take 9ml of stock solution and dilute to 100ml in a volumetric flask (6425.46 \u03bcEq/L)</li> <li>Add 4ml of this to the High Standard (diluted to 500ml = 51.4\u03bcEq/L).</li> </ul>"},{"location":"wasatchsnowdep/standards/#calcium-carbonate_2","title":"Calcium carbonate","text":"<p>Stock solution is 24.952\u03bcEq/ml</p> <p>Low std</p> <ul> <li>Take 3ml of stock solution and dilute to 250ml in a volumetric flask (299.42 \u03bcEq/L)</li> <li>Add 2ml of this to the Low Standard (diluted to 500ml = 1.20\u03bcEq/L).</li> </ul> <p>High std</p> <ul> <li>Take 10ml of stock solution and dilute to 100ml in a volumetric flask (2495.2 \u03bcEq/L)</li> <li>Add 13ml of this to the High Standard (diluted to 500ml = 64.88\u03bcEq/L).</li> </ul>"},{"location":"wiki/standards/","title":"Rules &amp; Conventions","text":""},{"location":"wiki/standards/#rules","title":"Rules","text":"<ul> <li>No Spamming</li> <li>Please think before you post and be constructive.</li> <li>Don't post copyrighted material or intellectual property without permission and proper attribution (also see copyrightpage).</li> </ul>"},{"location":"wiki/standards/#conventions","title":"Conventions","text":"<ul> <li>This site is intended for scientific collaboration. It is not to be used for:</li> <li>Research unrelated to climate, ecosystems, and carbon or water cycling.</li> <li>Commentaries on climate change or the vast conspiracy behind climate science.</li> <li>Rambling about random subjects - stick to research ideas and activities such as measurements, methodology, results, and relevant discussions about these things.</li> <li>Cite relevant research papers mentioned in your text.</li> <li>Please look at the content of the site before posting or editing and try to be consistent with the style (such as it is) of this content.</li> <li>Before creating a page, make sure it is consistent with the namespace and pagename conventions used on this site. The topic index is a good way to familiarize yourself with the site's organization. For example: If you add a research project, give it a namespace (<code>new_project/</code>) and first describe it with a new <code>overview/</code> page in that namespace (ie. newproject:overview). You can add site-specific pages, like measurement logs, preliminary data, etc., to the  project namespace (ie. new_project:soilcollectionlog). If you add procedures or other info that can be generalized to other projects, put them in the appropriate namespace (procedures, or instruments for example).</li> <li>Please send us an email if you are contributing to, or would like to contribute, the site. We'd love to chat with people with similar research projects and interests.</li> </ul>"},{"location":"wiki/transition2markdown/","title":"Migrating from Dokuwiki to markdown pages","text":"<p>I am in the process of migrating my old wiki from a Dokuwiki installation to a collection of markdown pages that I can maintain with git and some web tools (GitHub, ReadtheDocs, etc).</p> <p>Steps for this are:</p> <ol> <li> <p>Use dokuwiki2git to export a git repository of the pages</p> </li> <li> <p>Copy pages to new directory and change permissions of those pages</p> </li> <li> <p>Convert dokuwiki to markdown using the pandoc mediawiki converter:</p> <pre><code>find . -name \\*.txt -type f -exec pandoc -f mediawiki -t markdown -o {}.md {} \\;\n</code></pre> </li> <li> <p>do some custom editing of the new markdown pages</p> <pre><code>for file in *.txt.md; do git mv \"$file\" \"${file//txt./}\"; done\n</code></pre> </li> <li> <p>replace nonbreaking spaces in vim</p> <pre><code>:%s/&lt;NBSP&gt;/ /g\n</code></pre> <p>where entering <code>&lt;NBSP&gt;</code> is accomplished with <code>&lt;CTRL-k&gt;+space+space</code>. OR - remove in all files with sed:</p> <pre><code>find . -type f -exec sed -i 's/\\xC2\\xA0/ /g' {} +\n</code></pre> </li> <li> <p>Replace other stuff with sed:</p> <pre><code>find . -type f -exec sed -i 's/ \"wikilink\"//g' {} +\nfind . -type f -exec sed -i 's/&lt;\\/code&gt;/~~~/g' {} +\nfind . -type f -exec sed -i 's/&lt;code bash&gt;/~~~/g' {} + \nfind . -type f -exec sed -i 's/-   -   / **/g' {} +\n</code></pre> </li> <li> <p>Change all links to the markdown convention of:</p> <pre><code>[linktext]({subdir/}markdownpage.md)\n</code></pre> <p>or, for relative links to pages in other subdirectories:</p> <pre><code>[linktext](/subdir/markdownpage.md)\n</code></pre> </li> <li> <p>Convert tables - a pain...</p> </li> </ol>"}]}